{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "import theano\n",
    "import numpy as np\n",
    "\n",
    "# _K = K.variable(np.array(\n",
    "#             [\n",
    "#             [[0,1,0,0],\n",
    "#              [1,0,0,0],\n",
    "#              [0,0,0,0],\n",
    "#              [0,0,0,0]],\n",
    "            \n",
    "#             [[0,0,1,0],\n",
    "#              [0,0,0,0],\n",
    "#              [1,0,0,0],\n",
    "#              [0,0,0,0]],\n",
    "                \n",
    "#             [[0,0,0,1],\n",
    "#              [0,0,0,0],\n",
    "#              [0,0,0,0],\n",
    "#              [1,0,0,0]]\n",
    "#             ]\n",
    "#              ))\n",
    "np_K = np.zeros((3,4,4))\n",
    "#_K = K.zeros((3,4,4))\n",
    "for i in range(0,3):\n",
    "    print(i)\n",
    "    np_K[i,0,i+1] = 1\n",
    "    np_K[i,i+1,0] = 1\n",
    "    \n",
    "_K = K.variable(np_K)\n",
    "\n",
    "\n",
    "def _lorentz(x, w):\n",
    "    \n",
    "    batch_size = K.shape(x)[0]\n",
    "    vector_cluster_size = K.shape(w)[0]\n",
    "    # w = K.zeros(3)\n",
    "    # x = K.zeros(4)\n",
    "    _mag = K.sqrt(K.sum(K.square(w), axis=1,keepdims=True))\n",
    "    #assert _mag.ndim == 1\n",
    "    #print(\"_w\")\n",
    "    #print(K.eval(w))\n",
    "    #print(\"_mag\")\n",
    "    #print(K.eval(_mag))\n",
    "    \n",
    "    _g = 1.#/K.sqrt(1.-K.square(_mag))\n",
    "    #assert _g.ndim == 1\n",
    "    #print(\"_g\")\n",
    "    #print(K.shape(K.eval(_g)))\n",
    "    #print(K.eval(_g))\n",
    "    \n",
    "    _inv_mag = 1/_mag\n",
    "    #print(\"_inv_mag\")\n",
    "    #print(K.eval(_inv_mag))\n",
    "    #_n = K.batch_dot(w,  _inv_mag)\n",
    "    _n = w *  _inv_mag\n",
    "    \n",
    "    #print(\"_n\")\n",
    "    #print(K.eval(K.shape(_n)))\n",
    "    #print(K.eval(_n))\n",
    "    \n",
    "    #print(\"_K\")\n",
    "    #print(K.eval(_K))\n",
    "    _bK = K.reshape(_K, (1,3,4,4))\n",
    "    \n",
    "    #print(\"_bK\")\n",
    "    #print( K.eval(_bK))\n",
    "    _bK = K.repeat_elements(_bK, vector_cluster_size, axis=0)\n",
    "    \n",
    "    #print(\"_bK\")\n",
    "    #print(K.eval(K.shape(_bK)))\n",
    "    #print( K.eval(_bK))\n",
    "    \n",
    "    _bI = K.repeat_elements(K.reshape(K.eye(4), (1,4,4)), vector_cluster_size, axis=0)\n",
    "    #print(\"_bI\")\n",
    "    #print(K.eval(K.shape(_bI)))\n",
    "    \n",
    "    _b1 = K.repeat_elements(K.eye(1),vector_cluster_size, axis=0)\n",
    "    #print(\"_b1\")\n",
    "    #print(K.eval(K.shape(_b1)))\n",
    "    #print(\"_g-_b1\")\n",
    "    #print(K.eval(K.shape(_g-_b1)))\n",
    "    \n",
    "    #assert _n.ndim == 2\n",
    "    _nK = K.batch_dot(_n, _bK, axes=[[1],[1]])\n",
    "    #_nK = _n * _K\n",
    "    _nK = K.reshape(_nK, (vector_cluster_size,1,4,4))\n",
    "    #print(\"_nK\")\n",
    "    #print(K.eval(K.shape(_nK)))\n",
    "    #print(K.eval(_nK))\n",
    "    \n",
    "    #print(\"_g*_mag\")\n",
    "    #print(K.eval(K.shape(K.sum(_g*_mag, axis=1))))\n",
    "    #r = K.batch_dot(_g*_mag, _nK, axes=[[1],[1]])\n",
    "    #print(K.eval(r))\n",
    "    #print(K.shape(K.eval(r)))\n",
    "    #print(K.eval(K.batch_dot(K.sum(_g*_mag, axis=1), _nK)))\n",
    "    \n",
    "    #print(\"_nk2\")\n",
    "    #print(K.eval(K.batch_dot(_nK,_nK)))\n",
    "    #K.repeat(K.eye(4), K.shape(w)[0])  -\n",
    "    #*K.batch_dot(_nK,_nK)\n",
    "    _nKr = K.reshape(_nK, (vector_cluster_size,1,4,4))\n",
    "    _nK2r = K.reshape(K.batch_dot(_nK,_nK), (vector_cluster_size,1,4,4))\n",
    "    _B = _bI - K.batch_dot(_g*_mag, _nKr, axes=[[1],[1]]) +K.batch_dot(_g-_b1,_nK2r,axes=[[1],[1]])\n",
    "    \n",
    "    #print(\"_B\")\n",
    "    #print(K.shape(K.eval(_B)))\n",
    "    #print(K.eval(_B))\n",
    "    \n",
    "    x_shape = K.shape(x)\n",
    "    x = K.reshape(x, (x_shape[0],x_shape[1], 1, 4))\n",
    "    _B = K.reshape(_B, (1,vector_cluster_size,4,4))\n",
    "    _mB = K.repeat_elements(_B,batch_size, axis=0)\n",
    "    \n",
    "    #print(\"_mB\")\n",
    "    #print(K.shape(K.eval(_mB)))\n",
    "    #print(K.eval(_mB))\n",
    "    #r = K.variable(np.random.random((50, 12,4)))\n",
    "    #r = K.reshape(r, (50, 12,1,4))\n",
    "    #print(\"r\")\n",
    "    #print(K.shape(K.eval(r)))\n",
    "    #print(K.eval(r))\n",
    "    \n",
    "    #out = x + _mB\n",
    "    #out = K.dot(_mB, x, axes=[[1],[1]])\n",
    "    out = K.reshape(K.batch_dot(_mB, x, axes=[[1,3],[1,3]]), (batch_size, 1,4))\n",
    "    \n",
    "    #print(\"out\")\n",
    "    #print(K.shape(K.eval(out)))\n",
    "    #print(K.eval(out))\n",
    "    #f = lambda x_i: K.batch_dot(x_i, _B, axes=[[1],[1]])\n",
    "    #for\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class LorentzLayer(Layer):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(LorentzLayer, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        print(input_shape)\n",
    "        ## the input shape should be (..., 4)\n",
    "        # weights are including\n",
    "        input_dim = input_shape[1]\n",
    "        ## angle1, angle2, beta : dim 3\n",
    "        initial_weight_value = np.random.random((input_dim,3))\n",
    "        print(initial_weight_value.shape)\n",
    "        initial_bias_value = np.random.random((1,3))\n",
    "        print(initial_bias_value.shape)\n",
    "        self.W = K.variable(initial_weight_value)\n",
    "        self.B = K.variable(initial_bias_value)\n",
    "        # self.trainable_weights = [self.W]\n",
    "        self.trainable_weights = [self.W, self.B]\n",
    "\n",
    "    def call(self, T, mask=None):\n",
    "        print(\"call\")\n",
    "        print(type(T[0]))\n",
    "        print(T[0])\n",
    "        # should get a tensor (N,4) corresponding to n 4-vesctors\n",
    "        #lorentzboost of the vectorial sum of each lorentzboosted 4 vector\n",
    "        return _lorentz(_lorentz( T, self.W ), self.B)\n",
    "        #return K.sum(T, axis=1)\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        print(\"get ouput shape\")\n",
    "        return (input_shape[0], self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 12, 4)\n",
      "(12, 3)\n",
      "(1, 3)\n",
      "call\n",
      "<class 'theano.tensor.var.TensorVariable'>\n",
      "Subtensor{int64}.0\n",
      "get ouput shape\n",
      "call\n",
      "<class 'theano.tensor.var.TensorVariable'>\n",
      "Subtensor{int64}.0\n",
      "get ouput shape\n"
     ]
    }
   ],
   "source": [
    "# %load compile.py\n",
    "#Compile\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, LSTM\n",
    "model = Sequential()\n",
    "model.add(LorentzLayer(4 ,input_shape=(12,4)))\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  3.22298708e-01   8.21822361e-01   4.20888555e-01   1.84233083e-01]\n",
      " [  3.75308108e-01   7.12993626e-01   5.69778280e-04   5.50152228e-01]\n",
      " [  5.56942189e-01   8.86663236e-01   1.04850646e-01   9.20654928e-01]\n",
      " [  6.95596975e-01   2.57226106e-01   2.92598354e-01   1.61507268e-01]\n",
      " [  8.31834055e-01   1.69289018e-01   5.95563288e-01   7.58896548e-01]\n",
      " [  4.41004312e-01   8.32264725e-01   6.23875460e-01   8.35941442e-01]\n",
      " [  3.16786306e-01   5.31228389e-01   2.18218377e-01   2.79105613e-01]\n",
      " [  3.81873009e-01   1.95871543e-01   7.82164266e-01   3.03226405e-02]\n",
      " [  5.71220043e-01   7.75600515e-02   6.33320821e-01   9.89266780e-01]\n",
      " [  5.70398543e-02   4.80021424e-01   1.45255659e-01   6.34337581e-01]\n",
      " [  8.48047627e-01   5.96483578e-01   8.46676232e-01   7.71046195e-01]\n",
      " [  8.82264430e-01   5.57077439e-01   9.66980277e-01   6.28359901e-01]]\n",
      "[ 0.37183971  0.63824208  0.16335966  0.44440239]\n"
     ]
    }
   ],
   "source": [
    "#X_train = [[(8, 1, 1, 1),\n",
    "#          (7, 2, 1, 2),\n",
    "#          (10, 1, 2, 1),\n",
    "#          (16, 1, 1, 2),\n",
    "#          (20, 3, 1, 1)]]\n",
    "#y_train = [(15, 1, 1, 2)]\n",
    "\n",
    "x_train = np.random.random((1000, 12, 4))\n",
    "y_train = np.random.random((1000, 4))\n",
    "print(x_train[0])\n",
    "print(y_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO (theano.gof.compilelock): Refreshing lock /home/dweitekamp/.theano/compiledir_Linux-3.19--generic-x86_64-with-Ubuntu-15.04-vivid-x86_64-2.7.9-64/lock_dir/lock\n",
      "INFO:theano.gof.compilelock:Refreshing lock /home/dweitekamp/.theano/compiledir_Linux-3.19--generic-x86_64-with-Ubuntu-15.04-vivid-x86_64-2.7.9-64/lock_dir/lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1s - loss: 6.3045 - acc: 0.2547\n",
      "Epoch 2/100\n",
      "0s - loss: 0.8196 - acc: 0.2519\n",
      "Epoch 3/100\n",
      "1s - loss: 0.7074 - acc: 0.2507\n",
      "Epoch 4/100\n",
      "1s - loss: 0.7006 - acc: 0.2496\n",
      "Epoch 5/100\n",
      "0s - loss: 0.6922 - acc: 0.2515\n",
      "Epoch 6/100\n",
      "0s - loss: 0.6855 - acc: 0.2500\n",
      "Epoch 7/100\n",
      "1s - loss: 0.6813 - acc: 0.2517\n",
      "Epoch 8/100\n",
      "0s - loss: 0.6753 - acc: 0.2527\n",
      "Epoch 9/100\n",
      "0s - loss: 0.6702 - acc: 0.2477\n",
      "Epoch 10/100\n",
      "0s - loss: 0.6670 - acc: 0.2488\n",
      "Epoch 11/100\n",
      "0s - loss: 0.6603 - acc: 0.2527\n",
      "Epoch 12/100\n",
      "0s - loss: 0.6562 - acc: 0.2516\n",
      "Epoch 13/100\n",
      "1s - loss: 0.6535 - acc: 0.2463\n",
      "Epoch 14/100\n",
      "0s - loss: 0.6476 - acc: 0.2506\n",
      "Epoch 15/100\n",
      "0s - loss: 0.6455 - acc: 0.2507\n",
      "Epoch 16/100\n",
      "1s - loss: 0.6411 - acc: 0.2502\n",
      "Epoch 17/100\n",
      "0s - loss: 0.6371 - acc: 0.2530\n",
      "Epoch 18/100\n",
      "0s - loss: 0.6381 - acc: 0.2498\n",
      "Epoch 19/100\n",
      "1s - loss: 0.6316 - acc: 0.2479\n",
      "Epoch 20/100\n",
      "0s - loss: 0.6283 - acc: 0.2524\n",
      "Epoch 21/100\n",
      "0s - loss: 0.6266 - acc: 0.2505\n",
      "Epoch 22/100\n",
      "1s - loss: 0.6216 - acc: 0.2528\n",
      "Epoch 23/100\n",
      "0s - loss: 0.6191 - acc: 0.2504\n",
      "Epoch 24/100\n",
      "0s - loss: 0.6204 - acc: 0.2475\n",
      "Epoch 25/100\n",
      "1s - loss: 0.6154 - acc: 0.2500\n",
      "Epoch 26/100\n",
      "0s - loss: 0.6145 - acc: 0.2524\n",
      "Epoch 27/100\n",
      "0s - loss: 0.6119 - acc: 0.2511\n",
      "Epoch 28/100\n",
      "1s - loss: 0.6102 - acc: 0.2511\n",
      "Epoch 29/100\n",
      "0s - loss: 0.6078 - acc: 0.2528\n",
      "Epoch 30/100\n",
      "1s - loss: 0.6064 - acc: 0.2511\n",
      "Epoch 31/100\n",
      "1s - loss: 0.6046 - acc: 0.2500\n",
      "Epoch 32/100\n",
      "0s - loss: 0.6031 - acc: 0.2500\n",
      "Epoch 33/100\n",
      "1s - loss: 0.6018 - acc: 0.2530\n",
      "Epoch 34/100\n",
      "1s - loss: 0.5995 - acc: 0.2539\n",
      "Epoch 35/100\n",
      "1s - loss: 0.5979 - acc: 0.2508\n",
      "Epoch 36/100\n",
      "1s - loss: 0.5982 - acc: 0.2493\n",
      "Epoch 37/100\n",
      "0s - loss: 0.5947 - acc: 0.2492\n",
      "Epoch 38/100\n",
      "1s - loss: 0.5943 - acc: 0.2527\n",
      "Epoch 39/100\n",
      "1s - loss: 0.5953 - acc: 0.2494\n",
      "Epoch 40/100\n",
      "0s - loss: 0.5924 - acc: 0.2495\n",
      "Epoch 41/100\n",
      "1s - loss: 0.5915 - acc: 0.2522\n",
      "Epoch 42/100\n",
      "1s - loss: 0.5915 - acc: 0.2491\n",
      "Epoch 43/100\n",
      "1s - loss: 0.5885 - acc: 0.2517\n",
      "Epoch 44/100\n",
      "1s - loss: 0.5884 - acc: 0.2482\n",
      "Epoch 45/100\n",
      "0s - loss: 0.5892 - acc: 0.2496\n",
      "Epoch 46/100\n",
      "1s - loss: 0.5864 - acc: 0.2514\n",
      "Epoch 47/100\n",
      "1s - loss: 0.5859 - acc: 0.2520\n",
      "Epoch 48/100\n",
      "0s - loss: 0.5868 - acc: 0.2502\n",
      "Epoch 49/100\n",
      "1s - loss: 0.5837 - acc: 0.2508\n",
      "Epoch 50/100\n",
      "1s - loss: 0.5856 - acc: 0.2516\n",
      "Epoch 51/100\n",
      "1s - loss: 0.5838 - acc: 0.2502\n",
      "Epoch 52/100\n",
      "1s - loss: 0.5830 - acc: 0.2478\n",
      "Epoch 53/100\n",
      "0s - loss: 0.5809 - acc: 0.2489\n",
      "Epoch 54/100\n",
      "1s - loss: 0.5833 - acc: 0.2505\n",
      "Epoch 55/100\n",
      "1s - loss: 0.5813 - acc: 0.2535\n",
      "Epoch 56/100\n",
      "0s - loss: 0.5815 - acc: 0.2510\n",
      "Epoch 57/100\n",
      "1s - loss: 0.5807 - acc: 0.2509\n",
      "Epoch 58/100\n",
      "1s - loss: 0.5817 - acc: 0.2461\n",
      "Epoch 59/100\n",
      "1s - loss: 0.5808 - acc: 0.2504\n",
      "Epoch 60/100\n",
      "1s - loss: 0.5806 - acc: 0.2504\n",
      "Epoch 61/100\n",
      "0s - loss: 0.5773 - acc: 0.2505\n",
      "Epoch 62/100\n",
      "1s - loss: 0.5786 - acc: 0.2489\n",
      "Epoch 63/100\n",
      "1s - loss: 0.5785 - acc: 0.2518\n",
      "Epoch 64/100\n",
      "0s - loss: 0.5783 - acc: 0.2505\n",
      "Epoch 65/100\n",
      "1s - loss: 0.5776 - acc: 0.2494\n",
      "Epoch 66/100\n",
      "1s - loss: 0.5786 - acc: 0.2496\n",
      "Epoch 67/100\n",
      "1s - loss: 0.5764 - acc: 0.2498\n",
      "Epoch 68/100\n",
      "1s - loss: 0.5771 - acc: 0.2484\n",
      "Epoch 69/100\n",
      "0s - loss: 0.5766 - acc: 0.2490\n",
      "Epoch 70/100\n",
      "1s - loss: 0.5765 - acc: 0.2486\n",
      "Epoch 71/100\n",
      "1s - loss: 0.5748 - acc: 0.2499\n",
      "Epoch 72/100\n",
      "0s - loss: 0.5772 - acc: 0.2495\n",
      "Epoch 73/100\n",
      "1s - loss: 0.5767 - acc: 0.2466\n",
      "Epoch 74/100\n",
      "1s - loss: 0.5745 - acc: 0.2529\n",
      "Epoch 75/100\n",
      "1s - loss: 0.5755 - acc: 0.2507\n",
      "Epoch 76/100\n",
      "1s - loss: 0.5745 - acc: 0.2488\n",
      "Epoch 77/100\n",
      "0s - loss: 0.5761 - acc: 0.2486\n",
      "Epoch 78/100\n",
      "1s - loss: 0.5734 - acc: 0.2520\n",
      "Epoch 79/100\n",
      "1s - loss: 0.5741 - acc: 0.2500\n",
      "Epoch 80/100\n",
      "0s - loss: 0.5738 - acc: 0.2484\n",
      "Epoch 81/100\n",
      "1s - loss: 0.5726 - acc: 0.2523\n",
      "Epoch 82/100\n",
      "1s - loss: 0.5754 - acc: 0.2511\n",
      "Epoch 83/100\n",
      "1s - loss: 0.5729 - acc: 0.2521\n",
      "Epoch 84/100\n",
      "1s - loss: 0.5742 - acc: 0.2506\n",
      "Epoch 85/100\n",
      "0s - loss: 0.5738 - acc: 0.2507\n",
      "Epoch 86/100\n",
      "1s - loss: 0.5722 - acc: 0.2521\n",
      "Epoch 87/100\n",
      "1s - loss: 0.5736 - acc: 0.2501\n",
      "Epoch 88/100\n",
      "0s - loss: 0.5721 - acc: 0.2518\n",
      "Epoch 89/100\n",
      "1s - loss: 0.5746 - acc: 0.2482\n",
      "Epoch 90/100\n",
      "1s - loss: 0.5724 - acc: 0.2474\n",
      "Epoch 91/100\n",
      "1s - loss: 0.5730 - acc: 0.2495\n",
      "Epoch 92/100\n",
      "1s - loss: 0.5742 - acc: 0.2509\n",
      "Epoch 93/100\n",
      "0s - loss: 0.5720 - acc: 0.2493\n",
      "Epoch 94/100\n",
      "1s - loss: 0.5712 - acc: 0.2505\n",
      "Epoch 95/100\n",
      "1s - loss: 0.5718 - acc: 0.2544\n",
      "Epoch 96/100\n",
      "0s - loss: 0.5737 - acc: 0.2501\n",
      "Epoch 97/100\n",
      "1s - loss: 0.5715 - acc: 0.2508\n",
      "Epoch 98/100\n",
      "1s - loss: 0.5736 - acc: 0.2498\n",
      "Epoch 99/100\n",
      "1s - loss: 0.5714 - acc: 0.2511\n",
      "Epoch 100/100\n",
      "1s - loss: 0.5721 - acc: 0.2502\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=50,\n",
    "                    nb_epoch=100,\n",
    "                       verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 657,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
