{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(True, True)\n",
      "120.812519\n",
      "using gpu0\n",
      "using theano\n"
     ]
    }
   ],
   "source": [
    "#We can go into our root file and see what Trees are availiable\n",
    "%matplotlib inline\n",
    "import sys, os\n",
    "if __package__ is None:\n",
    "    import sys, os\n",
    "    sys.path.append(os.path.realpath(\"/data/shared/Software/\"))\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ntpath\n",
    "import glob\n",
    "import deepconfig\n",
    "\n",
    "from keras.utils.visualize_util import plot\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from CMS_Deep_Learning.utils.preprocessing import ObjectProfile, preprocessFromPandas_label_dir_pairs, label_dir_pairs_args_decoder, resolveProfileMaxes\n",
    "from CMS_Deep_Learning.utils.metrics import plot_history, print_accuracy_m\n",
    "from CMS_Deep_Learning.utils.callbacks import OverfitStopping, SmartCheckpoint\n",
    "from CMS_Deep_Learning.utils.archiving import *\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Flatten, Reshape, Activation, Dropout, Convolution2D, merge, Input, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "import time\n",
    "import threading\n",
    "\n",
    "print(time.clock())\n",
    "#Choose the GPU\n",
    "dc = deepconfig.deepconfig(gpu='gpu0', backend='theano')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Set up all of our preprocessing parameters\n",
    "observ_types = ['E/c', 'Px', 'Py', 'Pz', 'Charge', \"PT_ET\", \"Eta\", \"Phi\", \"Dxy_Ehad_Eem\"]\n",
    "vecsize = len(observ_types)\n",
    "sample_start = 0\n",
    "#NOT USING VERY MANY SMAPLES JUST SO ILLUSTRATE\n",
    "samples_per_label = 1000\n",
    "\n",
    "archive_dir = \"/data/shared/Delphes/keras_archive/\"\n",
    "\n",
    "\n",
    "object_profiles = [ObjectProfile(\"Electron\",5),\n",
    "                    ObjectProfile(\"MuonTight\", 5),\n",
    "                    ObjectProfile(\"Photon\", 25),\n",
    "                    ObjectProfile(\"MissingET\", 1),\n",
    "                    ObjectProfile(\"EFlowPhoton\",100, sort_columns=[\"PT_ET\"], sort_ascending=False),  #max ~1300\n",
    "                    ObjectProfile(\"EFlowNeutralHadron\",100, sort_columns=[\"PT_ET\"], sort_ascending=False),  #max ~1000\n",
    "                    ObjectProfile(\"EFlowTrack\",100, sort_columns=[\"PT_ET\"], sort_ascending=False)]  #max ~1050\n",
    "\n",
    "label_dir_pairs = \\\n",
    "            [   (\"ttbar\", \"/data/shared/Delphes/ttbar_lepFilter_13TeV/pandas_unjoined/\"),\n",
    "                (\"wjet\", \"/data/shared/Delphes/wjets_lepFilter_13TeV/pandas_unjoined/\")#,#\n",
    "#                (\"qcd\", \"/data/shared/Delphes/qcd_lepFilter_13TeV/pandas_unjoined/\")\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#object_profiles = [ObjectProfile(\"Electron\",-1),\n",
    "#                    ObjectProfile(\"MuonTight\", -1),\n",
    "#                    ObjectProfile(\"Photon\", -1),\n",
    "#                    ObjectProfile(\"MissingET\", -1),\n",
    "#                    ObjectProfile(\"EFlowPhoton\",-1, sort_columns=[\"PT_ET\"], sort_ascending=False),  #max ~1300\n",
    "#                    ObjectProfile(\"EFlowNeutralHadron\",-1, sort_columns=[\"PT_ET\"], sort_ascending=False),  #max ~1000\n",
    "#                    ObjectProfile(\"EFlowTrack\",-1, sort_columns=[\"PT_ET\"], sort_ascending=False)]  #max ~1050\n",
    "\n",
    "\n",
    "#resolveProfileMaxes(object_profiles, label_dir_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Electron', 5)\n",
      "('MuonTight', 5)\n",
      "('Photon', 25)\n",
      "('MissingET', 1)\n",
      "('EFlowPhoton', 100)\n",
      "('EFlowNeutralHadron', 100)\n",
      "('EFlowTrack', 100)\n"
     ]
    }
   ],
   "source": [
    "for profile in object_profiles:\n",
    "    print(profile.name , profile.max_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 97996), (97996, 20999), (118995, 20999)]\n",
      "Generating DataProcedure in range(0,97996):\n",
      "   From 2 labels in range(0,10000) for 2x10000 = 20000 Samples\n",
      "   From 2 labels in range(10000,20000) for 2x10000 = 20000 Samples\n",
      "   From 2 labels in range(20000,30000) for 2x10000 = 20000 Samples\n",
      "   From 2 labels in range(30000,40000) for 2x10000 = 20000 Samples\n",
      "   From 2 labels in range(40000,50000) for 2x10000 = 20000 Samples\n",
      "   From 2 labels in range(50000,60000) for 2x10000 = 20000 Samples\n",
      "   From 2 labels in range(60000,70000) for 2x10000 = 20000 Samples\n",
      "   From 2 labels in range(70000,80000) for 2x10000 = 20000 Samples\n",
      "   From 2 labels in range(80000,90000) for 2x10000 = 20000 Samples\n",
      "   From 2 labels in range(90000,97996) for 2x7996 = 15992 Samples\n",
      "Generating DataProcedure in range(97996,118995):\n",
      "   From 2 labels in range(97996,107996) for 2x10000 = 20000 Samples\n",
      "   From 2 labels in range(107996,117996) for 2x10000 = 20000 Samples\n",
      "   From 2 labels in range(117996,118995) for 2x999 = 1998 Samples\n",
      "Generating DataProcedure in range(118995,139994):\n",
      "   From 2 labels in range(118995,128995) for 2x10000 = 20000 Samples\n",
      "   From 2 labels in range(128995,138995) for 2x10000 = 20000 Samples\n",
      "   From 2 labels in range(138995,139994) for 2x999 = 1998 Samples\n",
      "Generating DataProcedure in range(0,97996):\n",
      "   From 2 labels in range(0,10000) for 2x10000 = 20000 Samples\n",
      "   From 2 labels in range(10000,20000) for 2x10000 = 20000 Samples\n",
      "   From 2 labels in range(20000,30000) for 2x10000 = 20000 Samples\n",
      "   From 2 labels in range(30000,40000) for 2x10000 = 20000 Samples\n",
      "   From 2 labels in range(40000,50000) for 2x10000 = 20000 Samples\n",
      "   From 2 labels in range(50000,60000) for 2x10000 = 20000 Samples\n",
      "   From 2 labels in range(60000,70000) for 2x10000 = 20000 Samples\n",
      "   From 2 labels in range(70000,80000) for 2x10000 = 20000 Samples\n",
      "   From 2 labels in range(80000,90000) for 2x10000 = 20000 Samples\n",
      "   From 2 labels in range(90000,97996) for 2x7996 = 15992 Samples\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "def maxMutualLength(label_dir_pairs, object_profiles):\n",
    "    label_totals = {}\n",
    "    for (label,data_dir) in label_dir_pairs:\n",
    "        files = glob.glob(data_dir+\"*.h5\")\n",
    "        files.sort()\n",
    "        \n",
    "        keys = None\n",
    "        if(object_profiles != None):\n",
    "            keys = [\"/\" + o.name for o in object_profiles]\n",
    "        \n",
    "        label_totals[label] = 0\n",
    "         #Loop the files associated with the current label\n",
    "        \n",
    "        for f in files:\n",
    "            #Get the HDF Store for the file\n",
    "            store = pd.HDFStore(f)\n",
    "            #print(keys)\n",
    "            #print(store.keys())\n",
    "            #print(set(keys).issubset(set(store.keys())))\n",
    "            if(keys != None and set(keys).issubset(set(store.keys())) == False):\n",
    "                raise KeyError('File: ' + f + ' may be corrupted:' + os.linesep + \n",
    "                                'Requested keys: ' + str(keys) + os.linesep + \n",
    "                                'But found keys: ' + str(store.keys()) )\n",
    "            \n",
    "            #Get file_total_entries\n",
    "            try:\n",
    "                num_val_frame = store.get('/NumValues')\n",
    "            except KeyError as e:\n",
    "                raise KeyError(str(e) + \" \" + f)\n",
    "            file_total_entries = len(num_val_frame.index)\n",
    "            label_totals[label] += file_total_entries\n",
    "    #print(label_totals)\n",
    "    return min(label_totals.items())[1]\n",
    "\n",
    "def start_num_fromSplits(splits, length):\n",
    "    if(np.isclose(sum(splits),1.0) == False):\n",
    "        raise ValueError(\"Sum of splits %r must equal 1.0\" % sum(splits))\n",
    "    if(True in [x < 0.0 for x in splits]):\n",
    "        raise ValueError(\"Splits cannot be negative\") \n",
    "    nums = [int(s*length) for s in splits]\n",
    "    out = []\n",
    "    start = 0\n",
    "    for n in nums:\n",
    "        out.append((start, n))\n",
    "        start += n\n",
    "    return out\n",
    "\n",
    "def procsFrom_label_dir_pairs(start, samples_per_label, stride, archive_dir,label_dir_pairs, object_profiles, observ_types, verbose=1):\n",
    "    procs = []\n",
    "    end = start+samples_per_label\n",
    "    if(verbose >= 1): print(\"Generating DataProcedure in range(%r,%r):\" % (start, end))\n",
    "    for proc_start in range(start, end, stride):\n",
    "        proc_num = min(stride, end-proc_start)\n",
    "        pp = DataProcedure(\n",
    "                archive_dir,\n",
    "                True,\n",
    "                preprocessFromPandas_label_dir_pairs,\n",
    "                label_dir_pairs,\n",
    "                proc_start,\n",
    "                proc_num,\n",
    "                object_profiles,\n",
    "                observ_types\n",
    "            )\n",
    "        procs.append(pp)\n",
    "        #print(proc_start, samples_per_label, stride)\n",
    "        if(verbose >= 1):\n",
    "            num_lables = len(label_dir_pairs)\n",
    "            print(\"   From %r labels in range(%r,%r) for %rx%r = %r Samples\"\n",
    "                     % (num_lables,proc_start, proc_start+proc_num, num_lables,proc_num,num_lables*proc_num))\n",
    "    #print([p.hash() for p in procs])\n",
    "    return procs\n",
    "\n",
    "class dataFetchThread(threading.Thread):\n",
    "\n",
    "    def __init__(self, proc, group=None, target=None, name=None,\n",
    "                 args=(), kwargs=None, verbose=None):\n",
    "        threading.Thread.__init__(self, group=group, target=target, name=name,\n",
    "                                  verbose=verbose)\n",
    "        self.proc = proc\n",
    "        self.args = args\n",
    "        self.kwargs = kwargs\n",
    "        self.X = None\n",
    "        self.Y = None\n",
    "        return\n",
    "\n",
    "    def run(self):\n",
    "        self.X, self.Y = self.proc.getData()\n",
    "        return\n",
    "\n",
    "def genFromPPs(pps, batch_size, threading=False):\n",
    "    for pp in pps:\n",
    "        if(isinstance(pp, DataProcedure) == False):\n",
    "            raise TypeError(\"Only takes DataProcedure got\" % type(pp))\n",
    "            \n",
    "    \n",
    "    while True:\n",
    "        if(threading == True):\n",
    "            datafetch = dataFetchThread(pps[0])\n",
    "            datafetch.start()\n",
    "        for i in range(0,len(pps)):\n",
    "            if(threading == True):\n",
    "                #Wait for the data to come in\n",
    "                while(datafetch.isAlive()):\n",
    "                    pass\n",
    "                X,Y = datafetch.X, datafetch.Y\n",
    "\n",
    "                #Start the next dataFetch\n",
    "                if(i != len(pps)-1):\n",
    "                    datafetch = dataFetchThread(pps[i+1])\n",
    "                else:\n",
    "                    datafetch = dataFetchThread(pps[0])\n",
    "                datafetch.start()\n",
    "            else:\n",
    "                X,Y = pps[i].getData()\n",
    "                                   \n",
    "            if(isinstance(X,list) == False): X = [X]\n",
    "            if(isinstance(Y,list) == False): Y = [Y]\n",
    "            tot = Y[0].shape[0]\n",
    "            assert tot == X[0].shape[0]\n",
    "            for start in range(0, tot, batch_size):\n",
    "                end = start+min(batch_size, tot-start)\n",
    "                yield [x[start:end] for x in X], [y[start:end] for y in Y]\n",
    "                \n",
    "def genFromPPs_noThreading(pps, batch_size):\n",
    "    for pp in pps:\n",
    "        if(isinstance(pp, DataProcedure) == False):\n",
    "            raise TypeError(\"Only takes DataProcedure got\" % type(pp))\n",
    "    while True:\n",
    "        for i in range(0,len(pps)):            \n",
    "            X,Y = pps[i].getData()\n",
    "            if(isinstance(X,list) == False): X = [X]\n",
    "            if(isinstance(Y,list) == False): Y = [Y]\n",
    "            tot = Y[0].shape[0]\n",
    "            assert tot == X[0].shape[0]\n",
    "            for start in range(0, tot, batch_size):\n",
    "                end = start+min(batch_size, tot-start)\n",
    "                yield [x[start:end] for x in X], [y[start:end] for y in Y]\n",
    "\n",
    "def genFrom_label_dir_pairs(start, samples_per_label, stride, batch_size, archive_dir,label_dir_pairs, object_profiles, observ_types):\n",
    "    pps = procsFrom_label_dir_pairs(start,\n",
    "                                    samples_per_label,\n",
    "                                    stride,\n",
    "                                    archive_dir,\n",
    "                                    label_dir_pairs,\n",
    "                                    object_profiles,\n",
    "                                    observ_types)\n",
    "    gen = genFromPPs(pps, batch_size, threading = False)\n",
    "    return (gen, len(label_dir_pairs)*samples_per_label)\n",
    "\n",
    "keras_archive = \"/data/shared/Delphes/keras_archive/\"\n",
    "SNs = start_num_fromSplits((.7,.15,.15), maxMutualLength(label_dir_pairs, object_profiles))\n",
    "print(SNs)\n",
    "gen_lambda =  lambda s : genFrom_label_dir_pairs(start=s[0],\n",
    "                                   samples_per_label=s[1],\n",
    "                                   stride=10000,\n",
    "                                    batch_size=100,\n",
    "                                   archive_dir=archive_dir,\n",
    "                                   label_dir_pairs=label_dir_pairs,\n",
    "                                   object_profiles=object_profiles,\n",
    "                                   observ_types=observ_types)\n",
    "train, val, test = tuple([gen_lambda(s) for s in SNs])\n",
    "\n",
    "train_procs = procsFrom_label_dir_pairs(start=SNs[0][0],\n",
    "                                   samples_per_label=SNs[0][1],\n",
    "                                   stride=10000,\n",
    "                                   archive_dir=archive_dir,\n",
    "                                   label_dir_pairs=label_dir_pairs,\n",
    "                                   object_profiles=object_profiles,\n",
    "                                   observ_types=observ_types)\n",
    "import threading\n",
    "\n",
    "\n",
    "\n",
    "#print(\"SOMTHEING\")\n",
    "#print(train_gen,val_gen,test_gen)\n",
    "#for X,Y in train[0]:\n",
    "    #print('\\r' + str(X[0].shape) + ',' + str(Y[0].shape),)\n",
    "#   pass\n",
    "#for X,Y in train_gen:\n",
    "#    pass\n",
    "    #print('\\r' + str(X[0].shape) + ',' + str(Y[0].shape),)\n",
    "    #sys.stdout.flush()\n",
    "#for X,Y in val_gen:\n",
    "#    print('\\r' + str(X[0].shape) + ',' + str(Y[0].shape),)\n",
    "#    sys.stdout.flush()\n",
    "#for X,Y in test_gen:\n",
    "#    print('\\r' + str(X[0].shape) + ',' + str(Y[0].shape),)\n",
    "#    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#X,Y = preprocessFromPandas_label_dir_pairs(label_dir_pairs, 128995, 301, object_profiles, observ_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "DataProcedure results '44f7c73ddbda7e3cbb436c464571f84bfeca3562' read from archive\n",
      " 10900/195992 [>.............................] - ETA: 26s - loss: 1.6455 - acc: 0.6626Reading 10000 samples from '/data/shared/Delphes/ttbar_lepFilter_13TeV/pandas_unjoined/ttbar_lepFilter_13TeV_1.h5':\n",
      "Mapping 5 Values/Sample from 'Electron'\n",
      " 17100/195992 [=>............................] - ETA: 57s - loss: 1.2425 - acc: 0.7038Mapping 5 Values/Sample from 'MuonTight'\n",
      " 20000/195992 [==>...........................] - ETA: 63s - loss: 1.1315 - acc: 0.7177"
     ]
    }
   ],
   "source": [
    "def genModel(depth, width):\n",
    "    inputs = [None] * len(object_profiles)\n",
    "    fist_layer = [None] * len(object_profiles)\n",
    "    #Build Inputs\n",
    "    for i,profile in enumerate(object_profiles):\n",
    "        key = profile.name\n",
    "        max_size = profile.max_size\n",
    "        inp = Input(shape=(max_size, vecsize), name=key)\n",
    "        inputs[i] = inp\n",
    "        fist_layer[i] = Flatten()(inp)\n",
    "    #Merge inputs\n",
    "    merged = merge(fist_layer, mode='concat', name=\"merge\")\n",
    "    a = merged\n",
    "    v = [width] * depth\n",
    "    for j in range(len(v)):\n",
    "        a = Dense(v[j], activation='relu', name=\"dense_2_\"+str(j))(a)\n",
    "    second_dense = a \n",
    "    \n",
    "    #Create dense sigmoid layer for classification\n",
    "    dense_out = Dense(len(label_dir_pairs), activation='sigmoid', name='main_output')(second_dense)\n",
    "    dense = Model(input=inputs, output=dense_out, name=\"dense\")\n",
    "    return dense\n",
    "\n",
    "#Get the training data\n",
    "#X_train, Y_train = preprocessFromPandas_label_dir_pairs(label_dir_pairs,\n",
    "#                                                        sample_start,\n",
    "#                                                        samples_per_label,\n",
    "#                                                        object_profiles,\n",
    "#                                                        observ_types)\n",
    "\n",
    "#Generate the model from our function above\n",
    "model = genModel(3,100)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "          optimizer='rmsprop',\n",
    "          metrics=['accuracy']\n",
    "              )\n",
    "\n",
    "#VERY FEW EPOCHS FOR ILLUSTRATIVE REASONS\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    "\n",
    "earlystopping = EarlyStopping(patience=10, verbose=1)\n",
    "\n",
    "start_time = time.clock()\n",
    "history = model.fit_generator(generator=train[0],\n",
    "                samples_per_epoch=train[1],\n",
    "                nb_epoch=epochs,\n",
    "                max_q_size=100,\n",
    "                #nb_workers=2,\n",
    "                validation_data=val[0],\n",
    "                nb_val_samples=val[1],\n",
    "                #YOU SHOULD REALLY USE CALLBACKS AND INCREASE THE # of EPOCHS TO LIKE 100\n",
    "                callbacks=[earlystopping])\n",
    "\n",
    "#for proc in train_procs:\n",
    "#    X,Y = proc.get_XY()\n",
    "#    history = model.fit(X,Y,batch_size=batch_size,\n",
    "#                 nb_epoch=epochs,\n",
    "#                 validation_split=.15)\n",
    "\n",
    "plot_history([(\"MySimpleModel\", history)])\n",
    "dot = plot(model, to_file=\"MySimpleModel.png\", show_shapes=True, show_layer_names=False)\n",
    "display(Image(\"MySimpleModel.png\"))\n",
    "print(\"ElapseTime:\", time.clock()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.evaluate_generator(test[0], test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}