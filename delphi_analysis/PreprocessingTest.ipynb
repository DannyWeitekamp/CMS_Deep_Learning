{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-183-ff8cd1375e3c>, line 151)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-183-ff8cd1375e3c>\"\u001b[1;36m, line \u001b[1;32m151\u001b[0m\n\u001b[1;33m    if(profile.query != None):\u001b[0m\n\u001b[1;37m                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import threading\n",
    "#from .archiving import DataProcedure, KerasTrial\n",
    "#from .meta import msgpack_assertMeta\n",
    "#from ..layers.lorentz import Lorentz\n",
    "#from ..layers.slice import Slice\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import socket\n",
    "import time\n",
    "if not \"/data/shared/Software/\" in sys.path:\n",
    "    sys.path.append(\"/data/shared/Software/\")\n",
    "from CMS_SURF_2016.utils.preprocessing import getFiles_StoreType,getNumValFrame\n",
    "DEFAULT_PROFILE = {\n",
    "                        \"name\" : \" \",\n",
    "                        \"max_size\" : 100,\n",
    "                        \"pre_sort_columns\" : None,\n",
    "                        \"pre_sort_ascending\" :True,\n",
    "                        \"sort_columns\" : None,\n",
    "                        \"sort_ascending\" : True,\n",
    "                        \"query\" : None,\n",
    "                        \"shuffle\" : False,\n",
    "                        \"addColumns\" : None}\n",
    "class ObjectProfile():\n",
    "    \n",
    "\n",
    "\n",
    "    def __init__(self, *args, **kargs):\n",
    "        ''' An object containing processing instructions for each observable object type\n",
    "            #unkeyed Arguements:\n",
    "                name       -- The name of the data type (i.e. Electron, Photon, EFlowTrack, etc.)\n",
    "                max_size   -- The maximum number of objects to use in training\n",
    "            #keyed Arguements\n",
    "                pre_sort_columns -- What columns to sort before cutting on max_size (See pandas.DataFrame.sort)\n",
    "                pre_sort_ascending -- Whether each column will be sorted ascending or decending before cutting on max_size (See pandas.DataFrame.sort)\n",
    "                sort_columns -- What columns to sort on after processing (See pandas.DataFrame.sort)\n",
    "                sort_ascending -- Whether each column will be sorted ascending or decending after processing (See pandas.DataFrame.sort)\n",
    "                query        -- A selection query string to use before truncating the data (See pands.DataFrame.query)\n",
    "                shuffle     -- Whether or not to shuffle the data\n",
    "                addColumns -- A dictionary with single constant floats or integers to fill an additional column in the table.\n",
    "                             This column should be in observ_types if it is used with preprocessFromPandas_label_dir_pairs\n",
    "        '''\n",
    "        d = {}\n",
    "        if(isinstance(args[0], dict)):\n",
    "            d = args[0]\n",
    "        elif(isinstance(args[0], str)):\n",
    "            d[\"name\"] = args[0]\n",
    "            if(isinstance(args[1], int)):\n",
    "                d[\"max_size\"] = args[1]\n",
    "        if(len(args) > 2):\n",
    "            raise ValueError(\"Please explicitly name arguements with values %r\" % args[2:])\n",
    "\n",
    "        for key, value in DEFAULT_PROFILE.items():\n",
    "            # print(kargs.get(key, \"Nope\"),d.get(key, \"Nope\"), value)\n",
    "            setattr(self, key, kargs.get(key, d.get(key, value)))\n",
    "\n",
    "        if(self.max_size < -1):\n",
    "            raise ValueError(\"max_size cannot be less than -1. Got %r\" % self.max_size)\n",
    "        if(self.addColumns != None and not isinstance(self.addColumns, dict)):\n",
    "            raise ValueError(\"arguement addColumns must be a dictionary, but got %r\" % type(self.addColumns))\n",
    "       \n",
    "        self.class_name = self.__class__.__name__\n",
    "\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        main_clause = 'name:%r max_size=%r ' % (self.name, self.max_size)\n",
    "        sort_clause = ''\n",
    "        query_clause = ''\n",
    "        if(self.pre_sort_columns != None):\n",
    "            sort_clause = 'pre_sort_columns=%r pre_sort_ascending=%r ' % (self.pre_sort_columns, self.pre_sort_ascending)\n",
    "        if(self.sort_columns != None):\n",
    "            sort_clause = 'sort_columns=%r sort_ascending=%r ' % (self.sort_columns, self.sort_ascending)\n",
    "        if(self.query != None):\n",
    "            query_clause = 'query=%r ' % (self.query)\n",
    "        shuffle_clause = 'shuffle=%r' % self.shuffle\n",
    "\n",
    "        return main_clause + sort_clause + query_clause + shuffle_clause\n",
    "    \n",
    "    __repr__ = __str__\n",
    "\n",
    "#def padItem(x,max_size, vecsize, shuffle=False):\n",
    "#    '''A helper function that pads a numpy array up to MAX_SIZE or trucates it down to MAX_SIZE. If shuffle==True,\n",
    "#        shuffles the padded output before returning'''\n",
    "#    if(len(x) > max_size):\n",
    "#        out = x[:max_size]\n",
    "#    else:\n",
    "#        out = np.append(x ,np.array(np.zeros((max_size - len(x), vecsize))), axis=0)\n",
    "#    if(shuffle == True): np.random.shuffle(out)\n",
    "#    return out\n",
    "def _getStore(f, storeType):\n",
    "    '''Helper Function - Gets the HDFStore or frames for the file and storeType'''\n",
    "    frames = None\n",
    "    if(storeType == \"hdf5\"):\n",
    "        store = pd.HDFStore(f)\n",
    "    elif(storeType == \"msgpack\"):\n",
    "        print(\"Bulk reading .msg. Be patient, reading in slices not supported.\")\n",
    "        sys.stdout.flush()\n",
    "        #Need to check for latin encodings due to weird pandas default\n",
    "        try:\n",
    "            frames = pd.read_msgpack(f)\n",
    "        except UnicodeDecodeError as e:\n",
    "            frames = pd.read_msgpack(f, encoding='latin-1')\n",
    "    return store,frames\n",
    "def _getFrame(store, storeType, key, select_start, select_stop,\n",
    "              samples_to_read, file_total_entries, frames):\n",
    "    '''Helper Function - gets '''\n",
    "    if(storeType == \"hdf5\"):\n",
    "        #If we are reading all the samples use get since it might be faster\n",
    "        #TODO: check if it is actually faster\n",
    "        if(samples_to_read == file_total_entries):\n",
    "            frame = store.get('/'+key)\n",
    "        else:\n",
    "            frame = store.select('/'+key, start=select_start, stop=select_stop)\n",
    "    elif(storeType == \"msgpack\"):\n",
    "        frame = frames[key]\n",
    "        frame = frame[select_start:select_stop]\n",
    "    return frame\n",
    "\n",
    "def _groupsByEntry(f, storeType, samples_per_label, samples_to_read, file_total_entries, num_val_frame,file_start_read,object_profiles):\n",
    "    '''Helper Function - produces dict keyed by object type and filled with groupBy objects w.r.t Entry'''\n",
    "    store, frames = _getStore(f, storeType)\n",
    "\n",
    "    #Get information about how many rows there are for each entry for the rows we want to skip and read\n",
    "    skip_val_frame = num_val_frame[:file_start_read]\n",
    "    num_val_frame = num_val_frame[file_start_read : file_start_read+samples_to_read]\n",
    "\n",
    "\n",
    "    groupBys = {}\n",
    "    #Loop over every profile and read the corresponding tables in the pandas file\n",
    "    for index, profile in enumerate(object_profiles):\n",
    "        key = profile.name                \n",
    "        #Where to start reading the table based on the sum of the selection start \n",
    "        select_start = int(skip_val_frame[key].sum())\n",
    "        select_stop = select_start + int(num_val_frame[key].sum())\n",
    "\n",
    "        frame = _getFrame(store, storeType, key, select_start, select_stop,\n",
    "                          samples_to_read, file_total_entries,frames)\n",
    "        #Group by Entry\n",
    "        groupBys[key] = frame.groupby([\"Entry\"], group_keys=True)\n",
    "    return groupBys, store\n",
    "\n",
    "def _applyCuts(df, profile,vecsize):\n",
    "    '''Helper Function - presorts, applies queries, adds columns, and makes cuts'''\n",
    "    if(profile.pre_sort_columns != None):\n",
    "        df = df.sort(profile.pre_sort_columns, ascending=profile.pre_sort_ascending\n",
    "    if(profile.query != None):\n",
    "        df = df.query(profile.query)\n",
    "    #Add any additional columns\n",
    "    if(profile.addColumns != None):\n",
    "        for key, value in profile.addColumns.items():\n",
    "            df[key] = value\n",
    "    #Make cut, preserving only profile.max_size of top of table\n",
    "    df = df.head(profile.max_size)\n",
    "    #Only use observable columns\n",
    "    df = df[observ_types]\n",
    "    return df\n",
    "    \n",
    "def _padAndSort(df, profile,vecsize):\n",
    "    '''Helper Function - pads the data and sorts it'''\n",
    "    if(isinstance(df, type(None))):\n",
    "        #If a DataFrame does not exist for this entry then just inject zeros \n",
    "        x = np.array(np.zeros((profile.max_size, vecsize)))\n",
    "    else:\n",
    "        #Find sort_locs before we convert to np array\n",
    "        sort_locs = None\n",
    "        if(profile.sort_columns != None):\n",
    "            sort_locs = [df.columns.get_loc(s) for s in profile.sort_columns]\n",
    "        \n",
    "        #x is an np array not a DataFrame\n",
    "        x = df.values\n",
    "        \n",
    "        if(sort_locs != None):\n",
    "            for loc in reversed(sort_locs):\n",
    "                if(profile.sort_ascending == True):\n",
    "                    x = x[x[:,loc].argsort()]\n",
    "                else:\n",
    "                    x = x[x[:,loc].argsort()[::-1]]\n",
    "    \n",
    "        #pad the array\n",
    "        x = np.append(x ,np.array(np.zeros((profile.max_size - len(x), vecsize))), axis=0)\n",
    "    return x    \n",
    "\n",
    "def _initializeXY(single_list, label_dir_pairs, num_object_profiles, samples_per_label, num_labels):\n",
    "    '''Helper Function - Generates the initial data structures for the X (data) and Y (target)'''\n",
    "    label_vecs = {}\n",
    "    for i, (label, data_dir) in enumerate(label_dir_pairs):\n",
    "        arr = np.zeros((num_labels,))\n",
    "        arr[i] = 1\n",
    "        label_vecs[label] = arr\n",
    "        \n",
    "    if(single_list):\n",
    "        X_train = [None] * (samples_per_label * num_labels)\n",
    "        #global_profile = ObjectProfile(\"list\", max_size=\"\")\n",
    "    else:\n",
    "        X_train = [None] * (num_object_profiles)\n",
    "        #Prefill the arrays so that we don't waste time resizing lists\n",
    "        for index in range(num_object_profiles):\n",
    "            X_train[index] = [None] * (samples_per_label * num_labels)\n",
    "            \n",
    "    y_train = [None] * (samples_per_label * num_labels)\n",
    "    return X_train, y_train, label_vecs\n",
    "   \n",
    "def _check_Object_Profiles(object_profiles):\n",
    "    '''Helper Function - Makes sure that all ObjectProfiles are correctly formatted,\n",
    "        makes formatting corrections if necessary'''\n",
    "    for i,profile in enumerate(object_profiles):\n",
    "        if(isinstance(profile, dict) and profile.get('class_name', None) == \"ObjectProfile\"):\n",
    "            profile = ObjectProfile(profile)\n",
    "            object_profiles[i] = profile\n",
    "        if(profile.max_size == -1 or profile.max_size == None):\n",
    "            raise ValueError(\"ObjectProfile max_sizes must be resolved before preprocessing. \\\n",
    "                         Please first use: utils.preprocessing.resolveProfileMaxes(object_profiles, label_dir_pairs)\")\n",
    "        if(profile.addColumns != None):\n",
    "            for key, value in profile.addColumns.items():\n",
    "                if(not key in observ_types):\n",
    "                    raise ValueError(\"addColumn Key %r must be in observ_types\" % key)\n",
    "    return object_profiles\n",
    "\n",
    "def _check_inputs(label_dir_pairs, observ_types):\n",
    "    '''Helper Function - Makes sure that label_dir_pairs, and observ_types are correctly formatted'''\n",
    "    labels = [x[0] for x in label_dir_pairs]\n",
    "    duplicates = list(set([x for x in labels if labels.count(x) > 1]))\n",
    "    if(len(duplicates) != 0):\n",
    "        raise ValueError(\"Cannot have duplicate labels %r\" % duplicates)\n",
    "    if(\"Entry\" in observ_types):\n",
    "        raise ValueError(\"Using Entry in observ_types can result in skewed training results. Just don't.\")\n",
    "        \n",
    "def preprocessFromPandas_label_dir_pairs(label_dir_pairs,start, samples_per_label, object_profiles, observ_types,\n",
    "                                         single_list=False, sort_columns=None, sort_ascending=True,verbose=1):\n",
    "    '''Gets training data from folders of pandas tables\n",
    "        #Arguements:\n",
    "            label_dir_pairs -- a list of tuples of the form (label, directory) where the directory contains\n",
    "                                tables containing data of all the same event types.\n",
    "            start             --    Where to start reading (as if all of the files are part of one long list)\n",
    "            samples_per_label -- The number of samples to read for each label\n",
    "            object_profiles -- A list of ObjectProfile(s) corresponding to each type of observable object and\n",
    "                                its preprocessing steps. \n",
    "            observ_types    -- The column headers for the data to be read from the panadas table\n",
    "            single_list -- If True all object types are joined into a single list.\n",
    "            sort_columns -- If single_list the columns to sort by.\n",
    "            sort_ascending -- If True sort in ascending order, false decending  \n",
    "        #Returns:\n",
    "            Training data with its correspoinding labels\n",
    "            (X_train, Y_train)\n",
    "    '''\n",
    "    _check_inputs(label_dir_pairs, observ_types)\n",
    "    #Make sure that all the profile are proper objects and have resolved max_sizes\n",
    "    object_profiles = _check_Object_Profiles(object_profiles)\n",
    "    \n",
    "    vecsize = len(observ_types)\n",
    "    num_labels = len(label_dir_pairs)\n",
    "    \n",
    "    #Build vectors in the form [1,0,0], [0,1,0], [0, 0, 1] corresponding to each label\n",
    "    X_train, y_train, label_vecs = _initializeXY(single_list, label_dir_pairs, len(object_profiles), samples_per_label, num_labels)\n",
    "    X_train_index = 0\n",
    "    \n",
    "    #Loop over label dir pairs and get the file list for each directory\n",
    "    y_train_start = 0\n",
    "    for (label,data_dir) in label_dir_pairs:\n",
    "        files, storeType = getFiles_StoreType(data_dir)\n",
    "        files.sort()\n",
    "        samples_read = 0\n",
    "        location = 0\n",
    "        \n",
    "         #Loop the files associated with the current label\n",
    "        for f in files:\n",
    "            num_val_frame = getNumValFrame(f,storeType)\n",
    "\n",
    "            file_total_entries = len(num_val_frame.index)\n",
    "\n",
    "            assert file_total_entries > 0, \"num_val_frame has zero values\"\n",
    "            \n",
    "            if(location + file_total_entries <= start):\n",
    "                location += file_total_entries\n",
    "                continue\n",
    "            \n",
    "            #Determine what row to start reading the num_val table which contains\n",
    "            #information about how many rows there are for each entry\n",
    "            file_start_read = start-location if start > location else 0\n",
    "            \n",
    "            #How many rows we will read from this table each corresponds to one entry\n",
    "            samples_to_read = min(samples_per_label-samples_read, file_total_entries-file_start_read)\n",
    "            assert samples_to_read >= 0\n",
    "            \n",
    "            if(verbose >= 1): print(\"Reading %r samples from %r:\" % (samples_to_read,f))\n",
    "            \n",
    "            groupBys,store = _groupsByEntry(f, storeType, samples_per_label,samples_to_read, file_total_entries,\n",
    "                                            num_val_frame,file_start_read,object_profiles)\n",
    "                \n",
    "            if(verbose >= 1): print(\"Values/Sample from: %r\" % {p.name: p.max_size for p in object_profiles})\n",
    "            \n",
    "            cut_tables = [None] * len(object_profiles)\n",
    "            last_time = time.clock()-1.0\n",
    "            prev_entry = file_start_read\n",
    "            for entry in range(file_start_read, file_start_read+samples_to_read):\n",
    "                #Make a pretty progress bar in the terminal\n",
    "                if(verbose >= 1):      \n",
    "                    c = time.clock() \n",
    "                    if(c > last_time + .25):\n",
    "                        percent = float(entry-file_start_read)/float(samples_to_read)\n",
    "                        sys.stdout.write('\\r')\n",
    "                        sys.stdout.write(\"[%-20s] %r/%r  %r(Entry/sec)\" % ('='*int(20*percent), entry, int(samples_to_read), 4 * (entry-prev_entry)))\n",
    "                        sys.stdout.flush()\n",
    "                        last_time = c\n",
    "                        prev_entry = entry\n",
    "                        \n",
    "                for index, profile in enumerate(object_profiles):\n",
    "                        #print(groupBys.keys())\n",
    "                        groupBy = groupBys[profile.name]\n",
    "                        if(entry in groupBy.groups):\n",
    "                            df = _applyCuts(groupBy.get_group(entry), profile, vecsize)\n",
    "                            cut_tables[index] = df\n",
    "                        else:\n",
    "                            cut_tables[index] = None\n",
    "                if(single_list):\n",
    "                    df = pd.concat(cut_tables)\n",
    "                    list_profile = ObjectProfile(\"single_list\",\n",
    "                                                sum([profile.max_size for profile in object_profiles]),\n",
    "                                                sort_columns=sort_columns,\n",
    "                                                sort_ascending=sort_ascending)    \n",
    "                                                    \n",
    "                    x  = _padAndSort(df,list_profile,vecsize)\n",
    "                    X_train[X_train_index + entry - file_start_read] = x\n",
    "                else:\n",
    "                    for index, profile in enumerate(object_profiles):\n",
    "                        arr = X_train[index]\n",
    "                        df = cut_tables[index]\n",
    "                        x  = _padAndSort(df,profile, vecsize)\n",
    "                        arr[X_train_index + entry - file_start_read] = x\n",
    "            \n",
    "            X_train_index += samples_to_read\n",
    "            \n",
    "            #Free this (probably not necessary)\n",
    "            num_val_frame = None\n",
    "            if(storeType == \"hdf5\"):\n",
    "                store.close()\n",
    "            location     += file_total_entries\n",
    "            samples_read += samples_to_read\n",
    "            if(verbose >= 1): print(\"*Read %r Samples of %r in range(%r, %r)\" % (samples_read, samples_per_label, start, samples_per_label+start))\n",
    "            if(samples_read >= samples_per_label):\n",
    "                if(verbose >= 1): print('-' * 50)\n",
    "                assert samples_read == samples_per_label\n",
    "                break\n",
    "        if(samples_read != samples_per_label):\n",
    "            raise IOError(\"Not enough data in %r to read in range(%r, %r)\" % (data_dir, start, samples_per_label+start))\n",
    "        \n",
    "        #Generate the target data as vectors like [1,0,0], [0,1,0], [0,0,1]\n",
    "        for i in range(samples_per_label):\n",
    "            y_train[y_train_start+i] = label_vecs[label]\n",
    "        y_train_start += samples_per_label\n",
    "    \n",
    "    #Turn everything into numpy arrays and shuffle them just in case.\n",
    "    #Although, we probably don't need to shuffle since keras shuffles by default.\n",
    "    y_train = np.array(y_train)\n",
    "    \n",
    "    indices = np.arange(len(y_train))\n",
    "    np.random.shuffle(indices)\n",
    "    if(single_list):\n",
    "        X_train = np.array(X_train)\n",
    "    else:\n",
    "        for index in range(len(X_train)):\n",
    "            X_train[index] = np.array(X_train[index])[indices]\n",
    "\n",
    "    y_train = y_train[indices]\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1000 samples from '/data/shared/Delphes/ttbar_lepFilter_13TeV/pandas_h5/ttbar_lepFilter_13TeV_0.h5':\n",
      "Values/Sample from: {'EFlowPhoton': 100, 'Electron': 100}\n",
      "[=================== ] 961/1000  160(Entry/sec)*Read 1000 Samples of 1000 in range(0, 1000)\n",
      "--------------------------------------------------\n",
      "Reading 1000 samples from '/data/shared/Delphes/wjets_lepFilter_13TeV/pandas_h5/wjets_lepFilter_13TeV_1.h5':\n",
      "Values/Sample from: {'EFlowPhoton': 100, 'Electron': 100}\n",
      "[=================== ] 962/1000  180(Entry/sec)*Read 1000 Samples of 1000 in range(0, 1000)\n",
      "--------------------------------------------------\n",
      "Reading 999 samples from '/data/shared/Delphes/qcd_lepFilter_13TeV/pandas_h5/qcd_lepFilter_13TeV_0.h5':\n",
      "Values/Sample from: {'EFlowPhoton': 100, 'Electron': 100}\n",
      "[=================== ] 966/999  164(Entry/sec)*Read 999 Samples of 1000 in range(0, 1000)\n",
      "Reading 1 samples from '/data/shared/Delphes/qcd_lepFilter_13TeV/pandas_h5/qcd_lepFilter_13TeV_1.h5':\n",
      "Values/Sample from: {'EFlowPhoton': 100, 'Electron': 100}\n",
      "[                    ] 0/1  0(Entry/sec)*Read 1000 Samples of 1000 in range(0, 1000)\n",
      "--------------------------------------------------\n",
      "Reading 1000 samples from '/data/shared/Delphes/ttbar_lepFilter_13TeV/pandas_h5/ttbar_lepFilter_13TeV_0.h5':\n",
      "Values/Sample from: {'EFlowPhoton': 100, 'Electron': 100}\n",
      "[================    ] 848/1000  140(Entry/sec)"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-163-28cecb6777dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m                   ] \n\u001b[0;32m     23\u001b[0m \u001b[0mX1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessFromPandas_label_dir_pairs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_dir_pairs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobject_profiles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobserv_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mX2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessFromPandas_label_dir_pairs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_dir_pairs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobject_profiles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobserv_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msingle_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-162-d506a10ec31b>\u001b[0m in \u001b[0;36mpreprocessFromPandas_label_dir_pairs\u001b[1;34m(label_dir_pairs, start, samples_per_label, object_profiles, observ_types, single_list, sort_columns, sort_ascending, verbose)\u001b[0m\n\u001b[0;32m    304\u001b[0m                         \u001b[0mgroupBy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroupBys\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m                         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentry\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgroupBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 306\u001b[1;33m                             \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_applyCuts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprofile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvecsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    307\u001b[0m                             \u001b[0mcut_tables\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m                         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/core/groupby.pyc\u001b[0m in \u001b[0;36mget_group\u001b[1;34m(self, name, obj)\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m         \u001b[0minds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, indices, axis, convert, is_copy)\u001b[0m\n\u001b[0;32m   1356\u001b[0m         new_data = self._data.take(indices,\n\u001b[0;32m   1357\u001b[0m                                    \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1358\u001b[1;33m                                    convert=True, verify=True)\n\u001b[0m\u001b[0;32m   1359\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, indexer, axis, verify, convert)\u001b[0m\n\u001b[0;32m   3271\u001b[0m                                 'the axis length')\n\u001b[0;32m   3272\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3273\u001b[1;33m         \u001b[0mnew_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3274\u001b[0m         return self.reindex_indexer(new_axis=new_labels, indexer=indexer,\n\u001b[0;32m   3275\u001b[0m                                     axis=axis, allow_dups=True)\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/core/index.pyc\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, indexer, axis)\u001b[0m\n\u001b[0;32m   1156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1157\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_platform_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1158\u001b[1;33m         \u001b[0mtaken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1160\u001b[0m         \u001b[1;31m# by definition cannot propogate freq\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#from CMS_SURF_2016.utils.preprocessing import *\n",
    "\n",
    "\n",
    "\n",
    "#The observables taken from the table\n",
    "observ_types = ['E/c', 'Px', 'Py', 'Pz', 'PT_ET','Eta', 'Phi', 'Charge', 'X', 'Y', 'Z',\\\n",
    "                     'Dxy', 'Ehad', 'Eem', 'MuIso', 'EleIso', 'ChHadIso','NeuHadIso','GammaIso', \"ObjType\"]\n",
    "vecsize = len(observ_types)\n",
    "epochs = 30\n",
    "batch_size = 100\n",
    "\n",
    "label_dir_pairs = \\\n",
    "            [   (\"ttbar\", \"/data/shared/Delphes/ttbar_lepFilter_13TeV/pandas_h5/\"),\n",
    "                (\"wjet\", \"/data/shared/Delphes/wjets_lepFilter_13TeV/pandas_h5/\"),\n",
    "                (\"qcd\", \"/data/shared/Delphes/qcd_lepFilter_13TeV/pandas_h5/\")\n",
    "            ]\n",
    "object_profiles = [ObjectProfile(\"EFlowPhoton\",100, pre_sort_columns=[\"PT_ET\"],\n",
    "                                 pre_sort_ascending=False, sort_columns=[\"Phi\"],\n",
    "                                 addColumns={\"ObjType\":1}),\n",
    "                   ObjectProfile(\"Electron\",100, pre_sort_columns=[\"PT_ET\"],\n",
    "                                 pre_sort_ascending=False, sort_columns=[\"Phi\"],\n",
    "                                 addColumns={\"ObjType\":1})\n",
    "                  \n",
    "                  ] \n",
    "X1, Y1 = preprocessFromPandas_label_dir_pairs(label_dir_pairs,0, 1000, object_profiles, observ_types, verbose=1)\n",
    "X2, Y2 = preprocessFromPandas_label_dir_pairs(label_dir_pairs,0, 1000, object_profiles, observ_types, single_list=True,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.64259722e+01  -1.21117621e+00  -1.98987888e-01 ...,   0.00000000e+00\n",
      "   -9.62650660e-01   1.00000000e+00]\n",
      " [  4.85319116e+01  -1.44196159e+00  -3.84855252e-01 ...,   1.55532244e-01\n",
      "   -8.66347893e-01   1.00000000e+00]\n",
      " [  6.36782964e+01  -9.73977580e-01  -3.98754945e-01 ...,   5.82509306e-02\n",
      "   -8.61783705e-01   1.00000000e+00]\n",
      " ..., \n",
      " [  7.67499237e+01  -1.32991701e+00   1.19275905e-01 ...,   1.39568590e-01\n",
      "   -9.07536685e-01   1.00000000e+00]\n",
      " [  3.07067674e+01  -1.11861911e+00   7.69236240e-02 ...,   2.16747297e-01\n",
      "   -7.87645092e-01   1.00000000e+00]\n",
      " [  1.98119325e+01  -2.02595219e+00   1.06812292e-01 ...,   1.17052374e-01\n",
      "   -9.57811825e-01   1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(X1[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-165-8e228f4c5201>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X2' is not defined"
     ]
    }
   ],
   "source": [
    "print(X2[0], X2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((0, 4), (1, 1, 4))\n",
      "((0, 4), (1, 1, 4))\n",
      "((0, 4), (1, 1, 4))\n",
      "Reading 5 samples from '/tmp/fake_delphes/ttbar/000.h5':\n",
      "Values/Sample from: {'EFlowPhoton': 20, 'MuonTight': 10, 'Electron': 10, 'MET': 1, 'EFlowTracks': 20, 'EFlowNeutralHadron': 20}\n",
      "[================    ] 4/5  8(Entry/sec)*Read 5 Samples of 5 in range(0, 5)\n",
      "--------------------------------------------------\n",
      "Reading 5 samples from '/tmp/fake_delphes/wjet/000.h5':\n",
      "Values/Sample from: {'EFlowPhoton': 20, 'MuonTight': 10, 'Electron': 10, 'MET': 1, 'EFlowTracks': 20, 'EFlowNeutralHadron': 20}\n",
      "[============        ] 3/5  4(Entry/sec)*Read 5 Samples of 5 in range(0, 5)\n",
      "--------------------------------------------------\n",
      "Reading 5 samples from '/tmp/fake_delphes/qcd/000.h5':\n",
      "Values/Sample from: {'EFlowPhoton': 20, 'MuonTight': 10, 'Electron': 10, 'MET': 1, 'EFlowTracks': 20, 'EFlowNeutralHadron': 20}\n",
      "[================    ] 4/5  8(Entry/sec)*Read 5 Samples of 5 in range(0, 5)\n",
      "--------------------------------------------------\n",
      "Reading 5 samples from '/tmp/fake_delphes/ttbar/000.h5':\n",
      "Values/Sample from: {'EFlowPhoton': 20, 'MuonTight': 10, 'Electron': 10, 'MET': 1, 'EFlowTracks': 20, 'EFlowNeutralHadron': 20}\n",
      "[================    ] 4/5  8(Entry/sec)*Read 5 Samples of 5 in range(0, 5)\n",
      "--------------------------------------------------\n",
      "Reading 5 samples from '/tmp/fake_delphes/wjet/000.h5':\n",
      "Values/Sample from: {'EFlowPhoton': 20, 'MuonTight': 10, 'Electron': 10, 'MET': 1, 'EFlowTracks': 20, 'EFlowNeutralHadron': 20}\n",
      "[================    ] 4/5  8(Entry/sec)*Read 5 Samples of 5 in range(0, 5)\n",
      "--------------------------------------------------\n",
      "Reading 5 samples from '/tmp/fake_delphes/qcd/000.h5':\n",
      "Values/Sample from: {'EFlowPhoton': 20, 'MuonTight': 10, 'Electron': 10, 'MET': 1, 'EFlowTracks': 20, 'EFlowNeutralHadron': 20}\n",
      "[================    ] 4/5  8(Entry/sec)*Read 5 Samples of 5 in range(0, 5)\n",
      "--------------------------------------------------\n",
      "Reading 5 samples from '/tmp/fake_delphes/ttbar/000.h5':\n",
      "Values/Sample from: {'EFlowPhoton': 20, 'MuonTight': 10, 'Electron': 10, 'MET': 1, 'EFlowTracks': 20, 'EFlowNeutralHadron': 20}\n",
      "[============        ] 3/5  4(Entry/sec)*Read 5 Samples of 5 in range(0, 5)\n",
      "--------------------------------------------------\n",
      "Reading 5 samples from '/tmp/fake_delphes/wjet/000.h5':\n",
      "Values/Sample from: {'EFlowPhoton': 20, 'MuonTight': 10, 'Electron': 10, 'MET': 1, 'EFlowTracks': 20, 'EFlowNeutralHadron': 20}\n",
      "[================    ] 4/5  8(Entry/sec)*Read 5 Samples of 5 in range(0, 5)\n",
      "--------------------------------------------------\n",
      "Reading 5 samples from '/tmp/fake_delphes/qcd/000.h5':\n",
      "Values/Sample from: {'EFlowPhoton': 20, 'MuonTight': 10, 'Electron': 10, 'MET': 1, 'EFlowTracks': 20, 'EFlowNeutralHadron': 20}\n",
      "[================    ] 4/5  8(Entry/sec)*Read 5 Samples of 5 in range(0, 5)\n",
      "--------------------------------------------------\n",
      "Reading 5 samples from '/tmp/fake_delphes/ttbar/000.h5':\n",
      "Values/Sample from: {'EFlowPhoton': 20, 'MuonTight': 10, 'Electron': 10, 'MET': 1, 'EFlowTracks': 20, 'EFlowNeutralHadron': 20}\n",
      "[================    ] 4/5  8(Entry/sec)*Read 5 Samples of 5 in range(0, 5)\n",
      "--------------------------------------------------\n",
      "Reading 5 samples from '/tmp/fake_delphes/wjet/000.h5':\n",
      "Values/Sample from: {'EFlowPhoton': 20, 'MuonTight': 10, 'Electron': 10, 'MET': 1, 'EFlowTracks': 20, 'EFlowNeutralHadron': 20}\n",
      "[================    ] 4/5  8(Entry/sec)*Read 5 Samples of 5 in range(0, 5)\n",
      "--------------------------------------------------\n",
      "Reading 5 samples from '/tmp/fake_delphes/qcd/000.h5':\n",
      "Values/Sample from: {'EFlowPhoton': 20, 'MuonTight': 10, 'Electron': 10, 'MET': 1, 'EFlowTracks': 20, 'EFlowNeutralHadron': 20}\n",
      "[================    ] 4/5  8(Entry/sec)*Read 5 Samples of 5 in range(0, 5)\n",
      "--------------------------------------------------\n",
      "((0, 4), (1, 1, 4))\n",
      "((0, 4), (1, 1, 4))\n",
      "((0, 4), (1, 1, 4))\n",
      "((0, 4), (1, 1, 4))\n",
      "((0, 4), (1, 1, 4))\n",
      "((0, 4), (1, 1, 4))\n",
      "((0, 4), (1, 1, 4))\n",
      "((0, 4), (1, 1, 4))\n",
      "((0, 4), (1, 1, 4))\n",
      "((0, 4), (1, 1, 4))\n",
      "((0, 4), (1, 1, 4))\n",
      "((0, 4), (1, 1, 4))\n",
      "Reading 5 samples from '/tmp/fake_delphes/ttbar/001.h5':\n",
      "Values/Sample from: {'EFlowPhoton': 20, 'MuonTight': 10, 'Electron': 10, 'MET': 1, 'EFlowTracks': 20, 'EFlowNeutralHadron': 20}\n",
      "[========            ] 2/5  8(Entry/sec)*Read 5 Samples of 15 in range(5, 20)\n",
      "Reading 5 samples from '/tmp/fake_delphes/ttbar/002.h5':\n",
      "Values/Sample from: {'EFlowPhoton': 20, 'MuonTight': 10, 'Electron': 10, 'MET': 1, 'EFlowTracks': 20, 'EFlowNeutralHadron': 20}\n",
      "[========            ] 2/5  8(Entry/sec)*Read 10 Samples of 15 in range(5, 20)\n",
      "Reading 5 samples from '/tmp/fake_delphes/ttbar/003.h5':\n",
      "Values/Sample from: {'EFlowPhoton': 20, 'MuonTight': 10, 'Electron': 10, 'MET': 1, 'EFlowTracks': 20, 'EFlowNeutralHadron': 20}\n",
      "[========            ] 2/5  8(Entry/sec)*Read 15 Samples of 15 in range(5, 20)\n",
      "--------------------------------------------------\n",
      "Reading 5 samples from '/tmp/fake_delphes/wjet/001.h5':\n",
      "Values/Sample from: {'EFlowPhoton': 20, 'MuonTight': 10, 'Electron': 10, 'MET': 1, 'EFlowTracks': 20, 'EFlowNeutralHadron': 20}\n",
      "[========            ] 2/5  8(Entry/sec)*Read 5 Samples of 15 in range(5, 20)\n",
      "Reading 5 samples from '/tmp/fake_delphes/wjet/002.h5':\n",
      "Values/Sample from: {'EFlowPhoton': 20, 'MuonTight': 10, 'Electron': 10, 'MET': 1, 'EFlowTracks': 20, 'EFlowNeutralHadron': 20}\n",
      "[============        ] 3/5  8(Entry/sec)*Read 10 Samples of 15 in range(5, 20)\n",
      "Reading 5 samples from '/tmp/fake_delphes/wjet/003.h5':\n",
      "Values/Sample from: {'EFlowPhoton': 20, 'MuonTight': 10, 'Electron': 10, 'MET': 1, 'EFlowTracks': 20, 'EFlowNeutralHadron': 20}\n",
      "[================    ] 4/5  8(Entry/sec)*Read 15 Samples of 15 in range(5, 20)\n",
      "--------------------------------------------------\n",
      "Reading 5 samples from '/tmp/fake_delphes/qcd/001.h5':\n",
      "Values/Sample from: {'EFlowPhoton': 20, 'MuonTight': 10, 'Electron': 10, 'MET': 1, 'EFlowTracks': 20, 'EFlowNeutralHadron': 20}\n",
      "[============        ] 3/5  12(Entry/sec)*Read 5 Samples of 15 in range(5, 20)\n",
      "Reading 5 samples from '/tmp/fake_delphes/qcd/002.h5':\n",
      "Values/Sample from: {'EFlowPhoton': 20, 'MuonTight': 10, 'Electron': 10, 'MET': 1, 'EFlowTracks': 20, 'EFlowNeutralHadron': 20}\n",
      "[========            ] 2/5  8(Entry/sec)*Read 10 Samples of 15 in range(5, 20)\n",
      "Reading 5 samples from '/tmp/fake_delphes/qcd/003.h5':\n",
      "Values/Sample from: {'EFlowPhoton': 20, 'MuonTight': 10, 'Electron': 10, 'MET': 1, 'EFlowTracks': 20, 'EFlowNeutralHadron': 20}\n",
      "[============        ] 3/5  8(Entry/sec)*Read 15 Samples of 15 in range(5, 20)\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:151: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "import tempfile\n",
    "gen_observ_types = ['PT_ET','Eta', 'Phi']\n",
    "observ_types = gen_observ_types + [\"ObjType\"]\n",
    "\n",
    "obs_pl_t = [\"Entry\"] + gen_observ_types\n",
    "vecsize = len(obs_pl_t)\n",
    "RANDOM_SEED = 7\n",
    "np.random.seed(seed=RANDOM_SEED)\n",
    "\n",
    "object_profiles1 = [ObjectProfile(\"EFlowPhoton\",20, pre_sort_columns=[\"PT_ET\"],\n",
    "                                 pre_sort_ascending=False,\n",
    "                                 addColumns={\"ObjType\":1}),\n",
    "                   ObjectProfile(\"EFlowTracks\",20, pre_sort_columns=[\"PT_ET\"],\n",
    "                                 pre_sort_ascending=True,\n",
    "                                 addColumns={\"ObjType\":2}),\n",
    "                   ObjectProfile(\"EFlowNeutralHadron\",20, pre_sort_columns=[\"PT_ET\"],\n",
    "                                 pre_sort_ascending=False, sort_columns=[\"Phi\"], sort_ascending=True,\n",
    "                                 addColumns={\"ObjType\":3}),\n",
    "                   ObjectProfile(\"MET\",1, pre_sort_columns=[\"PT_ET\"],\n",
    "                                 pre_sort_ascending=False, sort_columns=[\"Phi\"],\n",
    "                                 addColumns={\"ObjType\":4}),\n",
    "                   ObjectProfile(\"MuonTight\",10, pre_sort_columns=[\"PT_ET\"],\n",
    "                                 pre_sort_ascending=False, sort_columns=[\"Phi\"], sort_ascending=False,\n",
    "                                 addColumns={\"ObjType\":5}),\n",
    "                   ObjectProfile(\"Electron\",10, #pre_sort_columns=[\"PT_ET\"],\n",
    "                                 pre_sort_ascending=False, sort_columns=[\"Phi\"],\n",
    "                                 addColumns={\"ObjType\":6})\n",
    "                  ]\n",
    "def rand_pl_entry(entry, a,b):\n",
    "    out = np.concatenate([np.full((a,1),entry, dtype='float64'), np.random.randn(a,b)], axis = 1)\n",
    "    return out\n",
    "def norm_uint(mean, std):\n",
    "    return max(int(np.random.normal(mean, std)),0)\n",
    "def fake_frames(N,object_profiles):\n",
    "    vecsize = len(obs_pl_t)\n",
    "    frames = {profile.name:pd.DataFrame(columns=obs_pl_t) for profile in object_profiles}\n",
    "    #print(frames.values()[0].shape, (1,1, vecsize))\n",
    "    num_val_dict = {key:[None]*N for key, frame in frames.items()}\n",
    "    for profile in object_profiles:\n",
    "        frames[profile.name] = pd.DataFrame(columns=obs_pl_t)\n",
    "    for entry in range(N):\n",
    "        #n = norm_uint(100,35)\n",
    "        n = norm_uint(3,1)\n",
    "        num_val_dict[\"EFlowPhoton\"][entry] = n       \n",
    "        frames[\"EFlowPhoton\"] = pd.concat([frames[\"EFlowPhoton\"],pd.DataFrame(rand_pl_entry(entry,n, vecsize-1), columns=obs_pl_t)])\n",
    "       \n",
    "        #n = norm_uint(120, 23)\n",
    "        n = norm_uint(3,1)\n",
    "        num_val_dict[\"EFlowTracks\"][entry] = n  \n",
    "        frames[\"EFlowTracks\"] = pd.concat([frames[\"EFlowTracks\"] ,pd.DataFrame(rand_pl_entry(entry,n, vecsize-1), columns=obs_pl_t)])\n",
    "        \n",
    "        #n = norm_uint(90, 27)\n",
    "        n = norm_uint(3,1)\n",
    "        num_val_dict[\"EFlowNeutralHadron\"][entry] = n\n",
    "        frames[\"EFlowNeutralHadron\"] = pd.concat([frames[\"EFlowNeutralHadron\"] ,pd.DataFrame(rand_pl_entry(entry,n, vecsize-1), columns=obs_pl_t)])\n",
    "\n",
    "        n = 1\n",
    "        num_val_dict[\"MET\"][entry] = n\n",
    "        frames[\"MET\"] = pd.concat([frames[\"MET\"] ,pd.DataFrame(rand_pl_entry(entry,n, vecsize-1), columns=obs_pl_t)])\n",
    "\n",
    "        n = int(np.random.uniform(0, 5))\n",
    "        num_val_dict[\"MuonTight\"][entry] = n\n",
    "        frames[\"MuonTight\"] = pd.concat([frames[\"MuonTight\"] ,pd.DataFrame(rand_pl_entry(entry,n, vecsize-1), columns=obs_pl_t)])\n",
    "\n",
    "        n = int(np.random.uniform(0, 5))\n",
    "        num_val_dict[\"Electron\"][entry] = n\n",
    "        frames[\"Electron\"] = pd.concat([frames[\"Electron\"] ,pd.DataFrame(rand_pl_entry(entry,n, vecsize-1), columns=obs_pl_t)])\n",
    "    frames[\"NumValues\"] = pd.DataFrame(num_val_dict)\n",
    "    #print([f.shape for f in frames.values()])\n",
    "    #raise ValueError()\n",
    "    return frames\n",
    "def store_frames(frames, filepath):\n",
    "    store = pd.HDFStore(filepath)\n",
    "    for key,frame in frames.items():\n",
    "        store.put(key, frame, format='table')\n",
    "        #print(filepath,key, frame.shape)\n",
    "    store.close()\n",
    "    #print(frames.keys())\n",
    "    store = pd.HDFStore(filepath)\n",
    "    #print(store.keys())\n",
    "def store_fake(directory, size, num, object_profiles):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    frames_list = [None]* num\n",
    "    for i in range(num):\n",
    "        frames = fake_frames(size, object_profiles)\n",
    "        store_frames(frames, directory+\"%03i.h5\" % i)\n",
    "        frames_list[i] = frames\n",
    "    return frames_list\n",
    "\n",
    "temp_dir = tempfile.gettempdir() + \"/fake_delphes/\"\n",
    "ttbar_dir = temp_dir + \"ttbar/\"\n",
    "wjet_dir = temp_dir + \"wjet/\"\n",
    "qcd_dir = temp_dir + \"qcd/\"\n",
    "frame_lists = {}\n",
    "label_dir_pairs = [(\"ttbar\", ttbar_dir), (\"wjet\", wjet_dir), (\"qcd\", qcd_dir)]\n",
    "\n",
    "import operator\n",
    "\n",
    "def justCheckSize(X, Y,sizes):\n",
    "    if(not np.array_equal(\n",
    "            np.array([x.shape for x in X]),sizes)):\n",
    "        print(\"Failed: data is wrong size\")\n",
    "    if(True in [x.dtype == np.dtype(object) for x in X]):\n",
    "        print(\"Failed: data is not square\")\n",
    "\n",
    "def checkGeneralSanity(X, Y, frame_lists, sizes,  NUM, label_dir_pairs):\n",
    "    is_single_list = False\n",
    "    if(not isinstance(X, list)):\n",
    "        is_single_list = True\n",
    "        X = [X]\n",
    "    justCheckSize(X, Y,sizes)\n",
    "    \n",
    "    all_values_by_label = {tup[0]:[None] * NUM for tup in label_dir_pairs}\n",
    "    \n",
    "    for entry in range(NUM):\n",
    "        for label, frame_list in frame_lists.items():\n",
    "            for f in frame_list:\n",
    "                f = {k: df.query(\"Entry == %r\" % entry) for k, df in f.items() if k != \"NumValues\"}\n",
    "                all_values_by_label[label][entry] =pd.concat([df for df in f.values()]) \n",
    "                \n",
    "    tn = [0]*5\n",
    "    tn[4] = {p[0]: 0 for p in label_dir_pairs}\n",
    "    z = np.zeros(vecsize)\n",
    "    for i in range(1,len(sizes)+1):\n",
    "        x = X[i-1]\n",
    "        for s in x:\n",
    "            was_zero = False\n",
    "            for row in s:\n",
    "                iszero = np.array_equal(row,z)\n",
    "                if(not iszero):\n",
    "                    if(was_zero):\n",
    "                        tn[0] = 1 \n",
    "                    #print(row[vecsize-1])\n",
    "                    if (not row[vecsize-1] == i):\n",
    "                        tn[1] = 1\n",
    "                if(iszero): \n",
    "                    was_zero = True\n",
    "                else:\n",
    "                    row = row[:-1]\n",
    "                    ok = 0\n",
    "                    for label, frame in all_values_by_label.items():\n",
    "                        frame = pd.concat(frame)\n",
    "                        frame = frame.drop(\"Entry\", axis=1)\n",
    "                        if((frame == row).all(1).any()):\n",
    "                            ok += 1\n",
    "                            tn[4][label] = True\n",
    "                    if(ok == 0):\n",
    "                        tn[2] = 3\n",
    "                    if(ok > 1):\n",
    "                        tn[3] = 3\n",
    "                \n",
    "    if(tn[0] != 0):        \n",
    "        print(\"Failed: padding not at end\")\n",
    "    if(tn[1] != 0 ^ is_single_list):       \n",
    "        print(\"Failed: Add column value incorrect\")\n",
    "    if(tn[2] != 0):\n",
    "        print(\"Failed: Data does not come from table\")\n",
    "    if(tn[3] != 0):\n",
    "        print(\"Failed: Row persists between samples of different classes\")\n",
    "    if(min(tn[4].values()) == 0):\n",
    "        print(\"Failed: Not all labels are used\")\n",
    "def checkCutsAndSorts(X, Y, frame_lists, sizes,  NUM, label_dir_pairs, object_profiles, observ_types, sort_columns=None, sort_ascending=True):\n",
    "    first = lambda x: x if not isinstance(x, list) else x[0]\n",
    "    is_single_list = False\n",
    "    if(not isinstance(X, list)):\n",
    "        is_single_list = True\n",
    "        if(sort_columns != None):\n",
    "            object_profiles = [ObjectProfile(\" \", 0, sort_columns=sort_columns, sort_ascending=sort_ascending)]\n",
    "        else:\n",
    "            return\n",
    "        X = [X]\n",
    "        \n",
    "    tn = [0]* 2\n",
    "    z = np.zeros(vecsize)\n",
    "    \n",
    "    for index, profile in enumerate(object_profiles):\n",
    "        x = X[index]\n",
    "        sort_index = None\n",
    "        ascending = False\n",
    "        try:\n",
    "            if(profile.sort_columns == None):\n",
    "                sort_index = observ_types.index(first(profile.pre_sort_columns))\n",
    "                ascending = profile.pre_sort_ascending\n",
    "                ti = 0\n",
    "            else:\n",
    "                sort_index = observ_types.index(first(profile.sort_columns))\n",
    "                ascending = profile.sort_ascending\n",
    "                ti = 1\n",
    "        except ValueError:\n",
    "            pass\n",
    "        if(sort_index != None):\n",
    "            for s in x:\n",
    "                prev_row = None\n",
    "                for row in s:\n",
    "                    iszero = np.array_equal(row,z)\n",
    "                    if(not isinstance(prev_row, type(None)) and not iszero):\n",
    "                        if(ascending):\n",
    "                            if(row[sort_index] < prev_row[sort_index]):\n",
    "                                tn[ti] = 1\n",
    "                        else:\n",
    "                            if(row[sort_index] > prev_row[sort_index]):\n",
    "                                tn[ti] = 1    \n",
    "                    prev_row = row\n",
    "        if(tn[0] != 0):\n",
    "            print(\"Failed: presorting incorrect.\")\n",
    "        if(tn[1] != 0):\n",
    "            print(\"Failed: sorting incorrect.\")\n",
    "def checkDuplicates(X, Y, object_profiles):\n",
    "    is_single_list = False\n",
    "    if(not isinstance(X, list)):\n",
    "        is_single_list = True\n",
    "        X = [X]\n",
    "    z = np.zeros(vecsize)\n",
    "    rows = []\n",
    "    for index, profile in enumerate(object_profiles):\n",
    "        x = X[index]\n",
    "        for s in x:\n",
    "            for row in s:\n",
    "                iszero = np.array_equal(row,z)\n",
    "                if(not iszero):\n",
    "                    rows.append(tuple(row))\n",
    "    if(len(set(rows)) != len(rows)):\n",
    "        print(\"Duplicate Found\")\n",
    "                                       \n",
    "NUM = 5\n",
    "frame_lists = {l:store_fake(d,NUM, 1, object_profiles1) for l, d in label_dir_pairs}\n",
    "OPS = object_profiles1\n",
    "\n",
    "X, Y = preprocessFromPandas_label_dir_pairs(label_dir_pairs,0, NUM, OPS, observ_types, verbose=1)\n",
    "sizes = np.array([[len(label_dir_pairs)*NUM, p.max_size, vecsize] for p in OPS])\n",
    "checkGeneralSanity(X, Y, frame_lists, sizes,  NUM, label_dir_pairs)\n",
    "checkCutsAndSorts(X, Y, frame_lists, sizes,  NUM, label_dir_pairs, OPS, observ_types)\n",
    "\n",
    "X, Y = preprocessFromPandas_label_dir_pairs(label_dir_pairs,0, NUM, OPS, observ_types, verbose=1, single_list=True)\n",
    "sizes = np.array([[len(label_dir_pairs)*NUM, sum([p.max_size for p in OPS]), vecsize]])\n",
    "checkGeneralSanity(X, Y, frame_lists, sizes,  NUM, label_dir_pairs)\n",
    "checkCutsAndSorts(X, Y, frame_lists, sizes,  NUM, label_dir_pairs, OPS, observ_types)\n",
    "\n",
    "X, Y = preprocessFromPandas_label_dir_pairs(label_dir_pairs,0, NUM, OPS, observ_types, verbose=1, single_list=True,\n",
    "                                            sort_columns=[\"Eta\"], sort_ascending=False)\n",
    "checkCutsAndSorts(X, Y, frame_lists, sizes,  NUM, label_dir_pairs, OPS, observ_types,\n",
    "                 sort_columns=[\"Eta\"], sort_ascending=False)\n",
    "\n",
    "X, Y = preprocessFromPandas_label_dir_pairs(label_dir_pairs,0, NUM, OPS, observ_types, verbose=1, single_list=True,\n",
    "                                            sort_columns=[\"Phi\"], sort_ascending=True)\n",
    "checkCutsAndSorts(X, Y, frame_lists, sizes,  NUM, label_dir_pairs, OPS, observ_types,\n",
    "                  sort_columns=[\"Phi\"], sort_ascending=True)\n",
    "\n",
    "NUM = 15\n",
    "frame_lists = {l:store_fake(d,5, 4, object_profiles1) for l, d in label_dir_pairs}\n",
    "\n",
    "X, Y = preprocessFromPandas_label_dir_pairs(label_dir_pairs,5, NUM, OPS, observ_types, verbose=1)\n",
    "sizes = np.array([[len(label_dir_pairs)*NUM, p.max_size, vecsize] for p in OPS])\n",
    "justCheckSize(X,Y, sizes)\n",
    "checkDuplicates(X,Y,object_profiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys,os\n",
    "sys.path.append(os.path.realpath(\"../\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "from six import string_types\n",
    "from types import NoneType\n",
    "\n",
    "#if __package__ is None:\n",
    "\n",
    "    #sys.path.append(os.path.realpath(__file__+\"/../../../\"))\n",
    "\n",
    "\n",
    "import time, math,re,h5py,shutil\n",
    "import argparse\n",
    "from multiprocessing import Process\n",
    "from time import sleep\n",
    "from CMS_Deep_Learning.io import size_from_meta,get_sizes_meta_dict\n",
    "\n",
    "PARTICLE_OBSERVS = ['Energy', 'Px', 'Py', 'Pz', 'Pt', 'Eta', 'Phi', 'Charge',\n",
    "                    'ChPFIso', 'GammaPFIso', 'NeuPFIso',\n",
    "                    'isChHad', 'isEle', 'isGamma', 'isMu', 'isNeuHad',\n",
    "                    'vtxX', 'vtxY', 'vtxZ']\n",
    "HLF_OBSERVS = ['HT', 'MET', 'MT', 'PhiMET', 'bJets', 'nJets']\n",
    "#ROWS PER EVENT\n",
    "DEFAULT_RPE = {\"Particles\": 801, \"HLF\": 1}\n",
    "DEFAULT_OBSERVS = {\"Particles\": PARTICLE_OBSERVS, \"HLF\": HLF_OBSERVS }\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "\n",
    "#----------------------------IO-----------------------------\n",
    "def numpy_from_h5(f, file_start_read, samples_to_read, file_total_events=-1, format='numpy', observ_types=DEFAULT_OBSERVS, rows_per_event=DEFAULT_RPE):\n",
    "    '''Helper Function - Gets a numpy array from a pandas or numpy .h5 file\n",
    "    \n",
    "        :param f: The filepath of the pandas file\n",
    "        :type f: str\n",
    "        :param file_start_read: what samples to start reading with\n",
    "        :type file_start_read: uint\n",
    "        :param samples_to_read: the number of samples to read\n",
    "        :type samples_to_read: uint\n",
    "        :param file_total_events: the total events in the file (if you know it)\n",
    "        :type file_total_events: uint\n",
    "        :param observ_types: A dictionary of the features (ordered) to get from pandas for each data type\n",
    "        :type observ_types: dict\n",
    "        :param rows_per_event: A dictionary of the number of rows per event for each data type\n",
    "        :type rows_per_event: dict\n",
    "        :returns: the numpy array\n",
    "    '''\n",
    "    if(format == \"pandas\"):\n",
    "        store = pd.HDFStore(f)\n",
    "    else:\n",
    "        store = h5py.File(f)\n",
    "    \n",
    "    \n",
    "    values = {}\n",
    "    for key, rpe in rows_per_event.items():\n",
    "        # Where to start reading the table based on the sum of the selection start \n",
    "        select_start = file_start_read * rpe\n",
    "        select_stop = select_start + samples_to_read * rpe\n",
    "        \n",
    "        if(format == 'pandas'):\n",
    "            if (samples_to_read == file_total_events):\n",
    "                frame = store.get('/' + key)\n",
    "            else:\n",
    "                frame = store.select('/' + key, start=select_start, stop=select_stop)\n",
    "            columns = list(frame.columns)\n",
    "            x = frame.values\n",
    "        else:\n",
    "            if (samples_to_read == file_total_events):\n",
    "                x = store[key][:]\n",
    "            else:\n",
    "                x = store[key][select_start:select_stop]\n",
    "            columns = [\"EvtId\"] + DEFAULT_OBSERVS[key]\n",
    "\n",
    "        \n",
    "        if (observ_types != None):\n",
    "            evtIDS = x[:,columns.index(\"EvtId\")]\n",
    "            x = np.take(x, [columns.index(o) for o in observ_types[key]], axis=-1)\n",
    "        if (rpe > 1):\n",
    "            n_rows, n_columns = x.shape\n",
    "            x = x.reshape((n_rows / rpe, rpe, n_columns))\n",
    "            assert np.array([len(np.unique(y))==1 for y in evtIDS.reshape((n_rows / rpe, rpe))]).all(), \"FAIL, reshape does not correctly group event ids\"\n",
    "        values['sources'] = np.array([[f]*len(evtIDS), np.array(evtIDS,dtype='int')]).T\n",
    "        values[key] = x\n",
    "    store.close()\n",
    "    return values\n",
    "#------------------------------------------------------------\n",
    "\n",
    "\n",
    "#---------------------------HELPERS---------------------------\n",
    "def _gen_label_vecs(data_dirs):\n",
    "    num_labels = len(data_dirs)\n",
    "    label_vecs = {}\n",
    "    for i, data_dir in enumerate(data_dirs):\n",
    "        arr = np.zeros((num_labels,))\n",
    "        arr[i] = 1\n",
    "        label_vecs[data_dir] = arr\n",
    "    return label_vecs\n",
    "\n",
    "\n",
    "def _initializeArrays(data_dirs, samples_per_class):\n",
    "    '''Helper Function - Generates the initial data structures for the X (data) and Y (target)'''\n",
    "    num_classes = len(data_dirs)\n",
    "    X_train = [None] * (samples_per_class * num_classes)\n",
    "    y_train = [None] * (samples_per_class * num_classes)\n",
    "    HLF_train = [None] * (samples_per_class * num_classes)\n",
    "    sources_train = [None] * (samples_per_class * num_classes)\n",
    "    return X_train, y_train, HLF_train, sources_train\n",
    "#-------------------------------------------------------------\n",
    "\n",
    "\n",
    "def _check_inputs(data_dirs, observ_types):\n",
    "    '''Helper Function - Makes sure that data_dirs, and observ_types are correctly formatted'''\n",
    "    if (len(set(data_dirs)) != len(data_dirs)):\n",
    "        raise ValueError(\"Cannot have duplicate directories %r\" % data_dirs)\n",
    "    for x in observ_types.values():\n",
    "        if (\"EvtId\" in x):\n",
    "            raise ValueError(\"Using EvtId in observ_types can result in skewed training results. Just don't.\")\n",
    "\n",
    "\n",
    "\n",
    "#--------------------SORTING UTILS--------------------------------\n",
    "def maxLepPtEtaPhi(X, locs):\n",
    "    for x in X:\n",
    "        if (x[locs['isEle']] or x[locs[\"isMu\"]]):\n",
    "            return x[locs['Pt']], x[locs['Eta']], x[locs['Phi']]\n",
    "        \n",
    "def assertZerosBack(sort_slice, x, locs, sort_ascending):\n",
    "    from numpy import inf\n",
    "    sort_slice[np.all(x == 0.0, axis=1)] = inf if sort_ascending else -inf\n",
    "    return sort_slice\n",
    "\n",
    "def resolveMetric(s, locs, sort_ascending):\n",
    "    if s in SORT_METRICS:\n",
    "        return lambda x: assertZerosBack(SORT_METRICS[s](x, locs), x, locs, sort_ascending)\n",
    "    else:\n",
    "        raise ValueError(\"Unrecognized sorting metric %r\" % s)\n",
    "\n",
    "\n",
    "def _sortBy(x, sorts, sort_ascending):  \n",
    "    if (sorts != None):\n",
    "        for s in reversed(sorts):\n",
    "            if (isinstance(s, int)):\n",
    "                sort_slice = x[:, s]\n",
    "            else:\n",
    "                sort_slice = s(x)\n",
    "            if (sort_ascending == True):\n",
    "                x = x[sort_slice.argsort()]\n",
    "            else:\n",
    "                x = x[sort_slice.argsort()[::-1]]\n",
    "    return x\n",
    "\n",
    "\n",
    "def sort_numpy(x, sort_columns, sort_ascending, observ_types):\n",
    "    '''Helper Function - pads the data and sorts it'''\n",
    "    sort_locs = None\n",
    "    assert not isinstance(sort_columns, string_types), \"sort_columns improperly stored\"\n",
    "    if (sort_columns != None):\n",
    "        if (True in [c in sort_columns for c in [\"shuffle\", \"random\"]]):\n",
    "            np.random.shuffle(x)\n",
    "        elif (not None in sort_columns):\n",
    "            assert not False in [isinstance(s, string_types) for s in sort_columns], \\\n",
    "                \"Type should be string got %s\" % (\",\".join([str(type(s)) for s in sort_columns]))\n",
    "            locs = {t: s for s, t in enumerate(observ_types)}\n",
    "            sorts = [locs[s] if s in observ_types else resolveMetric(s, locs, sort_ascending)\n",
    "                     for s in sort_columns]\n",
    "            # KLUGE FIX\n",
    "            x[x[:, locs[\"Energy\"]] == 0] = 0.0\n",
    "            # Sort\n",
    "            x = _sortBy(x, sorts, sort_ascending)  \n",
    "\n",
    "    return x\n",
    "#------------------------------------------------------------------\n",
    "\n",
    "#-------------------------SORTINGS---------------------------------\n",
    "def MaxLepDeltaPhi(X, locs, mlpep=None):\n",
    "    maxLepPt, maxLepEta, maxLepPhi = maxLepPtEtaPhi(X, locs) if isinstance(mlpep, type(None)) else mlpep\n",
    "    out = maxLepPhi - X[:, locs[\"Phi\"]]\n",
    "\n",
    "    tooLarge = -2.0 * math.pi * (out > math.pi)\n",
    "    tooSmall = 2.0 * math.pi * (out < -math.pi)\n",
    "    out = out + tooLarge + tooSmall\n",
    "    return out\n",
    "\n",
    "\n",
    "def MaxLepDeltaEta(X, locs, mlpep=None):\n",
    "    maxLepPt, maxLepEta, maxLepPhi = maxLepPtEtaPhi(X, locs) if isinstance(mlpep, type(None)) else mlpep\n",
    "    return maxLepEta - X[:, locs[\"Eta\"]]\n",
    "\n",
    "\n",
    "def MaxLepDeltaR(X, locs, mlpep=None):\n",
    "    mlpep = maxLepPtEtaPhi(X, locs) if isinstance(mlpep, type(None)) else mlpep\n",
    "    return np.sqrt(MaxLepDeltaPhi(X, locs, mlpep) ** 2 + MaxLepDeltaEta(X, locs, mlpep) ** 2)\n",
    "\n",
    "\n",
    "def MaxLepKt(X, locs):\n",
    "    mlpep = maxLepPtEtaPhi(X, locs)\n",
    "    maxLepPt, maxLepEta, maxLepPhi = mlpep\n",
    "    return np.minimum(X[:, locs[\"Pt\"]] ** 2, maxLepPt ** 2) * MaxLepDeltaR(X, locs, mlpep) ** 2\n",
    "\n",
    "\n",
    "def MaxLepAntiKt(X, locs):\n",
    "    mlpep = maxLepPtEtaPhi(X, locs)\n",
    "    maxLepPt, maxLepEta, maxLepPhi = mlpep\n",
    "    return np.minimum(X[:, locs[\"Pt\"]] ** -2, maxLepPt ** -2) * MaxLepDeltaR(X, locs, mlpep) ** 2\n",
    "\n",
    "\n",
    "SORT_METRICS = {f.__name__: f for f in\n",
    "                [MaxLepDeltaPhi, MaxLepDeltaEta, MaxLepDeltaR, MaxLepKt, MaxLepAntiKt]}\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import glob\n",
    "\n",
    "\n",
    "def pandas_to_numpy(data_dirs, start, samples_per_class,\n",
    "                    observ_types=DEFAULT_OBSERVS, sort_columns=None, sort_ascending=True, particle_mean=None, particle_std=None, hlf_mean=None, hlf_std=None, verbose=1):\n",
    "    '''Builds a trainable (particle level) sorted and (event level) shuffled numpy array from directories of pandas .h5 files.\n",
    "    \n",
    "        :param data_dirs: \n",
    "            A list of pandas directories containing pandas .h5 files, tuples of ('label','dir'),\n",
    "            or dictionary with .values() equal to such a list. The order indicates which\n",
    "            files correspond to which output (i.e. the first directory corresponds to\n",
    "            [1,0,...,0] and the second to [0,1,...,0], etc.). For dictionaries the \n",
    "            order defaults to the alphabetical order of the directory names.\n",
    "        :param start:        Where to start reading (as if all of the files in a given directory are part of one long list)\n",
    "        :param samples_per_class: The number of samples to read for each label. Every directory must have enough data starting\n",
    "                            from 'start'.\n",
    "        :param observ_types: The column headers for the data to be read from the panadas table. Also indicated the order of the columns.\n",
    "        :param sort_columns: The columns to sort by, or special quantities including [MaxLepDeltaPhi,\n",
    "                            MaxLepDeltaEta,MaxLepDeltaR,MaxLepKt,MaxLepAntiKt]\n",
    "        :param sort_ascending: If True sort in ascending order, false decending  \n",
    "        :param particle_mean: The mean of the particle features to be used for centering the data. Default None indicates no centering\n",
    "        :param particle_std: The std of the particle features to be used for standardizing the data.Default None indicates no standardization\n",
    "        :param hlf_mean: The mean of the HLF features to be used for centering the data. Default None indicates no centering\n",
    "        :param hlf_std: The std of the HLF features to be used for standardizing the data. Default None indicates no standardization\n",
    "        :returns: (X_train, Y_train, HFL_train) \n",
    "    '''\n",
    "    if (isinstance(data_dirs, dict)): data_dirs = sorted(data_dirs.values(), key=lambda x: x.join(x.split(\"/\")[::-1]))\n",
    "    if (isinstance(data_dirs[0], tuple)): data_dirs = [x[1] for x in data_dirs]\n",
    "    _check_inputs(data_dirs, observ_types)\n",
    "\n",
    "    label_vecs = _gen_label_vecs(data_dirs)\n",
    "    X_train, y_train, HLF_train,sources_train = _initializeArrays(data_dirs, samples_per_class)\n",
    "    X_train_index = 0\n",
    "\n",
    "    y_train_start = 0\n",
    "    for data_dir in data_dirs:\n",
    "        files = glob.glob(os.path.abspath(data_dir) + \"/*.h5\")\n",
    "        files.sort()\n",
    "        samples_read, location = 0, 0\n",
    "\n",
    "        sizesDict = get_sizes_meta_dict(data_dir)\n",
    "\n",
    "        last_time = time.clock() - 1.0\n",
    "        count,last_count = 0,0\n",
    "        # Loop the files associated with the current label\n",
    "        for f in files:\n",
    "            file_total_events = size_from_meta(f, sizesDict=sizesDict)  # len(num_val_frame.index)\n",
    "            if (file_total_events == None or file_total_events == 0):\n",
    "                print(\"Skipping %r\" % f)\n",
    "                continue\n",
    "\n",
    "            if (location + file_total_events <= start):\n",
    "                location += file_total_events\n",
    "                continue\n",
    "\n",
    "            # Determine what row to start reading the num_val table which contains\n",
    "            # information about how many rows there are for each entry\n",
    "            file_start_read = start - location if start > location else 0\n",
    "\n",
    "            # How many rows we will read from this table each corresponds to one entry\n",
    "            samples_to_read = min(samples_per_class - samples_read, file_total_events - file_start_read)\n",
    "            assert samples_to_read >= 0\n",
    "\n",
    "            d = numpy_from_h5(f, file_start_read=file_start_read,\n",
    "                                       samples_to_read=samples_to_read,\n",
    "                                       file_total_events=file_total_events,\n",
    "                                       rows_per_event=DEFAULT_RPE,\n",
    "                                       observ_types=observ_types)\n",
    "            Particles, HLF,sources = d[\"Particles\"], d[\"HLF\"],d[\"sources\"]\n",
    "\n",
    "            for s, (particles, hlf,source) in enumerate(zip(Particles, HLF,sources)):\n",
    "                # ----------pretty progress bar---------------\n",
    "                \n",
    "                if (verbose >= 1):\n",
    "                    c = time.clock()\n",
    "                    if (c > last_time + .25):\n",
    "                        prog = X_train_index + s\n",
    "                        percent = float(prog) / (samples_per_class * len(data_dirs))\n",
    "                        sys.stdout.write('\\r')\n",
    "                        sys.stdout.write(\"[%-20s] %r/%r  %r(Event/sec)\" % ('=' * int(20 * percent), prog,\n",
    "                                                                           int(samples_per_class) * len(data_dirs),\n",
    "                                                                           4 * (count-last_count)))\n",
    "                        sys.stdout.flush()\n",
    "                        #prev_sample = s\n",
    "                        last_time = c\n",
    "                        last_count = count\n",
    "\n",
    "                count += 1\n",
    "                # ------------------------------------------\n",
    "                particles = sort_numpy(particles, sort_columns, sort_ascending, observ_types[\"Particles\"])\n",
    "                \n",
    "                #-----------------STANDARDIZATION --------------------------------------\n",
    "                #Mask out the padding so that it is not altered during standarization\n",
    "                if(not isinstance(particle_mean,NoneType) or not isinstance(particle_std,NoneType)):\n",
    "                    mask = (particles == 0.0).all(axis=-1)\n",
    "                    mask_extended = np.expand_dims(mask, axis=-1)\n",
    "                    mask_extruded = np.repeat(mask_extended, len(PARTICLE_OBSERVS), axis=-1)\n",
    "                    particles = ma.masked_array(particles, mask=mask_extruded)\n",
    "                \n",
    "                #Apply standardization\n",
    "                if(not isinstance(particle_mean,NoneType)):\n",
    "                    particles = particles - particle_mean.reshape(1,len(PARTICLE_OBSERVS))\n",
    "                if(not isinstance(particle_std,NoneType)):\n",
    "                    particles = particles / particle_std.reshape(1,len(PARTICLE_OBSERVS))\n",
    "                if (not isinstance(hlf_mean, NoneType)):\n",
    "                    hlf = hlf - hlf_mean\n",
    "                if (not isinstance(hlf_std, NoneType)):\n",
    "                    hlf = hlf / hlf_std\n",
    "                #------------------------------------------------------------------------\n",
    "\n",
    "                X_train[X_train_index + s] = np.array(particles)\n",
    "                HLF_train[X_train_index + s] = hlf\n",
    "                sources_train[X_train_index + s] = source\n",
    "\n",
    "            X_train_index += samples_to_read\n",
    "\n",
    "            location += file_total_events\n",
    "            samples_read += samples_to_read\n",
    "            if (samples_read >= samples_per_class):\n",
    "                assert samples_read == samples_per_class\n",
    "                break\n",
    "        if (samples_read != samples_per_class):\n",
    "            raise IOError(\n",
    "                \"Not enough data in %r to read in range(%r, %r)\" % (data_dir, start, samples_per_class + start))\n",
    "\n",
    "        # Generate the target data as vectors like [1,0,0], [0,1,0], [0,0,1]\n",
    "        for i in range(samples_per_class):\n",
    "            y_train[y_train_start + i] = label_vecs[data_dir]\n",
    "        y_train_start += samples_per_class\n",
    "\n",
    "    # Turn everything into numpy arrays and shuffle them just in case.\n",
    "    # Although, we probably don't need to shuffle since keras shuffles by default.\n",
    "    y_train = np.array(y_train)\n",
    "\n",
    "    indices = np.arange(len(y_train))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    X_train = np.array(X_train)[indices]\n",
    "    HLF_train = np.array(HLF_train)[indices]\n",
    "    y_train = y_train[indices]\n",
    "    sources_train = np.array(sources_train)[indices]\n",
    "    \n",
    "\n",
    "    return X_train, y_train, HLF_train,sources_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==========          ] 10/20  0(Event/sec)"
     ]
    }
   ],
   "source": [
    "dirs = [\"/bigdata/shared/Delphes/REDUCED_IsoLep_NEW/qcd_lepFilter_13TeV/\",\n",
    "        \"/bigdata/shared/Delphes/REDUCED_IsoLep_NEW/wjets_lepFilter/\"\n",
    "       ]\n",
    "\n",
    "a,b,c,d = pandas_to_numpy(dirs,0,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[ '/bigdata/shared/Delphes/REDUCED_IsoLep_NEW/qcd_lepFilter_13TeV/qcd_lepFilter_13TeV_1000_0_TO_998.h5',\n",
      "        '4'],\n",
      "       [ '/bigdata/shared/Delphes/REDUCED_IsoLep_NEW/wjets_lepFilter/wjets_lepFilter_475_0_TO_99.h5',\n",
      "        '9'],\n",
      "       [ '/bigdata/shared/Delphes/REDUCED_IsoLep_NEW/qcd_lepFilter_13TeV/qcd_lepFilter_13TeV_1000_0_TO_998.h5',\n",
      "        '1'],\n",
      "       [ '/bigdata/shared/Delphes/REDUCED_IsoLep_NEW/wjets_lepFilter/wjets_lepFilter_475_0_TO_99.h5',\n",
      "        '0'],\n",
      "       [ '/bigdata/shared/Delphes/REDUCED_IsoLep_NEW/qcd_lepFilter_13TeV/qcd_lepFilter_13TeV_1000_0_TO_998.h5',\n",
      "        '8'],\n",
      "       [ '/bigdata/shared/Delphes/REDUCED_IsoLep_NEW/wjets_lepFilter/wjets_lepFilter_475_0_TO_99.h5',\n",
      "        '8'],\n",
      "       [ '/bigdata/shared/Delphes/REDUCED_IsoLep_NEW/wjets_lepFilter/wjets_lepFilter_475_0_TO_99.h5',\n",
      "        '5'],\n",
      "       [ '/bigdata/shared/Delphes/REDUCED_IsoLep_NEW/wjets_lepFilter/wjets_lepFilter_475_0_TO_99.h5',\n",
      "        '6'],\n",
      "       [ '/bigdata/shared/Delphes/REDUCED_IsoLep_NEW/wjets_lepFilter/wjets_lepFilter_475_0_TO_99.h5',\n",
      "        '4'],\n",
      "       [ '/bigdata/shared/Delphes/REDUCED_IsoLep_NEW/wjets_lepFilter/wjets_lepFilter_475_0_TO_99.h5',\n",
      "        '3'],\n",
      "       [ '/bigdata/shared/Delphes/REDUCED_IsoLep_NEW/qcd_lepFilter_13TeV/qcd_lepFilter_13TeV_1000_0_TO_998.h5',\n",
      "        '6'],\n",
      "       [ '/bigdata/shared/Delphes/REDUCED_IsoLep_NEW/qcd_lepFilter_13TeV/qcd_lepFilter_13TeV_1000_0_TO_998.h5',\n",
      "        '3'],\n",
      "       [ '/bigdata/shared/Delphes/REDUCED_IsoLep_NEW/qcd_lepFilter_13TeV/qcd_lepFilter_13TeV_1000_0_TO_998.h5',\n",
      "        '0'],\n",
      "       [ '/bigdata/shared/Delphes/REDUCED_IsoLep_NEW/wjets_lepFilter/wjets_lepFilter_475_0_TO_99.h5',\n",
      "        '7'],\n",
      "       [ '/bigdata/shared/Delphes/REDUCED_IsoLep_NEW/qcd_lepFilter_13TeV/qcd_lepFilter_13TeV_1000_0_TO_998.h5',\n",
      "        '9'],\n",
      "       [ '/bigdata/shared/Delphes/REDUCED_IsoLep_NEW/qcd_lepFilter_13TeV/qcd_lepFilter_13TeV_1000_0_TO_998.h5',\n",
      "        '7'],\n",
      "       [ '/bigdata/shared/Delphes/REDUCED_IsoLep_NEW/qcd_lepFilter_13TeV/qcd_lepFilter_13TeV_1000_0_TO_998.h5',\n",
      "        '5'],\n",
      "       [ '/bigdata/shared/Delphes/REDUCED_IsoLep_NEW/wjets_lepFilter/wjets_lepFilter_475_0_TO_99.h5',\n",
      "        '1'],\n",
      "       [ '/bigdata/shared/Delphes/REDUCED_IsoLep_NEW/qcd_lepFilter_13TeV/qcd_lepFilter_13TeV_1000_0_TO_998.h5',\n",
      "        '2'],\n",
      "       [ '/bigdata/shared/Delphes/REDUCED_IsoLep_NEW/wjets_lepFilter/wjets_lepFilter_475_0_TO_99.h5',\n",
      "        '2']], \n",
      "      dtype='|S99'), array([[ 1.,  0.],\n",
      "       [ 0.,  1.],\n",
      "       [ 1.,  0.],\n",
      "       [ 0.,  1.],\n",
      "       [ 1.,  0.],\n",
      "       [ 0.,  1.],\n",
      "       [ 0.,  1.],\n",
      "       [ 0.,  1.],\n",
      "       [ 0.,  1.],\n",
      "       [ 0.,  1.],\n",
      "       [ 1.,  0.],\n",
      "       [ 1.,  0.],\n",
      "       [ 1.,  0.],\n",
      "       [ 0.,  1.],\n",
      "       [ 1.,  0.],\n",
      "       [ 1.,  0.],\n",
      "       [ 1.,  0.],\n",
      "       [ 0.,  1.],\n",
      "       [ 1.,  0.],\n",
      "       [ 0.,  1.]]))\n"
     ]
    }
   ],
   "source": [
    "print(d,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = numpy_from_h5(\"/bigdata/shared/Delphes/REDUCED_IsoLep_NEW/qcd_lepFilter_13TeV/qcd_lepFilter_13TeV_4115_0_TO_998.h5\",0,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['source'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ '/bigdata/shared/Delphes/REDUCED_IsoLep_NEW/qcd_lepFilter_13TeV/qcd_lepFilter_13TeV_4115_0_TO_998.h5',\n",
       "        '0'],\n",
       "       [ '/bigdata/shared/Delphes/REDUCED_IsoLep_NEW/qcd_lepFilter_13TeV/qcd_lepFilter_13TeV_4115_0_TO_998.h5',\n",
       "        '1'],\n",
       "       [ '/bigdata/shared/Delphes/REDUCED_IsoLep_NEW/qcd_lepFilter_13TeV/qcd_lepFilter_13TeV_4115_0_TO_998.h5',\n",
       "        '2'],\n",
       "       [ '/bigdata/shared/Delphes/REDUCED_IsoLep_NEW/qcd_lepFilter_13TeV/qcd_lepFilter_13TeV_4115_0_TO_998.h5',\n",
       "        '3'],\n",
       "       [ '/bigdata/shared/Delphes/REDUCED_IsoLep_NEW/qcd_lepFilter_13TeV/qcd_lepFilter_13TeV_4115_0_TO_998.h5',\n",
       "        '4'],\n",
       "       [ '/bigdata/shared/Delphes/REDUCED_IsoLep_NEW/qcd_lepFilter_13TeV/qcd_lepFilter_13TeV_4115_0_TO_998.h5',\n",
       "        '5'],\n",
       "       [ '/bigdata/shared/Delphes/REDUCED_IsoLep_NEW/qcd_lepFilter_13TeV/qcd_lepFilter_13TeV_4115_0_TO_998.h5',\n",
       "        '6'],\n",
       "       [ '/bigdata/shared/Delphes/REDUCED_IsoLep_NEW/qcd_lepFilter_13TeV/qcd_lepFilter_13TeV_4115_0_TO_998.h5',\n",
       "        '7'],\n",
       "       [ '/bigdata/shared/Delphes/REDUCED_IsoLep_NEW/qcd_lepFilter_13TeV/qcd_lepFilter_13TeV_4115_0_TO_998.h5',\n",
       "        '8'],\n",
       "       [ '/bigdata/shared/Delphes/REDUCED_IsoLep_NEW/qcd_lepFilter_13TeV/qcd_lepFilter_13TeV_4115_0_TO_998.h5',\n",
       "        '9']], \n",
       "      dtype='|S99')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'sources'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-d4ddf5845a35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m        ]\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpandas_to_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-26-861ab18347b3>\u001b[0m in \u001b[0;36mpandas_to_numpy\u001b[0;34m(data_dirs, start, samples_per_class, observ_types, sort_columns, sort_ascending, particle_mean, particle_std, hlf_mean, hlf_std, verbose)\u001b[0m\n\u001b[1;32m    279\u001b[0m                                        \u001b[0mrows_per_event\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEFAULT_RPE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m                                        observ_types=observ_types)\n\u001b[0;32m--> 281\u001b[0;31m             \u001b[0mParticles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHLF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msources\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Particles\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"HLF\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sources\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mparticles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhlf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParticles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHLF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msources\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sources'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
