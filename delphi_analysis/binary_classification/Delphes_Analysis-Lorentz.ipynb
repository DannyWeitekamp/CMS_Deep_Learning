{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using gpu0\n",
      "using theano\n"
     ]
    }
   ],
   "source": [
    "#We can go into our root file and see what Trees are availiable\n",
    "%matplotlib inline\n",
    "import sys, os\n",
    "if __package__ is None:\n",
    "    import sys, os\n",
    "    sys.path.append(os.path.realpath(\"/data/shared/Software/\"))\n",
    "    sys.path.append(os.path.realpath(\"../../\"))\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ntpath\n",
    "import glob\n",
    "import deepconfig\n",
    "\n",
    "from keras.utils.visualize_util import plot\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from CMS_SURF_2016.utils.preprocessing import ObjectProfile, \\\n",
    "        preprocessFromPandas_label_dir_pairs, label_dir_pairs_args_decoder, \\\n",
    "        genFrom_label_dir_pairs,maxMutualLength, start_num_fromSplits\n",
    "from CMS_SURF_2016.utils.metrics import plot_history, print_accuracy_m\n",
    "from CMS_SURF_2016.utils.callbacks import OverfitStopping, SmartCheckpoint\n",
    "from CMS_SURF_2016.utils.archiving import *\n",
    "from CMS_SURF_2016.layers.lorentz import Lorentz, _lorentz\n",
    "from CMS_SURF_2016.layers.slice import Slice\n",
    "\n",
    "from keras.models import Sequential, Model, model_from_json\n",
    "from keras.layers import Dense, Flatten, Reshape, Activation, Dropout, Convolution2D, merge, Input, Flatten, Lambda\n",
    "from keras.engine.topology import Layer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils.visualize_util import plot\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "\n",
    "\n",
    "dc = deepconfig.deepconfig(gpu='gpu0', backend='theano')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#The observables taken from the table\n",
    "observ_types = ['E/c', 'Px', 'Py', 'Pz', 'Charge', \"PT_ET\", \"Eta\", \"Phi\", \"Dxy_Ehad_Eem\"]\n",
    "vecsize = len(observ_types)\n",
    "epochs = 100\n",
    "batch_size = 100\n",
    "sample_start = 0\n",
    "num_samples = 10000\n",
    "\n",
    "\n",
    "object_profiles = [ObjectProfile(\"Electron\",5),\n",
    "                    ObjectProfile(\"MuonTight\", 5),\n",
    "                    ObjectProfile(\"Photon\", 25),\n",
    "                    ObjectProfile(\"MissingET\", 1),\n",
    "                    ObjectProfile(\"EFlowPhoton\",100, sort_columns=[\"PT_ET\"], sort_ascending=False),  #1300\n",
    "                    ObjectProfile(\"EFlowNeutralHadron\",100, sort_columns=[\"PT_ET\"], sort_ascending=False),  #1000\n",
    "                    ObjectProfile(\"EFlowTrack\",100, sort_columns=[\"PT_ET\"], sort_ascending=False)]  #1050\n",
    "\n",
    "\n",
    "label_dir_pairs = \\\n",
    "            [   (\"ttbar\", \"/data/shared/Delphes/ttbar_lepFilter_13TeV/pandas_unjoined/\"),\n",
    "                (\"wjet\", \"/data/shared/Delphes/wjets_lepFilter_13TeV/pandas_unjoined/\"),\n",
    "                (\"qcd\", \"/data/shared/Delphes/qcd_lepFilter_13TeV/pandas_unjoined/\")\n",
    "            ]\n",
    "#ttbar_files = glob.glob(\"/data/shared/Delphes/ttbar_lepFilter_13TeV/pandas_unjoined/*.h5\")\n",
    "#WJet_files = glob.glob(\"/data/shared/Delphes/wjets_lepFilter_13TeV/pandas_unjoined/*.h5\")\n",
    "#qcd_files = glob.glob(\"/data/shared/Delphes/qcd_lepFilter_13TeV/pandas_unjoined/*.h5\")\n",
    "\n",
    "#files = {0:ttbar_files, 1:WJet_files}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 88816), (88816, 19032), (107848, 19032)]\n",
      "Generating DataProcedure in range(0,88816):\n",
      "   From 2 labels in range(0,10000) for 2x10000 = 20000 Samples\n",
      "   From 2 labels in range(10000,20000) for 2x10000 = 20000 Samples\n",
      "   From 2 labels in range(20000,30000) for 2x10000 = 20000 Samples\n",
      "   From 2 labels in range(30000,40000) for 2x10000 = 20000 Samples\n",
      "   From 2 labels in range(40000,50000) for 2x10000 = 20000 Samples\n",
      "   From 2 labels in range(50000,60000) for 2x10000 = 20000 Samples\n",
      "   From 2 labels in range(60000,70000) for 2x10000 = 20000 Samples\n",
      "   From 2 labels in range(70000,80000) for 2x10000 = 20000 Samples\n",
      "   From 2 labels in range(80000,88816) for 2x8816 = 17632 Samples\n"
     ]
    }
   ],
   "source": [
    "archive_dir = \"/data/shared/Delphes/keras_archive/\"\n",
    "#def getXYGenerator()\n",
    "SNs = start_num_fromSplits((.7,.15,.15), maxMutualLength(label_dir_pairs, object_profiles))\n",
    "print(SNs)\n",
    "gen_lambda =  lambda s : (DataProcedure(archive_dir,\n",
    "                                           False,\n",
    "                                        genFrom_label_dir_pairs,\n",
    "                                       start=s[0],\n",
    "                                       samples_per_label=s[1],\n",
    "                                       stride=10000,\n",
    "                                        batch_size=100,\n",
    "                                       archive_dir=archive_dir,\n",
    "                                       label_dir_pairs=label_dir_pairs[:2],\n",
    "                                       object_profiles=object_profiles,\n",
    "                                       observ_types=observ_types),\n",
    "                         len(label_dir_pairs)*s[1])\n",
    "l = [gen_lambda(s) for s in SNs]\n",
    "train, num_train = l[0]\n",
    "val,   num_val   = l[1]\n",
    "test,  num_test  = l[2]\n",
    "\n",
    "gen = train.getData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#X_train, y_train = preprocessFromPandas_label_dir_pairs(label_dir_pairs, sample_start, num_samples, object_profiles,observ_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(len(X_train[\"Electron\"]))\n",
    "#X_train\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def genModel(name,out_dim, depth, width, dense_activation=\"relu\", dropout = 0.0,sphereCoords=True):\n",
    "\tinputs = []\n",
    "\tmergelist = []\n",
    "\tfor i, profile in enumerate(object_profiles):\n",
    "\t\t# print(o)\n",
    "\t\tinp = a = Input(shape=(profile.max_size, vecsize), name=\"input_\"+str(i))\n",
    "\t\tinputs.append(inp)\n",
    "\n",
    "\t\tif(name == 'lorentz'):\n",
    "\t\t\tb1 = Lorentz(sphereCoords=sphereCoords, name=\"lorentz_\"+str(i))(a)\n",
    "\t\telse:\n",
    "\t\t\tb1 = Slice('[:,0:4]',name='slice_1_'+str(i))(a)\n",
    "\t\tb1 = Flatten(name=\"flatten1_\"+str(i))(b1)\n",
    "\n",
    "\t\t\n",
    "\t\tb2 = Slice('[:,4:9]',name='slice_2_'+str(i))(a)\n",
    "\t\tb2 = Flatten(name=\"flatten_2_\"+str(i))(b2)\n",
    "\t\t# b2 = Dense(10, activation='relu')(b2)\n",
    "\t\tmergelist.append(b1)\n",
    "\t\tmergelist.append(b2)\n",
    "\t# print(mergelist)\n",
    "\ta = merge(mergelist,mode='concat', name=\"merge\")\n",
    "\t# a = Flatten()(a)\n",
    "\tfor i in range(depth):\n",
    "\t\ta =  Dense(width, activation=dense_activation, name=\"dense_\"+str(i))(a)\n",
    "        if(dropout > 0.0):\n",
    "            a =  Dropout(dropout, name=\"dropout_\"+str(i))(a)\n",
    "\tdense_out = Dense(out_dim, activation='sigmoid', name='main_output')(a)\n",
    "\tmodel = Model(input=inputs, output=dense_out, name=name)\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 88816), (88816, 19032), (107848, 19032)]\n",
      "('EXECUTE: ', 'lorentz', ['ttbar', 'wjet'], 1, 10, 'relu')\n",
      "Generating DataProcedure in range(88816,107848):"
     ]
    }
   ],
   "source": [
    "archive_dir = \"/data/shared/Delphes/keras_archive/\"\n",
    "earlyStopping = EarlyStopping(verbose=1, patience=10)\n",
    "overfitStopping = OverfitStopping(verbose=1, patience=20)\n",
    "trials = []\n",
    "ldps_pairs = []\n",
    "for i, j in [(0,1),(1,2),(2,0)]:\n",
    "    ldps = [label_dir_pairs[i]] + [label_dir_pairs[j]]\n",
    "    SNs = start_num_fromSplits((.7,.15,.15), maxMutualLength(label_dir_pairs, object_profiles))\n",
    "    print(SNs)\n",
    "    gen_lambda =  lambda s : (DataProcedure(archive_dir,\n",
    "                                           False,\n",
    "                                           genFrom_label_dir_pairs,\n",
    "                                           start=s[0],\n",
    "                                           samples_per_label=s[1],\n",
    "                                           stride=10000,\n",
    "                                            batch_size=100,\n",
    "                                           archive_dir=archive_dir,\n",
    "                                           label_dir_pairs=ldps,\n",
    "                                           object_profiles=object_profiles,\n",
    "                                           observ_types=observ_types),\n",
    "                             len(ldps)*s[1])\n",
    "    l = [gen_lambda(s) for s in SNs]\n",
    "    train, num_train = l[0]\n",
    "    val,   num_val   = l[1]\n",
    "    test,  num_test  = l[2]\n",
    "    \n",
    "    labels = [x[0] for x in ldps]\n",
    "    for name in ['lorentz', 'not_lorentz']:\n",
    "        for sphereCoords in [True,False]:\n",
    "            for depth in [1,2]:\n",
    "                for width in [10,100]:\n",
    "                    for activation in ['relu']:\n",
    "                        for dropout in [0.0]:\n",
    "                            activation_name = activation if isinstance(activation, str) \\\n",
    "                                                else activation.__name__\n",
    "\n",
    "                            model = genModel(name, len(ldps), depth, width, activation, dropout, sphereCoords)\n",
    "\n",
    "                            trial = KerasTrial(archive_dir, name=name, model=model)\n",
    "\n",
    "                            trial.setTrain(train_procedure=train,\n",
    "                                           samples_per_epoch=num_train\n",
    "                                          )\n",
    "                            trial.setValidation(val_procedure=val,\n",
    "                                               nb_val_samples=num_val)\n",
    "                            trial.setCompilation(loss='binary_crossentropy',\n",
    "                                      optimizer='rmsprop',\n",
    "                                      metrics=['accuracy']\n",
    "                                          )\n",
    "                            trial.setFit_Generator( \n",
    "                                            nb_epoch=epochs,\n",
    "                                            callbacks=[earlyStopping, overfitStopping],\n",
    "                                            max_q_size=100)\n",
    "                                            #pickle_safe=True)\n",
    "\n",
    "\n",
    "\n",
    "                            print(\"EXECUTE: \", name,labels, depth,width, activation_name)\n",
    "                            trial.execute(custom_objects={\"Lorentz\":Lorentz,\"Slice\": Slice},\n",
    "                                         train_arg_decode_func=label_dir_pairs_args_decoder,\n",
    "                                         val_arg_decode_func=label_dir_pairs_args_decoder)\n",
    "\n",
    "\n",
    "                            trial.test(test_proc=test,\n",
    "                                         test_samples=num_test,\n",
    "                                         custom_objects={\"Lorentz\":Lorentz,\"Slice\": Slice},\n",
    "                                        arg_decode_func = label_dir_pairs_args_decoder)\n",
    "\n",
    "\n",
    "\n",
    "                            trial.to_record({\"lables\": labels,\n",
    "                                             \"depth\": depth,\n",
    "                                             \"width\": width,\n",
    "                                             \"activation\": activation_name,\n",
    "                                             \"dropout\":dropout\n",
    "                                            })\n",
    "                            if(name == \"lorentz\"):\n",
    "                                trial.to_record({\"sphereCoords\" : sphereCoords})\n",
    "                                       \n",
    "\n",
    "                \n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "histories = []\n",
    "custom_objects ={'Lorentz': Lorentz, 'Slice': Slice}\n",
    "for i, trial in enumerate(trials):\n",
    "    #Execute and test\n",
    "    test_proc = PreprocessingProcedure(archive_dir, True,preprocessFromPandas_label_dir_pairs,\n",
    "                            ldps_pairs[i], num_samples*2,num_samples, object_profiles,observ_types)\n",
    "    trial.execute(arg_decode_func=label_dir_pairs_args_decoder, custom_objects=custom_objects)\n",
    "    metrics = trial.test(test_proc,custom_objects=custom_objects)\n",
    "    print(metrics)\n",
    "    \n",
    "    #Graphs n such\n",
    "    model = trial.get_model()\n",
    "    #image_name = 'model_%r.png' % trial.hash()\n",
    "    image_name = \"model_\" +str(i)+ \".png\"\n",
    "    dot = plot(model, to_file=image_name, show_shapes=True, show_layer_names=False)\n",
    "    histories.append( (trial, Image(image_name), trial.get_history(), trial.get_from_index(\"val_acc\") ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for trial, image, histDict, val_acc in histories:\n",
    "    display(image)\n",
    "    history = History()\n",
    "    history.history = histDict\n",
    "    plot_history([(\"dense\", history)])\n",
    "    print(trial.hash() + ': Best Validation accuracy: %s' % \"{:0.4f}\".format(val_acc))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "bar_width = 0.35\n",
    "depths = [ trial.get_from_index(\"depth\") for trial, image, histDict, val_acc in histories]\n",
    "index = np.arange(len(depths))\n",
    "accuracies = [val_acc for trial, image, histDict, val_acc in histories]\n",
    "\n",
    "rects1 = plt.scatter(index, accuracies,\n",
    "                 color='b',\n",
    "                 label='Men')\n",
    "\n",
    "\n",
    "plt.xlabel('Depth')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.title('Model validation accuracy by depth')\n",
    "plt.xticks(index, depths)\n",
    "#plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#smartCheckpoint = SmartCheckpoint(\"dense\"+add_title)\n",
    "#RUN Dense\n",
    "#dense_history = dense.fit(X_train_flatten, y_train,\n",
    "#                    batch_size=batch_size,\n",
    "#                    nb_epoch=epochs,\n",
    " #                   validation_split=.2,\n",
    "#                    callbacks=[earlyStopping, overfitStopping])\n",
    "#histories[\"dense\"+add_title] = (dense,dense_history,X_train_flatten, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keys = [key for key in histories]\n",
    "def p(key):\n",
    "    tup = histories[key]\n",
    "    model = tup[0]\n",
    "    history = tup[1]\n",
    "    #print_accuracy_m(model, tup[2], tup[3])\n",
    "    print(key + ': Best Validation accuracy: %r%%' % max(history.history[\"val_acc\"]))\n",
    "    plot_history([(key, history)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p(keys[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p(keys[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p(keys[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p(keys[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# import json\n",
    "class KerasStore:\n",
    "    def __init__(self,\n",
    "                    label_dir_pairs=None,\n",
    "                    num_samples=None,\n",
    "                    object_profiles=None,\n",
    "                    observ_types=None,\n",
    "                )\n",
    "    def to_JSON(self):\n",
    "        return json.dumps(self, default=lambda o: o.__dict__, \n",
    "            sort_keys=True, indent=4)\n",
    "    \n",
    "a = Object()\n",
    "a.label_dir_pairs = label_dir_pairs\n",
    "a.num_samples = num_samples\n",
    "a.object_profiles = object_profiles\n",
    "a.observ_types = observ_types\n",
    "\n",
    "\n",
    "f = JSONDecoder(object_hook = from_json).decode('{\"fname\": \"/foo/bar\"}')\n",
    "print(\"JSON:\")\n",
    "print(a.to_JSON())\n",
    "\n",
    "\n",
    "\n",
    "def setPreprocessingInfo(label_dir_pairs, num_samples, object_profiles, observ_types):\n",
    "    hashString = \"\"\n",
    "    for x, y in label_dir_pairs:\n",
    "        hashString += x\n",
    "        hashString += y\n",
    "    hashString += num_samples\n",
    "    for x in object_profiles:\n",
    "        hashString += str(x)\n",
    "    for x in observ_types:\n",
    "        hashString += x\n",
    "    \n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
