{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuDNN version 5105 on context None\n",
      "Mapped name None to device cuda: GeForce GTX 1080 (0000:07:00.0)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '3'\n",
    "\n",
    "if __package__ is None:\n",
    "    import sys, os\n",
    "    sys.path.append(os.path.realpath(\"../\"))\n",
    "from keras.layers import Input, GRU,RepeatVector\n",
    "from keras.engine import Model\n",
    "#from CMS_Deep_Learning.layers.ptr_net import Ptr_Layer\n",
    "from CMS_Deep_Learning.layers.slice import Slice\n",
    "from CMS_Deep_Learning.io import flatten\n",
    "from keras import backend as K#,initializers\n",
    "import numpy as np\n",
    "import theano\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def giniSparsity(softmax_matrix, sparsity_coeff=.025):\n",
    "#    return sparsity_coeff * K.sum(K.sum(K.sum(softmax_matrix*(1.0-softmax_matrix)))) / K.prod(K.shape(softmax_matrix)) \n",
    "#K.eval(giniSparsity(.002*np.random.random((100,100,100))) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 10.,   0.],\n",
       "       [  0.,  10.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(2)/(.1*np.ones((1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.engine.topology import Layer,initializations\n",
    "from keras import backend as K#,initializers\n",
    "from keras.regularizers import Regularizer\n",
    "#import theano\n",
    "#import tensorflow as tf \n",
    "\n",
    "def softmax(x, axis=-1):\n",
    "    \"\"\"Softmax activation function.\n",
    "    # Arguments\n",
    "        x : Tensor.\n",
    "        axis: Integer, axis along which the softmax normalization is applied.\n",
    "    # Returns\n",
    "        Tensor, output of softmax transformation.\n",
    "    # Raises\n",
    "        ValueError: In case `dim(x) == 1`.\n",
    "    \"\"\"\n",
    "    ndim = K.ndim(x)\n",
    "    #if ndim == 2:\n",
    "    #    return K.softmax(x)\n",
    "    if ndim >= 2:\n",
    "        e = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
    "        s = K.sum(e, axis=axis, keepdims=True)\n",
    "        return e / s\n",
    "    else:\n",
    "        raise ValueError('Cannot apply softmax to a tensor that is 1D')\n",
    "\n",
    "def giniSparsity(softmax_matrix, sparsity_coeff=.025):\n",
    "    return sparsity_coeff * K.sum(K.sum(K.sum(softmax_matrix*(1.0-softmax_matrix)))) / K.prod(K.cast((K.shape(softmax_matrix)),'float32')) \n",
    "#K.eval(giniSparsity(.002*np.random.random((100,100,100))) ) \n",
    "\n",
    "class Ptr_Layer(Layer):\n",
    "    def __init__(self, attention_width,implementation='custom', seq_len=None,sparsity_coeff=1000.0, return_U=False,**kwargs):\n",
    "       \n",
    "        self.supports_masking = True\n",
    "        self.init = initializations.get('glorot_uniform')\n",
    "        self.attention_width = attention_width\n",
    "        self.sparsity_coeff = sparsity_coeff\n",
    "        self.implementation = implementation\n",
    "        self.seq_len = seq_len\n",
    "        self.return_U = return_U\n",
    "        super(Ptr_Layer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        print(\"IMPLEMENTATION: %r\" % self.implementation)\n",
    "        assert len(input_shape) >= 2\n",
    "        \n",
    "        if(self.implementation == 'custom' or self.implementation == 'custom_T'):\n",
    "            assert self.attention_width == input_shape[1][-2], \"attention width %r != seq size %r\" %(self.attention_width,input_shape[1][-2])\n",
    "        \n",
    "        # self.attention_width = input_shape[1][-1]\n",
    "        self.W1 = self.add_weight((self.attention_width ,input_shape[1][-1]), #(att_dim, recurrent_dim)\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W1'.format(self.name))\n",
    "        self.W2 = self.add_weight((self.attention_width ,input_shape[2][-1] if len(input_shape) > 2 else input_shape[1][-1]), #(att_dim, recurrent_dim)\n",
    "                                  initializer=self.init,\n",
    "                                  name='{}_W2'.format(self.name))\n",
    "        if(self.implementation != 'custom' and self.implementation != 'custom_T'):\n",
    "            self.v = self.add_weight((self.attention_width,1),\n",
    "                                      initializer=self.init,\n",
    "                                      name='{}_v'.format(self.name))\n",
    "        \n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        if(self.return_U):\n",
    "            return [None,None]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def call(self, inp, mask=None):\n",
    "        assert isinstance(inp,list) and len(inp) >= 2, \"Bad input expecting list of input,encoder,decoder\"\n",
    "        \n",
    "        if(len(inp) == 3):\n",
    "            x_T,e_T,d_T = inp\n",
    "        elif(len(X) == 2):\n",
    "            x_T,e_T = inp\n",
    "            d_T = e_T\n",
    "        \n",
    "        #Let x be the inputs as column vectors\n",
    "        # (batch_size ,sequence_len, feature_dim) -> (batch_size ,feature_dim,sequence_len)\n",
    "        x = K.permute_dimensions(x_T,(0,2,1))\n",
    "        \n",
    "        \n",
    "        if(K.backend() == \"tensorflow\"):\n",
    "            assert self.seq_len != None, 'Must set Ptr_Layer(seq_len=?) if using Tensorflow'\n",
    "            seq_len = self.seq_len\n",
    "        else:\n",
    "            seq_len = K.shape(e_T)[1]\n",
    "        # Shape key:\n",
    "        # x_T:  #(batch_size ,sequence_len, feature_dim)\n",
    "        # e_T:  #(batch_size ,sequence_len, recurrent_dim)\n",
    "        # d_T:  #(batch_size ,sequence_len, recurrent_dim)\n",
    "        #*_T indicates row vectors\n",
    "           \n",
    "        #(batch_size ,sequence_len, recurrent_dim) * (recurrent_dim,att_dim) -> #(batch_size ,sequence_len,att_dim)\n",
    "        _e_T, _d_T = K.dot(e_T,K.transpose(self.W1)), K.dot(d_T, K.transpose(self.W2)) # (batch_size ,sequence_len, att_dim)\n",
    "        _e, _d = K.permute_dimensions(_e_T,(0,2,1)), K.permute_dimensions(_d_T,(0,2,1)) # (batch_size ,att_dim, sequence_len)\n",
    "        #_e = theano.printing.Print('_e', attrs=['shape'])(_e)\n",
    "        #_d = theano.printing.Print('_d', attrs=['shape'])(_d)\n",
    "        \n",
    "        def Tmap(fn, arrays, dtype='float32'):\n",
    "            # assumes all arrays have same leading dim\n",
    "            indices = K.range(K.shape(arrays[0])[0])\n",
    "            out = K.map_fn(lambda ii: fn(*[array[ii] for array in arrays]), indices, dtype=dtype)\n",
    "            return out\n",
    "        \n",
    "        \n",
    "        \n",
    "        if(self.implementation == 'ptr_net'):\n",
    "            print(\"PTR_NET\")\n",
    "            \n",
    "            E_T = K.repeat_elements(K.expand_dims(_e_T, dim=1), seq_len, axis=1) # (batch_size ,sequence_len, sequence_len, att_dim)\n",
    "            D_T = K.repeat_elements(K.expand_dims(_d_T, dim=1), seq_len, axis=1) # (batch_size ,sequence_len, sequence_len, att_dim)\n",
    "\n",
    "            D = K.permute_dimensions(D_T,(0,2,1,3)) # (batch_size ,sequence_len, sequence_len, att_dim)\n",
    "\n",
    "            u = K.squeeze(K.dot(K.tanh(E_T + D),self.v),axis=-1) # (batch_size ,sequence_len, sequence_len)\n",
    "            u = K.permute_dimensions(u, (0,2,1))\n",
    "            #axis=2 is row axis therefore u*x has columns that are linear combos of x\n",
    "            u = softmax(u,axis=2) # (batch_size ,sequence_len, sequence_len) \n",
    "        elif(self.implementation == 'ptr_net_scan'):\n",
    "            def _ptr_net_u(_e_T,_d_T):\n",
    "                __E_T = K.repeat_elements(K.expand_dims(_e_T, dim=0), seq_len, axis=0) # (sequence_len, sequence_len, att_dim)\n",
    "                __D_T = K.repeat_elements(K.expand_dims(_d_T, dim=0), seq_len, axis=0) # (sequence_len, sequence_len, att_dim)\n",
    "\n",
    "\n",
    "                __D = K.permute_dimensions(__D_T,(1,0,2)) # (sequence_len, sequence_len, att_dim)\n",
    "\n",
    "                u = K.dot(K.tanh(__E_T + __D),self.v) # (sequence_len, sequence_len)\n",
    "                u = K.squeeze(u,axis=-1)\n",
    "                u = K.permute_dimensions(u, (1,0))\n",
    "                u = softmax(u,axis=1) # (sequence_len, sequence_len) \n",
    "\n",
    "                return u\n",
    "            assert K.backend()=='tensorflow', 'ptr_net_scan only works with tensorflow backend'\n",
    "            import tensorflow as tf\n",
    "            u = tf.map_fn(lambda x: _ptr_net_u(x[0], x[1]), (_e_T,_d_T), dtype=tf.float32)\n",
    "            \n",
    "        elif(self.implementation == 'custom'):\n",
    "            #only onto if att_dim == sequence_len\n",
    "            u = _e + _d_T           ## (batch_size ,att_dim, att_dim)\n",
    "            u = softmax(u,axis=2)   ## (batch_size ,att_dim, att_dim)  \n",
    "        elif(self.implementation == 'custom_T'):\n",
    "            #only onto if att_dim == sequence_len\n",
    "            u = _e_T + _d           ## (batch_size ,att_dim, att_dim)\n",
    "            u = softmax(u,axis=2)   ## (batch_size ,att_dim, att_dim)  \n",
    "        else:\n",
    "            raise ValueError(\"implementation not recognized: %r\" % self.implementation)\n",
    "            \n",
    "        \n",
    "        self.add_loss(giniSparsity(u,self.sparsity_coeff))\n",
    "        \n",
    "        u_T = K.permute_dimensions(u, (0, 2, 1))\n",
    "        #For some reason batchdot dots the transposes (i.e batch_dot(X,Y) = X^TY^T = (YX)^T )\n",
    "        #Therefore batch_dot(U^T,X) = UX^T -> X_sorted^T\n",
    "        x_T = K.batch_dot(u_T,x, axes=[1, 2]) \n",
    "        \n",
    "        #x_T = K.permute_dimensions(soft_sorted_x, (0, 2, 1))\n",
    "        if(self.return_U):\n",
    "            return [x_T, u] #+K.sum(K.sum(K.sum(u)))#+ K.sum(K.sum(K.sum(_e))) + K.sum(K.sum(K.sum(_d))) \n",
    "        else:\n",
    "            return x_T\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        if(self.return_U):\n",
    "            return [tuple(input_shape[0]),tuple((input_shape[0][0],input_shape[0][-2],input_shape[0][-2]))]\n",
    "        else:\n",
    "            return tuple(input_shape[0])\n",
    "            \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.engine.topology import Layer,initializations\n",
    "from keras import backend as K#,initializers\n",
    "from keras.regularizers import Regularizer\n",
    "import theano.tensor as T\n",
    "\n",
    "def giniSparsity(softmax_matrix, sparsity_coeff=.02):\n",
    "    return sparsity_coeff * K.sum(K.sum(K.sum(softmax_matrix*(1.0-softmax_matrix)))) / K.prod(K.cast((K.shape(softmax_matrix)),'float32')) \n",
    "\n",
    "def injectionRegulizer(softmax_matrix, injection_coeff=.02):\n",
    "    shape = K.cast(K.shape(softmax_matrix),'float32') \n",
    "    return injection_coeff * K.sum(K.sum(K.square(1.0-K.sum(softmax_matrix,axis=-2)))) / (shape[0]*shape[-1])\n",
    "\n",
    "class WanderingGaussians(Layer):\n",
    "    def __init__(self, seq_len,return_U=False,**kwargs):\n",
    "        self.seq_len = seq_len\n",
    "        self.init = initializations.get('glorot_uniform')\n",
    "        self.return_U = return_U\n",
    "        super(WanderingGaussians, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "                \n",
    "        self.W = self.add_weight((self.seq_len,input_shape[1][-1]), #(att_dim, recurrent_dim\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name))\n",
    "        \n",
    "        self.sigma = K.variable(np.ones((1,)))\n",
    "        self.trainable_weights.append(self.sigma)\n",
    "        #self.sigma = self.add_weight((1,1), #(att_dim, recurrent_dim\n",
    "        #                         initializer=lambda x:\n",
    "        #                         name='{}_sigma'.format(self.name))\n",
    "        \n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        if(self.return_U):\n",
    "            return [None,None]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def call(self, inp, mask=None):\n",
    "        x_T,e_T = inp\n",
    "        #e = K.permute_dimensions(e_T,(0,2,1))\n",
    "        x = K.permute_dimensions(x_T,(0,2,1))\n",
    "        \n",
    "        g_centers = K.dot(e_T,self.W.T)\n",
    "        g_centers = K.expand_dims(g_centers,dim=-1)\n",
    "        \n",
    "        #g_centers = theano.printing.Print('g_centers')(g_centers)\n",
    "        \n",
    "        locs = K.expand_dims(K.expand_dims(K.arange(self.seq_len)/float(self.seq_len),dim=0),dim=0)\n",
    "        locs = K.repeat_elements(locs,self.seq_len,axis=1)\n",
    "        locs = K.repeat_elements(locs,g_centers.shape[0],axis=0)\n",
    "        diffs = locs - g_centers #(batch_size,seq_len, seq_len)\n",
    "        #diffs = theano.printing.Print('diffs', attrs=['shape'])(diffs)\n",
    "        \n",
    "        sigma = K.repeat_elements(self.sigma,self.seq_len,axis=-1)\n",
    "        #R = K.exp(-K.square(diffs)/sigma)\n",
    "        R = (diffs*sigma)**-2#/(2.0*sigma))\n",
    "        \n",
    "        \n",
    "        #u = R#K.square(R)\n",
    "        u = softmax(R,axis=2)   ## (batch_size ,seq_len, seq_len)  \n",
    "        u_T = K.permute_dimensions(u, (0, 2, 1))\n",
    "        #For some reason batchdot dots the transposes (i.e batch_dot(X,Y) = X^TY^T = (YX)^T )\n",
    "        #Therefore batch_dot(U^T,X) = UX^T -> X_sorted^T\n",
    "        x_T = K.batch_dot(u_T,x, axes=[1, 2]) \n",
    "        #a = inp[0] + K.sum(K.sum(K.sum(diffs))) + K.sum(K.sum(self.sigma))\n",
    "        return [x_T,u]\n",
    "        \n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        if(self.return_U):\n",
    "            return [tuple(input_shape[0]),tuple((input_shape[0][0],input_shape[0][-2],input_shape[0][-2]))]\n",
    "        else:\n",
    "            return tuple(input_shape[0])\n",
    "        \n",
    "\n",
    "from keras.engine.topology import Layer,initializations\n",
    "from keras import backend as K#,initializers\n",
    "from keras.regularizers import Regularizer\n",
    "import theano.tensor as T\n",
    "\n",
    "class DotAndSoftmax(Layer):\n",
    "    def __init__(self, return_U=False,sparsity_coeff=.000,injection_coeff=.000,**kwargs):\n",
    "        #self.seq_len = seq_len\n",
    "        #self.init = initializations.get('glorot_uniform')\n",
    "        self.injection_coeff =  injection_coeff\n",
    "        self.return_U = return_U\n",
    "        self.sparsity_coeff = sparsity_coeff\n",
    "        super(DotAndSoftmax, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        pass\n",
    "        \n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        if(self.return_U):\n",
    "            return [None,None]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def call(self, inp, mask=None):\n",
    "        x_T,R = inp\n",
    "        \n",
    "        x = K.permute_dimensions(x_T,(0,2,1))\n",
    "        \n",
    "        u = softmax(R,axis=2)   ## (batch_size ,seq_len, seq_len)  \n",
    "        self.add_loss(giniSparsity(u,self.sparsity_coeff))\n",
    "        self.add_loss(injectionRegulizer(u,self.injection_coeff))\n",
    "        u_T = K.permute_dimensions(u, (0, 2, 1))\n",
    "        #For some reason batchdot dots the transposes (i.e batch_dot(X,Y) = X^TY^T = (YX)^T )\n",
    "        #Therefore batch_dot(U^T,X) = UX^T -> X_sorted^T\n",
    "        x_T = K.batch_dot(u_T,x, axes=[1, 2]) \n",
    "        #a = inp[0] + K.sum(K.sum(K.sum(diffs))) + K.sum(K.sum(self.sigma))\n",
    "        return [x_T,u]\n",
    "        \n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        if(self.return_U):\n",
    "            return [tuple(input_shape[0]),tuple((input_shape[0][0],input_shape[0][-2],input_shape[0][-2]))]\n",
    "        else:\n",
    "            return tuple(input_shape[0])\n",
    "\n",
    "        \n",
    "def sort4_vec(inp):\n",
    "    a,b,c,d = inp[:,0:1],inp[:,1:2],inp[:,2:3],inp[:,3:4]\n",
    "    e,f = K.minimum(a,b),K.maximum(a,b) \n",
    "    g,h = K.minimum(c,d),K.maximum(c,d) \n",
    "    a_,p = K.minimum(e,g),K.maximum(e,g) \n",
    "    q,d_ = K.minimum(h,f),K.maximum(h,f) \n",
    "    b_,c_ = K.minimum(p,q),K.maximum(p,q) \n",
    "    return K.concatenate([a_,b_,c_,d_],axis=-1)\n",
    "\n",
    "def sort8_vec(inp):\n",
    "    a1_4,a5_8 = inp[:,0:4],inp[:,4:8]\n",
    "    a1_4 = sort4_vec(a1_4)\n",
    "    a5_8 = sort4_vec(a5_8)\n",
    "    a1_2,a3_4 = a1_4[:,0:2],a1_4[:,2:4]\n",
    "    a5_6,a7_8 = a5_8[:,0:2],a5_8[:,2:4]\n",
    "    \n",
    "    b1_4 = sort4_vec(K.concatenate([a1_2,a5_6],axis=-1))\n",
    "    b5_8 = sort4_vec(K.concatenate([a3_4,a7_8],axis=-1))\n",
    "    b1_2, b3_4 = b1_4[:,0:2],b1_4[:,2:4]\n",
    "    b5_6, b7_8 = b5_8[:,0:2],b5_8[:,2:4]\n",
    "    \n",
    "    c3_6 = sort4_vec(K.concatenate([b3_4,b5_6],axis=-1))\n",
    "    \n",
    "    return K.concatenate([b1_2,c3_6,b7_8],axis=-1)\n",
    "\n",
    "def sort4(a,b,c,d):\n",
    "    \n",
    "    e,f = K.minimum(a,b),K.maximum(a,b) \n",
    "    g,h = K.minimum(c,d),K.maximum(c,d) \n",
    "    a_,p = K.minimum(e,g),K.maximum(e,g) \n",
    "    q,d_ = K.minimum(h,f),K.maximum(h,f) \n",
    "    b_,c_ = K.minimum(p,q),K.maximum(p,q) \n",
    "    return a_,b_,c_,d_\n",
    "\n",
    "\n",
    "\n",
    "def sort2(a,b): \n",
    "    a_,b_ = K.minimum(a,b),K.maximum(a,b) \n",
    "    return a_,b_\n",
    "\n",
    "def sort3(a,b,c): \n",
    "    e,f = K.minimum(a,b),K.maximum(a,b) \n",
    "    g,c_ = K.minimum(f,c),K.maximum(f,c) \n",
    "    a_,b_ = K.minimum(e,g),K.maximum(e,g) \n",
    "    return a_,b_\n",
    "    \n",
    "class Sort4(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Sort4, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, inp, mask=None):\n",
    "        a,b,c,d = inp[:,0:1],inp[:,1:2],inp[:,2:3],inp[:,3:4]\n",
    "        _a,_b,_c,_d = sort4(a,b,c,d)\n",
    "        return K.concatenate([_a,_b,_c,_d],axis=-1)\n",
    "        \n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return input_shape\n",
    "    \n",
    "class SortN(Layer):\n",
    "    def __init__(self, seq_len,**kwargs):\n",
    "        self.seq_len = seq_len\n",
    "        super(SortN, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, inp, mask=None):\n",
    "        #print(self.input_shape)\n",
    "        n = self.seq_len#inp.shape[-1]\n",
    "        \n",
    "        def srt(lst):\n",
    "            m = n // 8\n",
    "        \n",
    "            lst = lst.reshape((8,m))\n",
    "            lst = K.permute_dimensions(lst,[1,0])\n",
    "            lst = sort8_vec(lst)\n",
    "            lst = K.permute_dimensions(lst,[1,0])\n",
    "            lst = lst.reshape((n,))\n",
    "            \n",
    "            lst = lst.reshape((m,8))\n",
    "            lst = sort8_vec(lst)\n",
    "            lst = lst.reshape((n,))\n",
    "            \n",
    "            \n",
    "            '''\n",
    "            l = []\n",
    "            for i in range(m):\n",
    "                subset = [x for x in range(i,n,m)]\n",
    "                print(subset)\n",
    "                sub = K.gather(lst, subset)\n",
    "                #sub = theano.printing.Print('sub',attrs=['shape'])(sub)\n",
    "                sub = sort4(sub[0],sub[1],sub[2],sub[3])\n",
    "                sub = K.concatenate([x.reshape((1,1)) for x in sub],axis=-1)\n",
    "                l.append(sub.reshape((4,1)))\n",
    "            restructured = K.concatenate(l,axis=-1)\n",
    "            lst = restructured.reshape((n,))\n",
    "            #m = n // 4\n",
    "            #lst = lst.reshape(m,4)\n",
    "            l = []\n",
    "            for i in range(0,n,4):\n",
    "                subset = [x for x in range(i,i+4,1)]\n",
    "                print(subset)\n",
    "                sub = K.gather(lst, subset)\n",
    "                #sub = theano.printing.Print('sub',attrs=['shape'])(sub)\n",
    "                sub = sort4(sub[0],sub[1],sub[2],sub[3])\n",
    "                sub = K.concatenate([x.reshape((1,1)) for x in sub],axis=-1)\n",
    "                l.append(sub.reshape((1,4)))\n",
    "            restructured = K.concatenate(l,axis=-2)\n",
    "            '''\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            return lst\n",
    "        \n",
    "        return K.map_fn(srt,inp)\n",
    "            \n",
    "            \n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return input_shape\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 43s - loss: 0.0184    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f336f5d7210>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Input(shape=(SEQ_LEN, 1), name=\"input\")\n",
    "o = Flatten()(x)\n",
    "o = SortN(SEQ_LEN)(o) \n",
    "o = Reshape((-1,1))(o)\n",
    "model = Model(input=x, output=o, name='test')\n",
    "model.compile(optimizer='adam',loss='mse')\n",
    "model.fit(inp, target, nb_epoch=1,batch_size=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.engine.topology import Layer,initializations,Merge\n",
    "from keras.layers import Convolution1D,MaxPooling1D,Flatten,Dense,Reshape\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "def null_cost(y_true, y_pred):\n",
    "    # output itself is cost and must be minimized\n",
    "    return K.sum(y_pred)*0 + K.sum(y_true)*0\n",
    "def buildModel(sparsity_coeff=0.0,injection_coeff=0.0):\n",
    "    x = Input(shape=(SEQ_LEN, 1), name=\"input\")\n",
    "    e = Flatten()(x)\n",
    "    e = Dense(800, activation='relu')(e)\n",
    "    e = Dense(800, activation='relu')(e)\n",
    "    #e = Dense(400, activation='relu')(e)\n",
    "    #e = Dense(400, activation='relu')(e)\n",
    "    e = Dense(SEQ_LEN*SEQ_LEN, activation='linear')(e)\n",
    "    print(e)\n",
    "    r = Reshape(target_shape=(SEQ_LEN,SEQ_LEN))(e)\n",
    "    p,u = DotAndSoftmax(sparsity_coeff=sparsity_coeff,injection_coeff=injection_coeff,return_U=True)([x,r])\n",
    "    #e = Convolution1D(nb_filter=10, filter_length=5, activation='relu')(x)\n",
    "    #e = MaxPooling1D()(e)     # Downsample the output of convolution by 2X.\n",
    "    #e = Convolution1D(nb_filter=10, filter_length=3, activation='relu')(e)\n",
    "    #e = MaxPooling1D()(e)\n",
    "    #e = Flatten()(e)\n",
    "    #e = Dense(100, activation='linear')(e)\n",
    "    #e = GRU(100,return_sequences=False)(x)\n",
    "    #s = Slice(\"[-1,:]\")(e)\n",
    "    # s = Slice('[-1,:]')(e)\n",
    "    # s = theano.printing.Print(\"s\")(s)\n",
    "    #r = RepeatVector(SEQ_LEN)(s)\n",
    "    #m = Merge(mode='concat',concat_axis=2)([r,x])\n",
    "    #d = GRU(128,return_sequences=True)(m)\n",
    "    #p,u = WanderingGaussians(SEQ_LEN,return_U=True)([x,e])\n",
    "\n",
    "    model = Model(input=x, output=[p,u], name='test')\n",
    "    model.compile(optimizer='adam',loss=['mse',null_cost],loss_weights=[1., 0.0])\n",
    "    #model = Model(input=x, output=p, name='test')\n",
    "    #model.compile(optimizer='adam',loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 256\n",
    "DATA_SHAPE =[None, SEQ_LEN, 1]\n",
    "\n",
    "inp = np.random.randint(size=tuple([50000]+DATA_SHAPE[1:]),low=0,high=1000)/1000.0\n",
    "indicies = np.argsort(inp[:, :, 0])\n",
    "target = np.array([np.take(inp[i], indicies[i], axis=-2) for i in range(inp.shape[0])])\n",
    "\n",
    "val = np.random.randint(size=tuple([10000]+DATA_SHAPE[1:]),low=0,high=1000)/1000.0\n",
    "indicies = np.argsort(val[:, :, 0])\n",
    "val_target = np.array([np.take(val[i], indicies[i], axis=-2) for i in range(val.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "map_fn() got an unexpected keyword argument 'axis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-206-91771940028d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSEQ_LEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSortN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dweitekamp/.local/lib/python2.7/site-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minbound_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m             \u001b[0;31m# This will call layer.build() if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_inbound_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minbound_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m             \u001b[0;31m# Outputs were already computed when calling self.add_inbound_node.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minbound_nodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dweitekamp/.local/lib/python2.7/site-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36madd_inbound_node\u001b[0;34m(self, inbound_layers, node_indices, tensor_indices)\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;31m# creating the node automatically updates self.inbound_nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[0;31m# as well as outbound_nodes on inbound layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mNode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minbound_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_output_shape_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dweitekamp/.local/lib/python2.7/site-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36mcreate_node\u001b[0;34m(cls, outbound_layer, inbound_layers, node_indices, tensor_indices)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0moutput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutbound_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_masks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m             \u001b[0moutput_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutbound_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;31m# TODO: try to auto-infer shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-205-be29f8018588>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inp, mask)\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mexpanded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: map_fn() got an unexpected keyword argument 'axis'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elemwise{add,no_inplace}.0\n",
      "Epoch 1/1\n",
      "50000/50000 [==============================] - 154s - loss: 0.0321 - dotandsoftmax_1_loss_1: 0.0259 - dotandsoftmax_1_loss_2: 0.0000e+00   \n",
      "('VAL_MSE:', 0.008444038211261712)\n",
      "Epoch 1/1\n",
      "10400/50000 [=====>........................] - ETA: 139s - loss: 0.0141 - dotandsoftmax_1_loss_1: 0.0078 - dotandsoftmax_1_loss_2: 0.0000e+00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-90e530600a94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mmse_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSEQ_LEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSEQ_LEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0;31m#model.fit(inp,target, nb_epoch=1,batch_size=100)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dweitekamp/.local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch)\u001b[0m\n\u001b[1;32m   1194\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1196\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dweitekamp/.local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m    880\u001b[0m                         \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m                         \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m                     raise TypeError('TypeError while preparing batch. '\n",
      "\u001b[0;32m/home/dweitekamp/.local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mslice_X\u001b[0;34m(X, start, stop)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sweep = [0.0,0.00001,0.0001,0.001,0.01,0.025,0.1,1.0]\n",
    "mses = []\n",
    "models = []\n",
    "runs = {}\n",
    "for n in range(1):\n",
    "    for sp in [0.0]:#sweep:#[0.0]:\n",
    "        model = buildModel(sparsity_coeff=.05,injection_coeff=.02)\n",
    "        mse_vals = []\n",
    "        for i in range(20):\n",
    "            model.fit(inp, [target,np.ones((target.shape[0],SEQ_LEN,SEQ_LEN))], nb_epoch=1,batch_size=100)\n",
    "            #model.fit(inp,target, nb_epoch=1,batch_size=100)\n",
    "            p,u = model.predict(val)\n",
    "            val_mse = float(K.eval(K.mean(K.pow(p - val_target, 2))))\n",
    "            mse_vals.append(val_mse )\n",
    "            print(\"VAL_MSE:\", val_mse)\n",
    "        models.append(model)\n",
    "        #val_mse = np.random.randint(size=1,low=0,high=100)[0]#(min(mse_vals))\n",
    "        #print(runs.get(sp,[]))\n",
    "        runs[sp] = runs.get(sp,[]) + [min(mse_vals)]\n",
    "print(runs)\n",
    "import json\n",
    "json.dump(runs, open(os.path.abspath(\"/home/dweitekamp/custom_out1.txt\"),'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(sweep)\n",
    "print(mses)\n",
    "print(runs)\n",
    "\n",
    "#for m,sp in zip(models,sweep):\n",
    "#    print(sp)\n",
    "p,u = model.predict(val[9:19])\n",
    "for c in u:\n",
    "    plt.imshow(c)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "#d = {\"one\":1, \"two\":2}\n",
    "\n",
    "#50 floats btw 0-1 \n",
    "    #w/o tanh ptr_net\n",
    "    #Does not converge\n",
    "    #Tanh ptr_net\n",
    "    #0.0006992769549385588\n",
    "    \n",
    "    #w/o tanh custom\n",
    "    #0.0027380140176726468\n",
    "    #Tanh custom\n",
    "    #0.008146504258126774\n",
    "    \n",
    "    #ptr_net 5 epoch:\n",
    "    #[0.0, 0.001, 0.01, 0.1, 1.0, 10.0]\n",
    "    #[0.0005530958605927241, 0.0008726149090503212, 0.000940361804486735, 0.0006479378772081163, 0.002113587790538396, 0.00886053613683831]\n",
    "    \n",
    "    #ptr_net 15 epoch:\n",
    "    #[0.0, 1e-05, 0.0001, 0.001, 0.01, 0.025, 0.1, 1.0]\n",
    "    #[0.0005856020492227538, 0.000435507757379088, 0.0005835100906981886, 0.0007385180880908348, 0.00038265960976375526, 0.0004894350985301985, 0.00042241677262379437, 0.000939572834186924]\n",
    "\n",
    "\n",
    "#print(\"VAL_LOSS:\", model.evaluate(val,val_target,batch_size=1))\n",
    "#print(\"VAL_LOSS:\", model.evaluate(val,val_target,batch_size=10))\n",
    "#print(\"VAL_LOSS:\", model.evaluate(val,val_target,batch_size=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.engine.topology import Layer,initializations,Merge\n",
    "def null_cost(y_true, y_pred):\n",
    "    # output itself is cost and must be minimized\n",
    "    return K.sum(y_pred)*0 + K.sum(y_true)*0\n",
    "def buildModel(sparsity_coeff=0.0):\n",
    "    x = Input(shape=(SEQ_LEN, 4), name=\"input\")\n",
    "    e = GRU(128,return_sequences=True)(x)\n",
    "    s = Slice(\"[-1,:]\")(e)\n",
    "    # s = Slice('[-1,:]')(e)\n",
    "    # s = theano.printing.Print(\"s\")(s)\n",
    "    r = RepeatVector(SEQ_LEN)(s)\n",
    "    m = Merge(mode='concat',concat_axis=2)([r,x])\n",
    "    d = GRU(128,return_sequences=True)(m)\n",
    "    p,u = Ptr_Layer(20,sparsity_coeff=sparsity_coeff,return_U=True,\n",
    "                    implementation='custom',seq_len=SEQ_LEN)([x,e,d])\n",
    "\n",
    "    model = Model(input=x, output=[p,u], name='test')\n",
    "    model.compile(optimizer='adam',loss=['mse',null_cost],loss_weights=[1., 0.0])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Epoch 1/1\\n50000/50000 [==============================] - 780s - loss: 0.0016   \\n('VAL_MSE:', 8.391598572468554e-05)\\nEpoch 1/1\\n50000/50000 [==============================] - 786s - loss: 1.3540e-04   \\n('VAL_MSE:', 0.00015946542762654568)\\nEpoch 1/1\\n50000/50000 [==============================] - 781s - loss: 1.3643e-04   \\n('VAL_MSE:', 7.359234000334871e-05)\\nEpoch 1/1\\n50000/50000 [==============================] - 790s - loss: 1.1043e-04   \\n('VAL_MSE:', 0.00037620730748726995)\\nEpoch 1/1\\n50000/50000 [==============================] - 778s - loss: 9.5742e-05   \\n('VAL_MSE:', 0.0004603614554866676)\\nEpoch 1/1\\n50000/50000 [==============================] - 793s - loss: 8.6783e-05   \\n('VAL_MSE:', 0.0006976922938582105)\\nEpoch 1/1\\n50000/50000 [==============================] - 766s - loss: 8.3102e-05   \\n('VAL_MSE:', 0.0010426845653969696)\\nEpoch 1/1\\n50000/50000 [==============================] - 781s - loss: 8.1857e-05   \\n('VAL_MSE:', 0.0010114745970606362)\\nEpoch 1/1\\n50000/50000 [==============================] - 776s - loss: 1.7436e-04   \\n('VAL_MSE:', 0.0001110254697785855)\\nEpoch 1/1\\n32700/50000 [==================>...........] - ETA: 260s - loss: 1.2367e-04\\n\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Epoch 1/1\n",
    "50000/50000 [==============================] - 780s - loss: 0.0016   \n",
    "('VAL_MSE:', 8.391598572468554e-05)\n",
    "Epoch 1/1\n",
    "50000/50000 [==============================] - 786s - loss: 1.3540e-04   \n",
    "('VAL_MSE:', 0.00015946542762654568)\n",
    "Epoch 1/1\n",
    "50000/50000 [==============================] - 781s - loss: 1.3643e-04   \n",
    "('VAL_MSE:', 7.359234000334871e-05)\n",
    "Epoch 1/1\n",
    "50000/50000 [==============================] - 790s - loss: 1.1043e-04   \n",
    "('VAL_MSE:', 0.00037620730748726995)\n",
    "Epoch 1/1\n",
    "50000/50000 [==============================] - 778s - loss: 9.5742e-05   \n",
    "('VAL_MSE:', 0.0004603614554866676)\n",
    "Epoch 1/1\n",
    "50000/50000 [==============================] - 793s - loss: 8.6783e-05   \n",
    "('VAL_MSE:', 0.0006976922938582105)\n",
    "Epoch 1/1\n",
    "50000/50000 [==============================] - 766s - loss: 8.3102e-05   \n",
    "('VAL_MSE:', 0.0010426845653969696)\n",
    "Epoch 1/1\n",
    "50000/50000 [==============================] - 781s - loss: 8.1857e-05   \n",
    "('VAL_MSE:', 0.0010114745970606362)\n",
    "Epoch 1/1\n",
    "50000/50000 [==============================] - 776s - loss: 1.7436e-04   \n",
    "('VAL_MSE:', 0.0001110254697785855)\n",
    "Epoch 1/1\n",
    "32700/50000 [==================>...........] - ETA: 260s - loss: 1.2367e-04\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-a7f6dcbc4f4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_printoptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuppress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"VAL_MSE:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mval_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1000.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1000.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(suppress=True,threshold=500000)\n",
    "p,u = model.predict(val)\n",
    "print(\"VAL_MSE:\", float(K.eval(K.mean(K.pow(p - val_target, 2)))))\n",
    "for x,a,b,c in zip(val[:5]*1000,p[:5]*1000.0,val_target[:5]*1000.0,u[:5]):\n",
    "    print(c.shape)\n",
    "    plt.imshow(c)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    print(\"INPUT:\",flatten(x[:,0].tolist()))\n",
    "    print(\"PREDICT:\",flatten(a[:,0].tolist()))\n",
    "    print(\"TRUE   :\",flatten(b[:,0].tolist()))\n",
    "#print([np.concatenate(list(x), axis=-1) for ])\n",
    "#print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('VAL_MSE:', 0.018438392943366057)\n",
      "('INPUT:', [926.0, 697.0, 377.0, 809.0, 333.0, 697.0, 886.0, 875.0, 272.0, 213.0, 312.0, 678.0, 426.0, 127.0, 556.0, 192.0, 887.0, 989.0, 229.0, 245.0, 168.0, 949.0, 56.0, 956.0, 805.0, 12.0, 716.0, 786.0, 363.0, 822.0, 7.0, 592.0, 752.0, 921.0, 242.0, 232.0, 786.0, 858.0, 685.0, 639.0, 762.0, 883.0, 965.0, 556.0, 294.0, 174.0, 568.0, 810.0, 505.0, 719.0, 755.0, 607.0, 365.0, 454.0, 156.0, 132.0, 238.0, 402.0, 452.0, 882.0, 663.0, 326.0, 621.0, 583.0, 52.0, 528.0, 992.0, 195.0, 786.0, 256.0, 463.0, 155.0, 300.0, 606.0, 338.0, 583.0, 955.0, 149.0, 81.0, 905.0, 244.0, 560.0, 896.0, 945.0, 938.0, 447.0, 8.0, 484.0, 500.0, 132.0, 926.0, 382.0, 856.0, 901.0, 987.0, 23.0, 357.0, 982.0, 143.0, 498.0, 316.0, 354.0, 559.0, 315.0, 477.0, 753.0, 393.0, 160.0, 51.0, 206.0, 679.0, 385.0, 534.0, 339.0, 46.0, 362.0, 760.0, 336.0, 587.0, 398.0, 157.0, 515.0, 750.0, 17.0, 153.0, 779.0, 717.0, 921.0, 986.0, 538.0, 143.0, 120.0, 667.0, 346.0, 573.0, 878.0, 777.0, 607.0, 823.0, 351.0, 536.0, 647.0, 755.0, 551.0, 404.0, 10.0, 839.0, 855.0, 756.0, 238.0, 957.0, 513.0, 113.0, 353.0, 579.0, 711.0, 751.0, 531.0, 625.0, 591.0, 258.0, 235.0, 179.0, 588.0, 56.0, 208.0, 128.0, 201.0, 370.0, 232.0, 535.0, 342.0, 336.0, 380.0, 652.0, 522.0, 841.0, 946.0, 410.0, 107.0, 363.0, 324.0, 206.0, 370.0, 748.0, 613.0, 307.0, 524.0, 462.0, 336.0, 473.0, 826.0, 671.0, 88.0, 794.0, 353.0, 614.0, 158.0, 489.0, 147.0, 458.0, 925.0, 195.0, 699.0, 268.0, 683.0, 272.0, 454.0, 70.0, 297.0, 117.0, 626.0, 174.0, 721.0, 292.0, 790.0, 93.0, 521.0, 324.0, 596.0, 909.0, 733.0, 29.0, 403.0, 723.0, 700.0, 873.0, 700.0, 528.0, 956.0, 313.0, 893.0, 884.0, 279.0, 551.0, 106.0, 869.0, 883.0, 296.0, 725.0, 974.0, 368.0, 555.0, 227.0, 156.0, 749.0, 283.0, 955.0, 325.0, 852.0, 798.0, 551.0, 215.0, 78.0, 44.0, 390.0])\n",
      "('PREDICT:', [52.0, 56.0, 88.0, 120.0, 128.0, 143.0, 147.0, 158.0, 51.0, 81.0, 106.0, 127.00000762939453, 192.0, 195.0, 213.0, 272.0, 8.0, 10.0, 46.0, 70.0, 107.0, 132.0, 156.0, 238.0, 7.0, 12.0, 17.0, 23.0, 78.0, 93.0, 153.0, 307.0, 143.0, 155.0, 195.0, 208.0, 235.0, 258.0, 313.0, 316.0, 149.0, 160.0, 232.0, 268.0, 272.0, 300.0, 312.0, 385.0, 56.0, 117.0, 168.0, 227.0, 244.0, 297.0, 324.0, 370.0, 29.0, 113.0, 132.0, 215.0, 324.0, 326.0, 382.0, 390.0, 179.0, 201.0, 232.0, 256.0, 333.0, 357.0, 463.0, 528.0, 174.0, 279.0, 294.0, 296.0, 338.0, 342.0, 370.0, 454.0, 156.0, 174.0, 229.0, 245.0, 336.0, 339.0, 398.0, 404.0, 44.0, 157.0, 336.0, 353.0, 363.0, 403.0, 452.0, 524.0, 242.0, 315.0, 346.0, 353.0, 489.0, 528.0, 538.0, 671.0, 206.0, 336.0, 351.0, 393.0, 458.0, 522.0, 556.0, 606.0, 206.0, 362.0, 363.0, 368.0, 410.0, 447.0, 484.0, 505.0, 238.0, 402.0, 462.0, 473.0, 531.0, 551.0, 579.0, 583.0, 354.0, 377.0, 498.0, 559.0, 614.0, 639.0, 697.0, 723.0, 380.0, 426.0, 477.0, 535.0, 551.0, 556.0, 568.0, 607.0, 283.0, 365.0, 454.0, 513.0, 534.0, 555.0, 560.0, 607.0, 325.0, 515.0, 591.0, 596.0, 621.0, 663.0, 716.0, 733.0, 573.0, 588.0, 667.0, 697.0, 700.0, 752.0, 794.0, 875.0, 536.0, 551.0, 583.0, 647.0, 652.0, 725.0, 753.0, 762.0, 292.0, 626.0, 719.0, 721.0, 755.0, 756.0, 790.0, 841.0, 500.0, 521.0, 592.0, 625.0, 711.0, 750.0, 751.0, 779.0, 685.0, 700.0, 786.0, 858.0, 873.0, 878.0, 921.0, 926.0, 678.0, 679.0, 683.0, 777.0, 810.0, 823.0, 869.0, 883.0, 587.0, 749.0, 760.0, 839.0, 855.0, 887.0, 946.0, 955.0, 613.0, 717.0, 748.0, 786.0, 798.0, 822.0, 826.0, 856.0, 786.0, 809.0, 886.0, 893.0, 956.0, 982.0, 986.0, 992.0, 699.0, 755.0, 883.0, 884.0, 905.0, 925.0, 955.0, 965.0, 896.0, 938.0, 945.0, 949.0, 956.0, 957.0, 974.0, 989.0, 805.0, 852.0, 882.0, 901.0, 909.0, 921.0, 926.0, 987.0])\n",
      "('TRUE   :', [7.0, 8.0, 10.0, 12.0, 17.0, 23.0, 29.0, 44.0, 46.0, 51.0, 52.0, 56.0, 56.0, 70.0, 78.0, 81.0, 88.0, 93.0, 106.0, 107.0, 113.0, 117.0, 120.0, 127.0, 128.0, 132.0, 132.0, 143.0, 143.0, 147.0, 149.0, 153.0, 155.0, 156.0, 156.0, 157.0, 158.0, 160.0, 168.0, 174.0, 174.0, 179.0, 192.0, 195.0, 195.0, 201.0, 206.0, 206.0, 208.0, 213.0, 215.0, 227.0, 229.0, 232.0, 232.0, 235.0, 238.0, 238.0, 242.0, 244.0, 245.0, 256.0, 258.0, 268.0, 272.0, 272.0, 279.0, 283.0, 292.0, 294.0, 296.0, 297.0, 300.0, 307.0, 312.0, 313.0, 315.0, 316.0, 324.0, 324.0, 325.0, 326.0, 333.0, 336.0, 336.0, 336.0, 338.0, 339.0, 342.0, 346.0, 351.0, 353.0, 353.0, 354.0, 357.0, 362.0, 363.0, 363.0, 365.0, 368.0, 370.0, 370.0, 377.0, 380.0, 382.0, 385.0, 390.0, 393.0, 398.0, 402.0, 403.0, 404.0, 410.0, 426.0, 447.0, 452.0, 454.0, 454.0, 458.0, 462.0, 463.0, 473.0, 477.0, 484.0, 489.0, 498.0, 500.0, 505.0, 513.0, 515.0, 521.0, 522.0, 524.0, 528.0, 528.0, 531.0, 534.0, 535.0, 536.0, 538.0, 551.0, 551.0, 551.0, 555.0, 556.0, 556.0, 559.0, 560.0, 568.0, 573.0, 579.0, 583.0, 583.0, 587.0, 588.0, 591.0, 592.0, 596.0, 606.0, 607.0, 607.0, 613.0, 614.0, 621.0, 625.0, 626.0, 639.0, 647.0, 652.0, 663.0, 667.0, 671.0, 678.0, 679.0, 683.0, 685.0, 697.0, 697.0, 699.0, 700.0, 700.0, 711.0, 716.0, 717.0, 719.0, 721.0, 723.0, 725.0, 733.0, 748.0, 749.0, 750.0, 751.0, 752.0, 753.0, 755.0, 755.0, 756.0, 760.0, 762.0, 777.0, 779.0, 786.0, 786.0, 786.0, 790.0, 794.0, 798.0, 805.0, 809.0, 810.0, 822.0, 823.0, 826.0, 839.0, 841.0, 852.0, 855.0, 856.0, 858.0, 869.0, 873.0, 875.0, 878.0, 882.0, 883.0, 883.0, 884.0, 886.0, 887.0, 893.0, 896.0, 901.0, 905.0, 909.0, 921.0, 921.0, 925.0, 926.0, 926.0, 938.0, 945.0, 946.0, 949.0, 955.0, 955.0, 956.0, 956.0, 957.0, 965.0, 974.0, 982.0, 986.0, 987.0, 989.0, 992.0])\n",
      "('INPUT:', [519.0, 771.0, 641.0, 837.0, 335.0, 69.0, 345.0, 788.0, 734.0, 921.0, 803.0, 456.0, 912.0, 618.0, 939.0, 250.0, 33.0, 795.0, 535.0, 830.0, 564.0, 273.0, 738.0, 622.0, 782.0, 130.0, 213.0, 391.0, 684.0, 616.0, 456.0, 887.0, 876.0, 156.0, 489.0, 638.0, 501.0, 170.0, 547.0, 339.0, 759.0, 562.0, 970.0, 442.0, 156.0, 108.0, 636.0, 193.0, 565.0, 970.0, 643.0, 551.0, 903.0, 375.0, 820.0, 196.0, 758.0, 353.0, 753.0, 620.0, 668.0, 693.0, 9.0, 214.0, 340.0, 8.0, 359.0, 899.0, 947.0, 896.0, 997.0, 869.0, 867.0, 992.0, 76.0, 449.0, 817.0, 417.0, 534.0, 956.0, 191.0, 684.0, 706.0, 999.0, 936.0, 504.0, 720.0, 639.0, 586.0, 676.0, 410.0, 526.0, 429.0, 397.0, 612.0, 281.0, 33.0, 883.0, 678.0, 800.0, 997.0, 636.0, 684.0, 647.0, 626.0, 34.0, 42.0, 88.0, 548.0, 822.0, 91.0, 845.0, 227.0, 106.0, 529.0, 896.0, 858.0, 622.0, 609.0, 580.0, 975.0, 327.0, 869.0, 961.0, 68.0, 239.0, 907.0, 786.0, 26.0, 132.0, 217.0, 764.0, 419.0, 932.0, 438.0, 68.0, 165.0, 129.0, 500.0, 582.0, 744.0, 544.0, 123.0, 105.0, 876.0, 401.0, 576.0, 478.0, 983.0, 899.0, 246.0, 655.0, 936.0, 447.0, 414.0, 518.0, 423.0, 928.0, 4.0, 721.0, 651.0, 538.0, 688.0, 521.0, 505.0, 482.0, 298.0, 805.0, 965.0, 47.0, 376.0, 929.0, 232.0, 629.0, 640.0, 190.0, 9.0, 978.0, 570.0, 782.0, 385.0, 100.0, 152.0, 619.0, 753.0, 782.0, 310.0, 247.0, 10.0, 155.0, 513.0, 234.0, 724.0, 8.0, 170.0, 625.0, 749.0, 403.0, 553.0, 710.0, 217.0, 25.0, 779.0, 770.0, 971.0, 164.0, 476.0, 704.0, 847.0, 759.0, 984.0, 991.0, 854.0, 597.0, 444.0, 107.0, 772.0, 108.0, 866.0, 927.0, 620.0, 229.0, 440.0, 746.0, 369.0, 237.0, 937.0, 484.0, 215.0, 860.0, 793.0, 542.0, 585.0, 878.0, 757.0, 524.0, 170.0, 592.0, 915.0, 726.0, 396.0, 578.0, 792.0, 322.0, 378.0, 841.0, 160.0, 101.0, 191.0, 454.0, 984.0, 502.0, 347.0, 673.0, 573.0, 771.0])\n",
      "('PREDICT:', [8.0, 26.0, 68.0, 69.0, 170.0, 215.0, 298.0, 484.0, 25.0, 42.0, 88.0, 91.0, 105.0, 108.0, 156.0, 165.0, 9.0, 100.0, 101.0, 106.0, 152.0, 322.0, 378.0, 529.0, 4.0, 10.0, 108.0, 155.0, 191.0, 213.0, 214.0, 247.0, 8.0, 33.0, 170.0, 217.0, 335.0, 339.0, 345.0, 521.0, 34.0, 76.0, 123.0, 164.0, 170.0, 190.0, 217.0, 442.0, 33.0, 107.0, 160.0, 273.0, 385.0, 401.0, 478.0, 535.0, 9.0, 68.0, 130.0, 229.0, 234.0, 310.0, 391.0, 586.0, 132.0, 340.0, 359.0, 403.0, 419.0, 438.0, 542.0, 625.0, 47.0, 193.0, 232.0, 376.0, 417.0, 449.0, 476.0, 585.0, 191.0, 196.0, 246.0, 375.0, 551.0, 564.0, 570.0, 578.0, 239.0, 281.0, 327.0, 347.0, 410.0, 440.0, 501.9999694824219, 753.0, 156.0, 369.0, 482.0, 489.0, 501.0, 547.0, 638.0, 647.0, 129.0, 250.0, 456.0, 500.0, 534.0, 544.0, 548.0, 626.0, 227.0, 444.0, 504.0, 576.0, 580.0, 684.0, 782.0, 854.0, 353.0, 397.0, 414.0, 423.0, 456.0, 518.0, 721.0, 758.0, 237.0, 505.0, 519.0, 553.0, 636.0, 641.0, 710.0, 764.0, 524.0, 562.0, 592.0, 636.0, 704.0, 734.0, 744.0, 757.0, 396.0, 597.0, 609.0, 619.0, 643.0, 759.0, 830.0, 858.0, 429.0, 447.0, 513.0, 526.0, 616.0, 746.0, 753.0, 772.0, 538.0, 651.0, 678.0, 684.0, 749.0, 788.0, 800.0, 860.0, 582.0, 618.0, 640.0, 726.0, 759.0, 779.0, 817.0, 878.0, 565.0, 622.0, 622.0, 706.0, 720.0, 795.0, 896.0, 903.0, 454.0, 573.0, 620.0, 620.0, 673.0, 771.0, 782.0, 866.0, 688.0, 724.0, 771.0, 793.0, 805.0, 837.0, 896.0, 947.0, 629.0, 770.0, 803.0, 845.0, 867.0, 912.0, 915.0, 921.0, 639.0, 738.0, 792.0, 841.0, 847.0, 936.0, 970.0, 991.0, 612.0, 668.0, 676.0, 693.0, 786.0, 869.0, 927.0, 936.0, 869.0, 876.0, 883.0, 899.0, 932.0, 937.0, 997.0, 997.0, 822.0, 929.0, 939.0, 956.0, 965.0, 970.0, 971.0, 992.0, 655.0, 820.0, 876.0, 899.0, 978.0, 983.0, 984.0, 999.0, 684.0, 782.0, 887.0, 907.0, 928.0, 961.0, 975.0, 984.0])\n",
      "('TRUE   :', [4.0, 8.0, 8.0, 9.0, 9.0, 10.0, 25.0, 26.0, 33.0, 33.0, 34.0, 42.0, 47.0, 68.0, 68.0, 69.0, 76.0, 88.0, 91.0, 100.0, 101.0, 105.0, 106.0, 107.0, 108.0, 108.0, 123.0, 129.0, 130.0, 132.0, 152.0, 155.0, 156.0, 156.0, 160.0, 164.0, 165.0, 170.0, 170.0, 170.0, 190.0, 191.0, 191.0, 193.0, 196.0, 213.0, 214.0, 215.0, 217.0, 217.0, 227.0, 229.0, 232.0, 234.0, 237.0, 239.0, 246.0, 247.0, 250.0, 273.0, 281.0, 298.0, 310.0, 322.0, 327.0, 335.0, 339.0, 340.0, 345.0, 347.0, 353.0, 359.0, 369.0, 375.0, 376.0, 378.0, 385.0, 391.0, 396.0, 397.0, 401.0, 403.0, 410.0, 414.0, 417.0, 419.0, 423.0, 429.0, 438.0, 440.0, 442.0, 444.0, 447.0, 449.0, 454.0, 456.0, 456.0, 476.0, 478.0, 482.0, 484.0, 489.0, 500.0, 501.0, 502.0, 504.0, 505.0, 513.0, 518.0, 519.0, 521.0, 524.0, 526.0, 529.0, 534.0, 535.0, 538.0, 542.0, 544.0, 547.0, 548.0, 551.0, 553.0, 562.0, 564.0, 565.0, 570.0, 573.0, 576.0, 578.0, 580.0, 582.0, 585.0, 586.0, 592.0, 597.0, 609.0, 612.0, 616.0, 618.0, 619.0, 620.0, 620.0, 622.0, 622.0, 625.0, 626.0, 629.0, 636.0, 636.0, 638.0, 639.0, 640.0, 641.0, 643.0, 647.0, 651.0, 655.0, 668.0, 673.0, 676.0, 678.0, 684.0, 684.0, 684.0, 688.0, 693.0, 704.0, 706.0, 710.0, 720.0, 721.0, 724.0, 726.0, 734.0, 738.0, 744.0, 746.0, 749.0, 753.0, 753.0, 757.0, 758.0, 759.0, 759.0, 764.0, 770.0, 771.0, 771.0, 772.0, 779.0, 782.0, 782.0, 782.0, 786.0, 788.0, 792.0, 793.0, 795.0, 800.0, 803.0, 805.0, 817.0, 820.0, 822.0, 830.0, 837.0, 841.0, 845.0, 847.0, 854.0, 858.0, 860.0, 866.0, 867.0, 869.0, 869.0, 876.0, 876.0, 878.0, 883.0, 887.0, 896.0, 896.0, 899.0, 899.0, 903.0, 907.0, 912.0, 915.0, 921.0, 927.0, 928.0, 929.0, 932.0, 936.0, 936.0, 937.0, 939.0, 947.0, 956.0, 961.0, 965.0, 970.0, 970.0, 971.0, 975.0, 978.0, 983.0, 984.0, 984.0, 991.0, 992.0, 997.0, 997.0, 999.0])\n",
      "('INPUT:', [527.0, 249.0, 754.0, 258.0, 408.0, 11.0, 489.0, 144.0, 443.0, 824.0, 545.0, 981.0, 579.0, 220.0, 11.0, 61.0, 446.0, 845.0, 805.0, 498.0, 802.0, 369.0, 79.0, 529.0, 512.0, 917.0, 616.0, 111.0, 242.0, 942.0, 865.0, 555.0, 471.0, 879.0, 882.0, 375.0, 207.0, 311.0, 595.0, 183.0, 862.0, 421.0, 558.0, 262.0, 228.0, 888.0, 120.0, 304.0, 118.0, 723.0, 364.0, 340.0, 183.0, 82.0, 358.0, 491.0, 697.0, 357.0, 464.0, 95.0, 261.0, 842.0, 137.0, 978.0, 295.0, 597.0, 314.0, 909.0, 991.0, 692.0, 73.0, 27.0, 58.0, 316.0, 841.0, 143.0, 324.0, 7.0, 86.0, 133.0, 478.0, 6.0, 126.0, 371.0, 992.0, 787.0, 146.0, 179.0, 450.0, 492.0, 783.0, 663.0, 599.0, 103.0, 557.0, 172.0, 381.0, 158.0, 432.0, 813.0, 790.0, 312.0, 243.0, 33.0, 821.0, 593.0, 471.0, 604.0, 922.0, 215.0, 201.0, 182.0, 449.0, 922.0, 214.0, 140.0, 15.0, 291.0, 992.0, 165.0, 899.0, 228.0, 893.0, 117.0, 200.0, 857.0, 830.0, 264.0, 222.0, 843.0, 251.0, 609.0, 85.0, 204.0, 815.0, 140.0, 17.0, 137.0, 815.0, 696.0, 294.0, 912.0, 840.0, 113.0, 349.0, 581.0, 611.0, 711.0, 517.0, 789.0, 693.0, 117.0, 773.0, 118.0, 591.0, 269.0, 226.0, 773.0, 499.0, 32.0, 381.0, 204.0, 338.0, 851.0, 785.0, 821.0, 707.0, 630.0, 791.0, 260.0, 953.0, 7.0, 560.0, 394.0, 973.0, 979.0, 363.0, 823.0, 697.0, 493.0, 371.0, 865.0, 157.0, 504.0, 982.0, 346.0, 209.0, 936.0, 391.0, 709.0, 553.0, 396.0, 161.0, 267.0, 485.0, 804.0, 956.0, 754.0, 872.0, 483.0, 647.0, 791.0, 65.0, 788.0, 782.0, 895.0, 827.0, 280.0, 177.0, 244.0, 251.0, 708.0, 265.0, 10.0, 543.0, 169.0, 327.0, 304.0, 541.0, 621.0, 986.0, 902.0, 273.0, 278.0, 711.0, 602.0, 266.0, 874.0, 157.0, 960.0, 850.0, 745.0, 582.0, 544.0, 425.0, 366.0, 774.0, 460.0, 765.0, 598.0, 363.0, 679.0, 434.0, 253.0, 217.0, 785.0, 651.0, 997.0, 230.0, 943.0, 228.0, 168.0, 322.0, 663.0, 433.0, 889.0])\n",
      "('PREDICT:', [11.0, 27.0, 73.0, 85.0, 158.0, 161.0, 250.99998474121094, 258.0, 7.0, 7.0, 11.0, 17.0, 61.0, 65.0, 137.0, 228.0, 6.0, 10.0, 15.0, 79.0, 117.0, 118.0, 126.0, 140.0, 32.0, 95.0, 103.0, 118.0, 137.0, 200.0, 209.0, 230.0, 33.0, 157.0, 204.0, 204.0, 222.0, 243.0, 266.0, 375.0, 58.0, 86.0, 113.0, 143.0, 215.0, 260.0, 294.0, 425.0, 82.0, 146.0, 165.0, 177.0, 183.0, 214.0, 244.0, 252.99998474121094, 111.0, 172.0, 226.0, 228.0, 228.0, 273.0, 327.0, 663.0, 140.0, 207.0, 249.0, 295.0, 311.0, 314.0, 489.0, 609.0, 120.0, 133.0, 220.0, 262.0, 316.0, 324.0, 443.0, 471.0, 157.0, 169.0, 217.0, 250.99998474121094, 291.0, 340.0, 349.0, 581.0, 117.0, 242.0, 264.0, 304.0, 433.0, 450.0, 464.0, 709.0, 144.0, 267.0, 312.0, 338.0, 381.0, 408.0, 595.0, 804.0, 182.0, 201.0, 366.0, 394.0, 421.0, 545.0, 560.0, 582.0, 179.0, 265.0, 358.0, 363.0, 364.0, 369.0, 371.0, 679.0, 168.0, 261.0, 278.0, 346.0, 499.0, 512.0, 541.0, 773.0, 183.0, 381.0, 432.0, 597.0, 692.0, 707.0, 785.0, 813.0, 280.0, 460.0, 544.0, 558.0, 579.0, 604.0, 647.0, 765.0, 363.0, 371.0, 434.0, 491.0, 493.0, 543.0, 723.0, 785.0, 269.0, 322.0, 357.0, 396.0, 553.0, 591.0, 697.0, 842.0, 471.0, 483.0, 485.0, 602.0, 754.0, 790.0, 815.0, 851.0, 304.0, 593.0, 696.0, 774.0, 791.0, 815.0, 827.0, 888.0, 446.0, 498.0, 504.0, 517.0, 611.0, 651.0, 787.0, 823.0, 391.0, 492.0, 555.0, 557.0, 616.0, 621.0, 773.0, 857.0, 527.0, 630.0, 754.0, 821.0, 843.0, 850.0, 874.0, 956.0, 598.0, 782.0, 788.0, 791.0, 821.0, 840.0, 841.0, 895.0, 449.0, 529.0, 693.0, 697.0, 708.0, 789.0, 802.0, 845.0, 599.0, 663.0, 783.0, 830.0, 889.0, 899.0, 902.0, 917.0, 711.0, 745.0, 872.0, 879.0, 882.0, 909.0, 960.0, 991.0, 824.0, 862.0, 912.0, 922.0, 953.0, 973.0, 979.0, 981.0, 478.0, 711.0, 805.0, 865.0, 922.0, 992.0, 992.0, 997.0, 865.0, 893.0, 936.0, 942.0, 943.0, 978.0, 982.0, 986.0])\n",
      "('TRUE   :', [6.0, 7.0, 7.0, 10.0, 11.0, 11.0, 15.0, 17.0, 27.0, 32.0, 33.0, 58.0, 61.0, 65.0, 73.0, 79.0, 82.0, 85.0, 86.0, 95.0, 103.0, 111.0, 113.0, 117.0, 117.0, 118.0, 118.0, 120.0, 126.0, 133.0, 137.0, 137.0, 140.0, 140.0, 143.0, 144.0, 146.0, 157.0, 157.0, 158.0, 161.0, 165.0, 168.0, 169.0, 172.0, 177.0, 179.0, 182.0, 183.0, 183.0, 200.0, 201.0, 204.0, 204.0, 207.0, 209.0, 214.0, 215.0, 217.0, 220.0, 222.0, 226.0, 228.0, 228.0, 228.0, 230.0, 242.0, 243.0, 244.0, 249.0, 251.0, 251.0, 253.0, 258.0, 260.0, 261.0, 262.0, 264.0, 265.0, 266.0, 267.0, 269.0, 273.0, 278.0, 280.0, 291.0, 294.0, 295.0, 304.0, 304.0, 311.0, 312.0, 314.0, 316.0, 322.0, 324.0, 327.0, 338.0, 340.0, 346.0, 349.0, 357.0, 358.0, 363.0, 363.0, 364.0, 366.0, 369.0, 371.0, 371.0, 375.0, 381.0, 381.0, 391.0, 394.0, 396.0, 408.0, 421.0, 425.0, 432.0, 433.0, 434.0, 443.0, 446.0, 449.0, 450.0, 460.0, 464.0, 471.0, 471.0, 478.0, 483.0, 485.0, 489.0, 491.0, 492.0, 493.0, 498.0, 499.0, 504.0, 512.0, 517.0, 527.0, 529.0, 541.0, 543.0, 544.0, 545.0, 553.0, 555.0, 557.0, 558.0, 560.0, 579.0, 581.0, 582.0, 591.0, 593.0, 595.0, 597.0, 598.0, 599.0, 602.0, 604.0, 609.0, 611.0, 616.0, 621.0, 630.0, 647.0, 651.0, 663.0, 663.0, 679.0, 692.0, 693.0, 696.0, 697.0, 697.0, 707.0, 708.0, 709.0, 711.0, 711.0, 723.0, 745.0, 754.0, 754.0, 765.0, 773.0, 773.0, 774.0, 782.0, 783.0, 785.0, 785.0, 787.0, 788.0, 789.0, 790.0, 791.0, 791.0, 802.0, 804.0, 805.0, 813.0, 815.0, 815.0, 821.0, 821.0, 823.0, 824.0, 827.0, 830.0, 840.0, 841.0, 842.0, 843.0, 845.0, 850.0, 851.0, 857.0, 862.0, 865.0, 865.0, 872.0, 874.0, 879.0, 882.0, 888.0, 889.0, 893.0, 895.0, 899.0, 902.0, 909.0, 912.0, 917.0, 922.0, 922.0, 936.0, 942.0, 943.0, 953.0, 956.0, 960.0, 973.0, 978.0, 979.0, 981.0, 982.0, 986.0, 991.0, 992.0, 992.0, 997.0])\n",
      "('INPUT:', [888.0, 782.0, 704.0, 199.0, 175.0, 219.0, 465.0, 315.0, 822.0, 146.0, 88.0, 138.0, 219.0, 916.0, 499.0, 626.0, 732.0, 431.0, 182.0, 701.0, 306.0, 664.0, 219.0, 347.0, 672.0, 737.0, 419.0, 303.0, 521.0, 448.0, 810.0, 712.0, 988.0, 208.0, 115.0, 387.0, 633.0, 965.0, 954.0, 868.0, 895.0, 834.0, 899.0, 710.0, 798.0, 648.0, 966.0, 968.0, 145.0, 157.0, 335.0, 144.0, 314.0, 940.0, 310.0, 159.0, 471.0, 496.0, 458.0, 893.0, 944.0, 460.0, 878.0, 969.0, 28.0, 837.0, 56.0, 965.0, 298.0, 936.0, 301.0, 611.0, 161.0, 825.0, 799.0, 275.0, 821.0, 230.0, 976.0, 346.0, 754.0, 523.0, 653.0, 336.0, 238.0, 257.0, 81.0, 621.0, 973.0, 959.0, 47.0, 807.0, 932.0, 212.0, 272.0, 138.0, 409.0, 801.0, 67.0, 454.0, 771.0, 900.0, 707.0, 148.0, 530.0, 389.0, 958.0, 719.0, 633.0, 490.0, 291.0, 945.0, 512.0, 186.0, 240.0, 153.0, 282.0, 233.0, 242.0, 568.0, 723.0, 362.0, 658.0, 61.0, 104.0, 420.0, 776.0, 260.0, 172.0, 555.0, 729.0, 848.0, 34.0, 295.0, 366.0, 997.0, 361.0, 879.0, 687.0, 114.0, 742.0, 211.0, 618.0, 987.0, 113.0, 858.0, 762.0, 35.0, 454.0, 272.0, 776.0, 446.0, 157.0, 535.0, 460.0, 306.0, 805.0, 561.0, 93.0, 700.0, 250.0, 412.0, 152.0, 178.0, 725.0, 897.0, 97.0, 757.0, 692.0, 639.0, 223.0, 941.0, 76.0, 527.0, 53.0, 552.0, 688.0, 482.0, 93.0, 160.0, 490.0, 306.0, 699.0, 142.0, 743.0, 424.0, 521.0, 273.0, 654.0, 348.0, 574.0, 157.0, 280.0, 805.0, 670.0, 241.0, 884.0, 552.0, 366.0, 842.0, 276.0, 464.0, 753.0, 807.0, 592.0, 601.0, 974.0, 705.0, 197.0, 233.0, 54.0, 341.0, 633.0, 683.0, 553.0, 311.0, 322.0, 368.0, 685.0, 987.0, 25.0, 351.0, 513.0, 98.0, 340.0, 191.0, 220.0, 858.0, 974.0, 806.0, 722.0, 92.0, 435.0, 146.0, 869.0, 652.0, 24.0, 255.0, 473.0, 706.0, 922.0, 210.0, 691.0, 855.0, 222.0, 869.0, 624.0, 865.0, 245.0, 428.0, 293.0, 962.0, 603.0, 293.0, 493.0, 477.0])\n",
      "('PREDICT:', [28.0, 34.0, 56.0, 92.0, 97.0, 178.0, 191.0, 219.0, 24.0, 53.0, 88.0, 114.0, 146.0, 161.0, 211.0, 346.0, 35.0, 54.0, 81.0, 113.0, 142.0, 157.0, 222.0, 233.0, 25.0, 47.0, 61.0, 93.0, 98.0, 157.0, 212.0, 362.0, 67.0, 148.0, 172.0, 175.0, 199.0, 208.0, 295.0, 301.0, 76.0, 138.0, 146.0, 223.0, 230.0, 276.0, 291.0, 552.0, 93.0, 144.0, 145.0, 159.0, 186.0, 219.0, 238.0, 257.0, 104.0, 138.0, 245.0, 272.0, 273.0, 293.0, 293.0, 368.0, 115.0, 241.0, 250.0, 298.0, 315.0, 366.0, 412.0, 552.0, 219.0, 255.0, 275.0, 361.0, 389.0, 473.0, 626.0, 687.0, 153.0, 182.0, 197.0, 210.0, 242.0, 272.0, 282.0, 311.0, 157.0, 303.0, 322.0, 348.0, 419.0, 424.0, 493.0, 521.0, 152.0, 280.0, 366.0, 387.0, 555.0, 611.0, 633.0, 806.0, 435.0, 464.0, 490.0, 499.0, 592.0, 652.0, 705.0, 753.0, 160.0, 233.0, 240.0, 306.0, 306.0, 310.0, 347.0, 512.0, 260.0, 306.0, 351.0, 428.0, 458.0, 471.0, 513.0, 603.0, 220.0, 340.0, 454.0, 465.0, 725.0, 757.0, 782.0, 897.0, 527.0, 530.0, 618.0, 633.0, 639.0, 706.0, 710.0, 799.0, 314.0, 335.0, 336.0, 431.0, 446.0, 553.0, 664.0, 688.0, 420.0, 460.0, 477.0, 496.0, 574.0, 654.0, 672.0, 807.0, 409.0, 670.0, 707.0, 771.0, 801.0, 842.0, 848.0, 900.0, 601.0, 692.0, 719.0, 742.0, 825.0, 869.0, 945.0, 966.0, 341.0, 454.0, 482.0, 568.0, 624.0, 653.0, 683.0, 732.0, 448.0, 521.0, 535.0, 700.0, 723.0, 776.0, 805.0, 893.0, 704.0, 722.0, 805.0, 858.0, 868.0, 884.0, 888.0, 936.0, 648.0, 798.0, 807.0, 822.0, 834.0, 899.0, 968.0, 974.0, 490.0, 523.0, 621.0, 691.0, 699.0, 701.0, 754.0, 869.0, 460.0, 658.0, 712.0, 737.0, 743.0, 810.0, 932.0, 962.0, 729.0, 837.0, 954.0, 965.0, 965.0, 974.0, 988.0, 997.0, 821.0, 879.0, 895.0, 916.0, 941.0, 958.0, 976.0, 987.0, 633.0, 762.0, 776.0, 855.0, 858.0, 865.0, 922.0, 940.0, 561.0, 685.0, 878.0, 944.0, 959.0, 969.0, 973.0, 987.0])\n",
      "('TRUE   :', [24.0, 25.0, 28.0, 34.0, 35.0, 47.0, 53.0, 54.0, 56.0, 61.0, 67.0, 76.0, 81.0, 88.0, 92.0, 93.0, 93.0, 97.0, 98.0, 104.0, 113.0, 114.0, 115.0, 138.0, 138.0, 142.0, 144.0, 145.0, 146.0, 146.0, 148.0, 152.0, 153.0, 157.0, 157.0, 157.0, 159.0, 160.0, 161.0, 172.0, 175.0, 178.0, 182.0, 186.0, 191.0, 197.0, 199.0, 208.0, 210.0, 211.0, 212.0, 219.0, 219.0, 219.0, 220.0, 222.0, 223.0, 230.0, 233.0, 233.0, 238.0, 240.0, 241.0, 242.0, 245.0, 250.0, 255.0, 257.0, 260.0, 272.0, 272.0, 273.0, 275.0, 276.0, 280.0, 282.0, 291.0, 293.0, 293.0, 295.0, 298.0, 301.0, 303.0, 306.0, 306.0, 306.0, 310.0, 311.0, 314.0, 315.0, 322.0, 335.0, 336.0, 340.0, 341.0, 346.0, 347.0, 348.0, 351.0, 361.0, 362.0, 366.0, 366.0, 368.0, 387.0, 389.0, 409.0, 412.0, 419.0, 420.0, 424.0, 428.0, 431.0, 435.0, 446.0, 448.0, 454.0, 454.0, 458.0, 460.0, 460.0, 464.0, 465.0, 471.0, 473.0, 477.0, 482.0, 490.0, 490.0, 493.0, 496.0, 499.0, 512.0, 513.0, 521.0, 521.0, 523.0, 527.0, 530.0, 535.0, 552.0, 552.0, 553.0, 555.0, 561.0, 568.0, 574.0, 592.0, 601.0, 603.0, 611.0, 618.0, 621.0, 624.0, 626.0, 633.0, 633.0, 633.0, 639.0, 648.0, 652.0, 653.0, 654.0, 658.0, 664.0, 670.0, 672.0, 683.0, 685.0, 687.0, 688.0, 691.0, 692.0, 699.0, 700.0, 701.0, 704.0, 705.0, 706.0, 707.0, 710.0, 712.0, 719.0, 722.0, 723.0, 725.0, 729.0, 732.0, 737.0, 742.0, 743.0, 753.0, 754.0, 757.0, 762.0, 771.0, 776.0, 776.0, 782.0, 798.0, 799.0, 801.0, 805.0, 805.0, 806.0, 807.0, 807.0, 810.0, 821.0, 822.0, 825.0, 834.0, 837.0, 842.0, 848.0, 855.0, 858.0, 858.0, 865.0, 868.0, 869.0, 869.0, 878.0, 879.0, 884.0, 888.0, 893.0, 895.0, 897.0, 899.0, 900.0, 916.0, 922.0, 932.0, 936.0, 940.0, 941.0, 944.0, 945.0, 954.0, 958.0, 959.0, 962.0, 965.0, 965.0, 966.0, 968.0, 969.0, 973.0, 974.0, 974.0, 976.0, 987.0, 987.0, 988.0, 997.0])\n",
      "('INPUT:', [40.0, 285.0, 827.0, 861.0, 702.0, 772.0, 637.0, 383.0, 682.0, 371.0, 784.0, 558.0, 909.0, 438.0, 951.0, 343.0, 995.0, 543.0, 991.0, 766.0, 228.0, 341.0, 39.0, 2.0, 344.0, 36.0, 214.0, 437.0, 648.0, 938.0, 147.0, 981.0, 759.0, 696.0, 412.0, 700.0, 157.0, 979.0, 995.0, 797.0, 823.0, 629.0, 564.0, 894.0, 794.0, 80.0, 178.0, 316.0, 52.0, 375.0, 643.0, 654.0, 9.0, 679.0, 576.0, 603.0, 719.0, 256.0, 622.0, 341.0, 815.0, 503.0, 666.0, 847.0, 376.0, 686.0, 494.0, 431.0, 331.0, 754.0, 861.0, 625.0, 896.0, 893.0, 215.0, 98.0, 699.0, 206.0, 63.0, 483.0, 330.0, 359.0, 794.0, 261.0, 168.0, 495.0, 864.0, 225.0, 450.0, 716.0, 905.0, 398.0, 487.0, 640.0, 626.0, 370.0, 528.0, 399.0, 509.0, 879.0, 580.0, 383.0, 771.0, 559.0, 821.0, 440.0, 853.0, 17.0, 205.0, 188.0, 189.0, 633.0, 174.0, 951.0, 344.0, 698.0, 600.0, 453.0, 938.0, 202.0, 741.0, 246.0, 94.0, 603.0, 721.0, 680.0, 918.0, 46.0, 614.0, 52.0, 444.0, 534.0, 123.0, 931.0, 270.0, 807.0, 894.0, 774.0, 23.0, 286.0, 843.0, 510.0, 43.0, 452.0, 577.0, 409.0, 114.0, 8.0, 103.0, 62.0, 56.0, 486.0, 609.0, 225.0, 118.0, 520.0, 512.0, 856.0, 175.0, 597.0, 974.0, 445.0, 170.0, 383.0, 708.0, 73.0, 421.0, 149.0, 940.0, 556.0, 338.0, 36.0, 453.0, 682.0, 367.0, 594.0, 193.0, 585.0, 410.0, 969.0, 931.0, 508.0, 60.0, 242.0, 930.0, 429.0, 400.0, 867.0, 726.0, 175.0, 833.0, 879.0, 759.0, 159.0, 362.0, 362.0, 78.0, 320.0, 922.0, 338.0, 477.0, 318.0, 880.0, 342.0, 219.0, 872.0, 775.0, 374.0, 236.0, 972.0, 997.0, 877.0, 385.0, 291.0, 319.0, 117.0, 249.0, 761.0, 951.0, 937.0, 194.0, 194.0, 846.0, 353.0, 933.0, 684.0, 367.0, 104.0, 319.0, 394.0, 321.0, 536.0, 587.0, 88.0, 225.0, 352.0, 623.0, 902.0, 525.0, 308.0, 302.0, 325.0, 624.0, 592.0, 109.0, 94.0, 504.0, 207.0, 20.0, 996.0, 831.0, 903.0, 547.0, 503.0, 277.0, 422.0])\n",
      "('PREDICT:', [40.0, 52.0, 73.0, 78.0, 104.0, 149.0, 170.0, 270.0, 17.0, 23.0, 43.0, 80.0, 88.0, 205.0, 308.0, 477.0, 2.0, 8.0, 9.0, 39.0, 52.0, 62.0, 114.0, 325.0, 20.0, 36.0, 46.0, 94.0, 147.0, 175.0, 194.0, 341.0, 123.0, 159.0, 320.0, 321.0, 338.0, 362.0, 362.0, 376.0, 36.0, 63.0, 188.0, 215.0, 219.0, 316.0, 318.0, 587.0, 56.0, 94.0, 103.0, 117.0, 174.0, 261.0, 344.0, 359.0, 118.0, 175.0, 194.0, 225.0, 249.0, 353.0, 398.0, 487.0, 157.0, 285.0, 367.0, 383.0, 383.0, 383.0, 421.0, 528.0, 98.0, 178.0, 206.0, 225.0, 343.0, 371.0, 453.0, 682.0, 60.0, 109.0, 193.0, 202.0, 291.0, 375.0, 410.0, 592.0, 214.0, 246.0, 277.0, 344.0, 370.0, 437.0, 503.0000305175781, 512.0, 319.0, 394.0, 399.0, 412.0, 431.0, 536.0, 614.0, 637.0, 189.0, 286.0, 338.0, 374.0, 438.0, 440.0, 623.0, 821.0, 168.0, 207.0, 236.0, 319.0, 341.0, 409.0, 624.0, 654.0, 256.0, 400.0, 422.0, 450.0, 503.0000305175781, 520.0, 547.0, 626.0, 331.0, 444.0, 445.0, 534.0, 559.0, 754.0, 759.0, 771.0, 342.0, 367.0, 452.0, 510.0, 556.0, 564.0, 699.0, 823.0, 225.0, 228.0, 302.0, 453.0, 504.0, 543.0, 643.0, 698.0, 429.0, 597.0, 603.0, 609.0, 622.0, 640.0, 648.0, 666.0, 494.0, 580.0, 625.0, 684.0, 700.0, 759.0, 772.0, 861.0, 352.0, 483.0, 525.0, 629.0, 682.0, 784.0, 794.0, 894.0, 242.0, 330.0, 385.0, 495.0, 576.0, 585.0, 766.0, 794.0, 680.0, 716.0, 719.0, 721.0, 831.0, 833.0, 847.0, 867.0, 509.0, 686.0, 702.0, 797.0, 861.0, 922.0, 931.0, 933.0, 558.0, 594.0, 774.0, 775.0, 843.0, 853.0, 872.0, 896.0, 486.0, 508.0000305175781, 577.0, 600.0, 864.0, 877.0, 951.0, 991.0, 726.0, 741.0, 761.0, 846.0, 856.0, 879.0, 903.0, 905.0, 696.0, 708.0, 807.0, 827.0, 879.0, 974.0, 979.0, 995.0, 633.0, 880.0, 893.0, 894.0, 902.0, 909.0, 940.0, 951.0, 603.0, 679.0, 931.0, 938.0, 969.0, 972.0, 995.0, 997.0, 815.0, 918.0, 930.0, 937.0, 938.0, 951.0, 981.0, 996.0])\n",
      "('TRUE   :', [2.0, 8.0, 9.0, 17.0, 20.0, 23.0, 36.0, 36.0, 39.0, 40.0, 43.0, 46.0, 52.0, 52.0, 56.0, 60.0, 62.0, 63.0, 73.0, 78.0, 80.0, 88.0, 94.0, 94.0, 98.0, 103.0, 104.0, 109.0, 114.0, 117.0, 118.0, 123.0, 147.0, 149.0, 157.0, 159.0, 168.0, 170.0, 174.0, 175.0, 175.0, 178.0, 188.0, 189.0, 193.0, 194.0, 194.0, 202.0, 205.0, 206.0, 207.0, 214.0, 215.0, 219.0, 225.0, 225.0, 225.0, 228.0, 236.0, 242.0, 246.0, 249.0, 256.0, 261.0, 270.0, 277.0, 285.0, 286.0, 291.0, 302.0, 308.0, 316.0, 318.0, 319.0, 319.0, 320.0, 321.0, 325.0, 330.0, 331.0, 338.0, 338.0, 341.0, 341.0, 342.0, 343.0, 344.0, 344.0, 352.0, 353.0, 359.0, 362.0, 362.0, 367.0, 367.0, 370.0, 371.0, 374.0, 375.0, 376.0, 383.0, 383.0, 383.0, 385.0, 394.0, 398.0, 399.0, 400.0, 409.0, 410.0, 412.0, 421.0, 422.0, 429.0, 431.0, 437.0, 438.0, 440.0, 444.0, 445.0, 450.0, 452.0, 453.0, 453.0, 477.0, 483.0, 486.0, 487.0, 494.0, 495.0, 503.0, 503.0, 504.0, 508.0, 509.0, 510.0, 512.0, 520.0, 525.0, 528.0, 534.0, 536.0, 543.0, 547.0, 556.0, 558.0, 559.0, 564.0, 576.0, 577.0, 580.0, 585.0, 587.0, 592.0, 594.0, 597.0, 600.0, 603.0, 603.0, 609.0, 614.0, 622.0, 623.0, 624.0, 625.0, 626.0, 629.0, 633.0, 637.0, 640.0, 643.0, 648.0, 654.0, 666.0, 679.0, 680.0, 682.0, 682.0, 684.0, 686.0, 696.0, 698.0, 699.0, 700.0, 702.0, 708.0, 716.0, 719.0, 721.0, 726.0, 741.0, 754.0, 759.0, 759.0, 761.0, 766.0, 771.0, 772.0, 774.0, 775.0, 784.0, 794.0, 794.0, 797.0, 807.0, 815.0, 821.0, 823.0, 827.0, 831.0, 833.0, 843.0, 846.0, 847.0, 853.0, 856.0, 861.0, 861.0, 864.0, 867.0, 872.0, 877.0, 879.0, 879.0, 880.0, 893.0, 894.0, 894.0, 896.0, 902.0, 903.0, 905.0, 909.0, 918.0, 922.0, 930.0, 931.0, 931.0, 933.0, 937.0, 938.0, 938.0, 940.0, 951.0, 951.0, 951.0, 969.0, 972.0, 974.0, 979.0, 981.0, 991.0, 995.0, 995.0, 996.0, 997.0])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(suppress=True,threshold=500000)\n",
    "p = model.predict(val)\n",
    "print(\"VAL_MSE:\", float(K.eval(K.mean(K.pow(p - val_target, 2)))))\n",
    "for x,a,b in zip(val[:5]*1000,p[:5]*1000.0,val_target[:5]*1000.0):\n",
    "    #print(c.shape)\n",
    "    #plt.imshow(c)\n",
    "    #plt.colorbar()\n",
    "    #lt.show()\n",
    "    print(\"INPUT:\",flatten(x[:,0].tolist()))\n",
    "    print(\"PREDICT:\",flatten(a[:,0].tolist()))\n",
    "    print(\"TRUE   :\",flatten(b[:,0].tolist()))\n",
    "#print([np.concatenate(list(x), axis=-1) for ])\n",
    "#print(target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''CUSTOM 50 ints mapped to [0,1]\n",
    "('VAL_MSE:', 0.001569600218234435)\n",
    "('PREDICT:', [14.08072566986084, 24.556095123291016, 28.295637130737305, 30.411785125732422, 33.13698196411133, 37.32411193847656, 46.1008186340332, 53.2075080871582, 63.4344596862793, 72.48200988769531, 85.3075942993164, 93.4632568359375, 98.74871826171875, 108.7425765991211, 110.91658020019531, 123.70269775390625, 134.5385284423828, 143.88876342773438, 159.08541870117188, 181.8461456298828, 190.42434692382812, 203.7500457763672, 220.6626434326172, 239.72796630859375, 245.00320434570312, 246.1892547607422, 236.80545043945312, 245.6599884033203, 232.46725463867188, 239.04486083984375, 245.08265686035156, 248.84344482421875, 264.63299560546875, 285.3381652832031, 289.361083984375, 299.7632751464844, 313.9530944824219, 323.9080505371094, 322.595947265625, 334.1806335449219, 332.671875, 323.8444519042969, 335.371337890625, 339.9270935058594, 359.6188659667969, 392.69171142578125, 423.1754455566406, 479.6092224121094, 546.0584716796875, 667.3662719726562])\n",
    "('TRUE   :', [2.0, 15.0, 30.0, 33.0, 45.0, 51.0, 63.0, 65.0, 71.0, 76.0, 81.0, 91.0, 92.0, 93.0, 98.0, 103.0, 115.0, 127.0, 163.0, 188.0, 192.0, 193.0, 208.0, 223.0, 225.0, 229.0, 238.0, 247.0, 257.0, 263.0, 265.0, 272.0, 277.0, 282.0, 291.0, 293.0, 297.0, 313.0, 347.0, 350.0, 350.0, 368.0, 385.0, 410.0, 426.0, 436.0, 446.0, 450.0, 452.0, 484.0])\n",
    "('PREDICT:', [17.798559188842773, 33.463016510009766, 39.10608673095703, 43.00112533569336, 44.50833511352539, 48.93029022216797, 58.13621139526367, 64.9529037475586, 75.26228332519531, 85.29837799072266, 102.46138763427734, 113.64612579345703, 124.90634155273438, 140.658935546875, 146.15133666992188, 162.67030334472656, 172.6062469482422, 179.36094665527344, 194.64930725097656, 214.78004455566406, 219.95840454101562, 231.04476928710938, 246.4239959716797, 264.68646240234375, 269.5362548828125, 272.99102783203125, 266.89630126953125, 275.0792236328125, 260.5761413574219, 267.4671936035156, 272.8695373535156, 275.08282470703125, 289.3886413574219, 310.0047302246094, 313.9212951660156, 327.7631530761719, 342.7054748535156, 356.8041076660156, 354.9676208496094, 364.4736328125, 364.2120361328125, 353.3575744628906, 363.51544189453125, 365.5188903808594, 381.6290283203125, 414.7137756347656, 445.9398498535156, 501.1640930175781, 568.592529296875, 690.3679809570312])\n",
    "('TRUE   :', [24.0, 35.0, 42.0, 46.0, 54.0, 63.0, 67.0, 67.0, 68.0, 71.0, 119.0, 124.0, 148.0, 151.0, 151.0, 153.0, 162.0, 174.0, 180.0, 212.0, 214.0, 216.0, 219.0, 234.0, 243.0, 247.0, 257.0, 266.0, 274.0, 275.0, 282.0, 289.0, 294.0, 299.0, 320.0, 334.0, 343.0, 359.0, 387.0, 390.0, 393.0, 403.0, 404.0, 420.0, 427.0, 445.0, 477.0, 485.0, 491.0, 496.0])\n",
    "('PREDICT:', [15.987208366394043, 22.936832427978516, 24.929330825805664, 23.940183639526367, 26.281198501586914, 28.679780960083008, 35.26726150512695, 41.234291076660156, 49.22452926635742, 57.713165283203125, 69.93475341796875, 81.98285675048828, 92.2459487915039, 106.03779602050781, 111.96952056884766, 127.2327880859375, 139.74241638183594, 149.53489685058594, 164.30023193359375, 187.7772979736328, 195.0567169189453, 207.03494262695312, 223.97085571289062, 243.4896697998047, 249.49842834472656, 252.11972045898438, 244.68350219726562, 256.1998596191406, 243.84954833984375, 249.20632934570312, 256.2884826660156, 260.6770935058594, 276.7026062011719, 297.2806091308594, 304.63134765625, 313.7358703613281, 328.58837890625, 340.76373291015625, 339.2470703125, 354.3557434082031, 352.0670166015625, 342.35394287109375, 349.97125244140625, 354.6605224609375, 372.7105407714844, 405.00885009765625, 434.8564453125, 491.1487121582031, 557.1630859375, 677.7264404296875])\n",
    "('TRUE   :', [5.0, 7.0, 18.0, 19.0, 22.0, 34.0, 42.0, 46.0, 62.0, 67.0, 73.0, 76.0, 86.0, 91.0, 97.0, 126.0, 129.0, 136.0, 146.0, 188.0, 190.0, 221.0, 225.0, 239.0, 240.0, 244.0, 252.0, 267.0, 269.0, 280.0, 286.0, 288.0, 302.0, 305.0, 309.0, 311.0, 326.0, 327.0, 328.0, 346.0, 360.0, 376.0, 380.0, 383.0, 440.0, 441.0, 466.0, 471.0, 492.0, 498.0])\n",
    "('PREDICT:', [11.489262580871582, 21.53560447692871, 26.528684616088867, 31.471845626831055, 35.73295593261719, 39.66163635253906, 47.582332611083984, 53.18813705444336, 61.857879638671875, 68.80801391601562, 79.4161148071289, 87.07296752929688, 95.16581726074219, 105.8012924194336, 109.74620819091797, 124.03075408935547, 132.88026428222656, 140.9227752685547, 157.01925659179688, 179.16712951660156, 188.27259826660156, 203.6239776611328, 223.47901916503906, 243.16221618652344, 251.58175659179688, 261.0755615234375, 258.14581298828125, 272.0953369140625, 260.53173828125, 268.5575866699219, 278.5340881347656, 282.1059875488281, 296.9542541503906, 322.8467712402344, 329.10272216796875, 339.53363037109375, 352.66217041015625, 367.4928283691406, 364.8778076171875, 375.7081604003906, 372.43109130859375, 359.92608642578125, 368.53411865234375, 367.78338623046875, 379.3459777832031, 407.968017578125, 437.1671142578125, 491.94964599609375, 557.7061767578125, 677.7650146484375])\n",
    "('TRUE   :', [7.0, 18.0, 23.0, 35.0, 39.0, 58.0, 58.0, 58.0, 71.0, 72.0, 83.0, 93.0, 95.0, 105.0, 106.0, 107.0, 116.0, 118.0, 119.0, 121.0, 146.0, 152.0, 202.0, 209.0, 215.0, 243.0, 263.0, 264.0, 295.0, 306.0, 316.0, 327.0, 340.0, 345.0, 349.0, 358.0, 362.0, 363.0, 385.0, 387.0, 392.0, 407.0, 417.0, 418.0, 420.0, 452.0, 470.0, 480.0, 488.0, 497.0])\n",
    "('PREDICT:', [7.55507755279541, 16.36643409729004, 23.913782119750977, 32.16132354736328, 38.57605743408203, 45.92708206176758, 59.185604095458984, 69.88860321044922, 86.69200134277344, 104.30606079101562, 125.08720397949219, 140.39102172851562, 150.21304321289062, 162.2456512451172, 164.09713745117188, 178.21006774902344, 189.20077514648438, 200.25869750976562, 219.09280395507812, 244.91690063476562, 254.4220733642578, 267.34674072265625, 286.1042785644531, 302.54522705078125, 306.969482421875, 310.17462158203125, 300.88873291015625, 308.5691833496094, 298.9417724609375, 308.135498046875, 319.60076904296875, 323.387939453125, 340.26904296875, 363.6531066894531, 365.5494689941406, 374.900634765625, 384.005126953125, 393.2171630859375, 386.8927917480469, 394.5536193847656, 391.1708984375, 380.7538757324219, 390.9411315917969, 392.7253723144531, 409.16033935546875, 441.95941162109375, 472.1098327636719, 531.0437622070312, 600.9254760742188, 717.796875])\n",
    "('TRUE   :', [0.0, 2.0, 4.0, 28.0, 51.0, 67.0, 83.0, 88.0, 92.0, 98.0, 111.0, 121.0, 132.0, 134.0, 154.0, 185.0, 191.0, 194.0, 198.0, 226.0, 253.0, 263.0, 279.0, 284.0, 290.0, 304.0, 311.0, 313.0, 317.0, 340.0, 354.0, 374.0, 378.0, 387.0, 392.0, 396.0, 399.0, 403.0, 406.0, 407.0, 417.0, 420.0, 420.0, 458.0, 459.0, 469.0, 473.0, 476.0, 484.0, 492.0])'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
