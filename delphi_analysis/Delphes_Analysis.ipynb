{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We can go into our root file and see what Trees are availiable\n",
    "%matplotlib inline\n",
    "import sys, os\n",
    "if __package__ is None:\n",
    "    import sys, os\n",
    "    sys.path.append(os.path.realpath(\"/data/shared/Software/\"))\n",
    "    sys.path.append(os.path.realpath(\"../../\"))\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ntpath\n",
    "import glob\n",
    "from CMS_SURF_2016.utils.metrics import plot_history, print_accuracy_m\n",
    "from CMS_SURF_2016.utils.callbacks import OverfitStopping\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Reshape, Activation, Dropout, Convolution2D\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "#The observables taken from the table\n",
    "observ_types = ['E/c', 'Px', 'Py', 'Pz', 'PID', 'Charge']\n",
    "set_size = 20\n",
    "vecsize = len(observ_types)\n",
    "epochs = 100\n",
    "batch_size = 100\n",
    "\n",
    "#http://pdg.lbl.gov/2012/reviews/rpp2012-rev-monte-carlo-numbering.pdf\n",
    "observable_vocab = [0,-11,11,15,-15,-13,13,22]\n",
    "quark_vocab = [5,-5,6,-6]\n",
    "neutrino_vocab = [12,-12, 14, -14, 16, -16, 18, -18]\n",
    "unobservable_boson_vocab = [24, -24]\n",
    "\n",
    "particle_vocab = observable_vocab + neutrino_vocab + quark_vocab +  unobservable_boson_vocab\n",
    "particle_dict = {particle_vocab[i]:i for i in range(len(particle_vocab))}\n",
    "#print(particle_dict)\n",
    "def cullNonObservables(frame):\n",
    "    #Status of 1 means that the particle is a stable product\n",
    "    stable_cond = frame[\"Status\"] == 1 \n",
    "    #All even leptons are neutrinos which we can't measure\n",
    "    notNeutrino_cond = np.abs(frame[\"PID\"] % 2) == 1\n",
    "    #Get all entries that satisfy the conditions\n",
    "    frame = frame[stable_cond & notNeutrino_cond]\n",
    "    #Drop the Status frame since we only needed it to see if the particle was stable\n",
    "    frame = frame.drop([\"Status\"], axis=1)\n",
    "    return frame\n",
    "\n",
    "def mapPIDS(observables):\n",
    "    PIDS = observables[\"PID\"] \n",
    "    observables[\"PID\"] = PIDS.apply(lambda x: particle_dict[x])\n",
    "\n",
    "def padItem(x, shuffle=False):\n",
    "    if(len(x) > set_size):\n",
    "        return x[:set_size]\n",
    "    else:\n",
    "        out = np.append(x ,np.array(np.zeros((set_size - len(x), vecsize))), axis=0)\n",
    "        if(shuffle): np.random.shuffle(out)\n",
    "        return out\n",
    "def padInput(l,shuffle=False):\n",
    "    out = []\n",
    "    for x in l:\n",
    "        out.append(padItem(x, shuffle=shuffle))\n",
    "    return out\n",
    "\n",
    "def helper_gETA(x, arr):\n",
    "    arr.append(np.array(x))\n",
    "    return 0\n",
    "def groupEntriesToArrays(frame, select):\n",
    "    arr = []\n",
    "    grouped = frame.groupby([\"Entry\"])[select].apply(lambda x: helper_gETA(x,arr))\n",
    "    return arr\n",
    "def preprocessFromPandas_file_label_pairs(files, cull=False):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_train_by_label = {}\n",
    "    y_train_by_label = {}\n",
    "    for f,label in files:\n",
    "        all_particles = pd.read_hdf(f, 'data')\n",
    "        if(cull):\n",
    "            observables = cullNonObservables(all_particles)\n",
    "        else:\n",
    "            observables = all_particles\n",
    "        \n",
    "        mapPIDS(observables)\n",
    "        processedInput = padInput(groupEntriesToArrays(observables, observ_types))\n",
    "       \n",
    "        X_train_by_label[label] = X_train_by_label.get(label, []) + processedInput\n",
    "        y_train_by_label[label] = y_train_by_label.get(label, []) + ([label] * len(processedInput))\n",
    "    \n",
    "    #Truncate the data so that we have the same amount in each catagory\n",
    "    minimumN = min([len(X_train_by_label[label]) for label in X_train_by_label])\n",
    "    for label in X_train_by_label:\n",
    "        X_train = X_train + X_train_by_label[label][:minimumN]\n",
    "        y_train = y_train + y_train_by_label[label][:minimumN]\n",
    "    X_train_by_label = None\n",
    "    y_train_by_label = None\n",
    "    \n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    \n",
    "    indices = np.arange(len(y_train))\n",
    "    np.random.shuffle(indices)\n",
    "    X_train = X_train[indices]\n",
    "    y_train = y_train[indices]\n",
    "   \n",
    "    return X_train, y_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nFiles = 10\n",
    "ttbar_files = glob.glob(\"/data/shared/Delphes/ttbar_13TeV/parsed_pandas/*.h5\")\n",
    "WJet_files = glob.glob(\"/data/shared/Delphes/WJets_13TeV/parsed_pandas/*.h5\")\n",
    "files = []\n",
    "for i in range(nFiles):\n",
    "    files.append((ttbar_files[i],0))\n",
    "    files.append((WJet_files[i],1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143080 samples, validate on 35770 samples\n",
      "Epoch 1/100\n",
      "143080/143080 [==============================] - 2s - loss: 0.9967 - acc: 0.9291 - val_loss: 0.2404 - val_acc: 0.9829\n",
      "Epoch 2/100\n",
      "121600/143080 [========================>.....] - ETA: 0s - loss: 0.2281 - acc: 0.9837"
     ]
    }
   ],
   "source": [
    "histories = {}\n",
    "cull=False\n",
    "add_title = \"_AllParticles\"\n",
    "for i in range (2):\n",
    "   \n",
    "    X_train, y_train = preprocessFromPandas_file_label_pairs(files, cull=cull)\n",
    "    X_train_flatten = np.array([np.ndarray.flatten(x) for x in X_train])\n",
    "    #DENSE\n",
    "    dense = Sequential()\n",
    "    dense.add(Dense(10, input_dim=set_size * vecsize,activation='relu'))\n",
    "    #model.add(Dropout(.5))\n",
    "    dense.add(Dense(1, activation='sigmoid'))\n",
    "    dense.compile(loss='binary_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    #CONVOLUTIONAL\n",
    "    conv = Sequential()\n",
    "    conv.add(Convolution2D(40,4,4, input_shape=(1,set_size,vecsize),activation='relu'))\n",
    "    conv.add(Flatten())\n",
    "    conv.add(Dense(1, activation='sigmoid'))\n",
    "    conv.compile(loss='binary_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    earlyStopping = EarlyStopping(verbose=1, patience=10)\n",
    "    overfitStopping = OverfitStopping(verbose=1, patience=10)\n",
    "    #RUN Dense\n",
    "    dense_history = dense.fit(X_train_flatten, y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        nb_epoch=epochs,\n",
    "                        validation_split=.2,\n",
    "                        callbacks=[earlyStopping, overfitStopping])\n",
    "    histories[\"dense\"+add_title] = (dense,dense_history,X_train_flatten, y_train)\n",
    "#   plot_history([(\"dense\",dense_history)])\n",
    "\n",
    "    \n",
    "    earlyStopping = EarlyStopping(verbose=1, patience=10)\n",
    "    overfitStopping = OverfitStopping(verbose=1, patience=10)\n",
    "    #Run Conv\n",
    "    conv_history = conv.fit(np.reshape(X_train, (len(X_train), 1, set_size, vecsize)), y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        nb_epoch=epochs,\n",
    "                        validation_split=.2,\n",
    "                        callbacks=[earlyStopping, overfitStopping])\n",
    "    #plot_history([(\"conv\",conv_history)])\n",
    "    histories[\"conv\"+add_title] = (conv,conv_history,X_train, y_train)\n",
    "    add_title = \"_ObservableOnly\"\n",
    "    cull=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keys = [key for key in histories]\n",
    "def p(key):\n",
    "    tup = histories[key]\n",
    "    model = tup[0]\n",
    "    history = tup[1]\n",
    "    #print_accuracy_m(model, tup[2], tup[3])\n",
    "    print(key + ': Best Validation accuracy: %r%%' % max(history.history[\"val_acc\"]))\n",
    "    plot_history([(key, history)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p(keys[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p(keys[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p(keys[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p(keys[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
