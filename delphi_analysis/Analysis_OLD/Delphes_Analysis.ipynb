{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using gpu1\n",
      "using theano\n"
     ]
    }
   ],
   "source": [
    "#We can go into our root file and see what Trees are availiable\n",
    "%matplotlib inline\n",
    "import sys, os\n",
    "if __package__ is None:\n",
    "    import sys, os\n",
    "    sys.path.append(os.path.realpath(\"/data/shared/Software/\"))\n",
    "    sys.path.append(os.path.realpath(\"../../\"))\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ntpath\n",
    "import glob\n",
    "import deepconfig\n",
    "\n",
    "from CMS_Deep_Learning.utils.metrics import plot_history, print_accuracy_m\n",
    "from CMS_Deep_Learning.utils.callbacks import OverfitStopping, SmartCheckpoint\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Reshape, Activation, Dropout, Convolution2D\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "dc = deepconfig.deepconfig(gpu='gpu1', backend='theano')\n",
    "\n",
    "#The observables taken from the table\n",
    "observ_types = ['E/c', 'Px', 'Py', 'Pz', 'PID', 'Charge', \"PT\", \"Eta\", \"Phi\", \"Dxy\", \"Eem\", \"Ehad\"]\n",
    "set_size = 5\n",
    "vecsize = len(observ_types)\n",
    "epochs = 100\n",
    "batch_size = 100\n",
    "\n",
    "#http://pdg.lbl.gov/2012/reviews/rpp2012-rev-monte-carlo-numbering.pdf\n",
    "observable_vocab = [-11,11,-13,13,22]\n",
    "quark_vocab = [5,-5,6,-6]\n",
    "neutrino_vocab = [12,-12, 14, -14, 16, -16, 18, -18]\n",
    "unobservable_boson_vocab = [24, -24]\n",
    "jet_vocab = [100, -100]\n",
    "missing_ET_vocab = [83,84] #83 Reg, #84 Puppi \n",
    "Eflow_vocab = [89,90,91] #89 Track, #90 Photon #91 Hadron\n",
    "\n",
    "\n",
    "#particle_vocab = observable_vocab + neutrino_vocab + quark_vocab +  unobservable_boson_vocab + jet_vocab + missing_ET_vocab + Eflow_vocab\n",
    "particle_vocab = observable_vocab + missing_ET_vocab + Eflow_vocab\n",
    "particle_dict = {particle_vocab[i]:i for i in range(len(particle_vocab))}\n",
    "#print(particle_dict)\n",
    "def cullNonObservables(frame):\n",
    "    #Status of 1 means that the particle is a stable product\n",
    "    stable_cond = frame[\"Status\"] == 1 \n",
    "    #All even leptons are neutrinos which we can't measure\n",
    "    notNeutrino_cond = ((np.abs(frame[\"PID\"]) != 12) & (np.abs(frame[\"PID\"]) != 14) & (np.abs(frame[\"PID\"]) != 16)) \n",
    "    #Get all entries that satisfy the conditions\n",
    "    frame = frame[stable_cond & notNeutrino_cond]\n",
    "    #Drop the Status frame since we only needed it to see if the particle was stable\n",
    "    frame = frame.drop([\"Status\"], axis=1)\n",
    "    return frame\n",
    "\n",
    "def mapPIDS(observables):\n",
    "    PIDS = observables[\"PID\"] \n",
    "    observables[\"PID\"] = PIDS.apply(lambda x: particle_dict[x])\n",
    "\n",
    "def padItem(x, shuffle=False):\n",
    "    if(len(x) > set_size):\n",
    "        return x[:set_size]\n",
    "    else:\n",
    "        out = np.append(x ,np.array(np.zeros((set_size - len(x), vecsize))), axis=0)\n",
    "        if(shuffle): np.random.shuffle(out)\n",
    "        return out\n",
    "def padInput(l,shuffle=False):\n",
    "    #out = []\n",
    "    for i,x in enumerate(l):\n",
    "        #out.append(padItem(x, shuffle=shuffle))\n",
    "        l[i] = padItem(x, shuffle=shuffle)\n",
    "    return l\n",
    "\n",
    "def helper_gPID(x, pidDict):\n",
    "    pid = x.iloc[0]['PID']\n",
    "    x = x.drop([\"PID\"], axis=1)\n",
    "    pidDict[pid] = np.array(x)\n",
    "\n",
    "def helper_gEntry(x, arr, groupByPID=True):\n",
    "    if(groupByPID == True):\n",
    "        pidDict = {}\n",
    "        group = x.groupby([\"PID\"]).apply(lambda x: helper_gPID(x, pidDict))\n",
    "        arr.append(np.array(pidDict))\n",
    "    else:\n",
    "        arr.append(np.array(x))\n",
    "    return 0\n",
    "def groupEntriesToArrays(frame, select, groupByPID=True):\n",
    "    arr = []\n",
    "    \n",
    "    if(groupByPID == True):\n",
    "        arr = {}\n",
    "        grouped = frame.groupby([\"Entry\",\"PID\"])[select]#.apply(lambda x: helper_gEntry(x,arr,groupByPID=groupByPID))\n",
    "        keys = sorted(grouped.groups, key = lambda key:key[0])# [key for key in  grouped.groups].sort()\n",
    "        for key in  keys:\n",
    "            if((key[1] in arr) == False):\n",
    "                arr[key[1]] = []\n",
    "            arr[key[1]].append(grouped.get_group(key).values)\n",
    "            #print(key)\n",
    "            #print(grouped.get_group(key).values)\n",
    "            #print(np.array(grouped.get_group(key)))\n",
    "            #print()\n",
    "        #print([(x, len(groups[x])) for x in groups])\n",
    "    else:\n",
    "        grouped = frame.groupby([\"Entry\"])[select].apply(lambda x: helper_gEntry(x,arr,groupByPID=groupByPID))\n",
    "    #print(arr)\n",
    "    return arr\n",
    "def preprocessFromPandas_file_label_pairs(files, cull=False, groupByPID=True):\n",
    "    X_train = {}\n",
    "    y_train = []\n",
    "    X_train_by_label = {}\n",
    "    y_train_by_label = {}\n",
    "    max_size = 0\n",
    "    for f,label in files:\n",
    "        all_particles = pd.read_hdf(f, 'data')\n",
    "        cond = (all_particles[\"Entry\"] < 2)\n",
    "        all_particles = all_particles[cond]\n",
    "        if(cull==True):\n",
    "            observables = cullNonObservables(all_particles)\n",
    "        else:\n",
    "            observables = all_particles\n",
    "        #print(observables)\n",
    "        #cond = ((np.abs(observables[\"PID\"]) == 11) | (np.abs(observables[\"PID\"]) == 13) \\\n",
    "        #    | (np.abs(observables[\"PID\"]) == 22) | (np.abs(observables[\"PID\"]) == 83))\n",
    "        #cond = (np.abs(observables[\"PID\"]) != 91)&(observables[\"E/c\"] > 10)\n",
    "        #observables = observables[cond].sort([\"PID\"])\n",
    "        #print(observables)\n",
    "        #print(label, f)\n",
    "        #print(frame_cond)\n",
    "        \n",
    "        #mapPIDS(observables)\n",
    "        \n",
    "       \n",
    "            \n",
    "        grouped_arrays = groupEntriesToArrays(observables, observ_types, groupByPID=groupByPID)\n",
    "        #m = len(max(grouped_arrays,key=len))\n",
    "       \n",
    "        if(groupByPID == True):\n",
    "            for key in grouped_arrays:\n",
    "                padInput(grouped_arrays[key])\n",
    "            processedInput = grouped_arrays\n",
    "            if((label in X_train_by_label) == False):\n",
    "                X_train_by_label[label] = {key:[] for key in processedInput}\n",
    "            for key in processedInput:\n",
    "                X_train_by_label[label][key] = X_train_by_label[label].get(key, []) + processedInput[key]\n",
    "                \n",
    "            y_train_by_label[label] = y_train_by_label.get(label, []) + ([label] * len(processedInput))\n",
    "            \n",
    "            #print(X_train_by_label[label])\n",
    "        else:\n",
    "            processedInput = padInput(grouped_arrays)\n",
    "        #if( m > max_size):\n",
    "        #    max_size =m\n",
    "        \n",
    "        print(len(X_train_by_label[label]))\n",
    "    X_train = {key:[] for key in y_train_by_label[label]}\n",
    "    y_train = []\n",
    "    #print(X_train_by_label[0])\n",
    "    #Truncate the data so that we have the same amount in each catagory\n",
    "    minimumN = min([len(X_train_by_label[label][11.0]) for label in X_train_by_label])\n",
    "    print(\"mininumM:\", minimumN)\n",
    "    for label in X_train_by_label:\n",
    "        for key in X_train_by_label[label]:\n",
    "            X_train[key] = X_train[key] + X_train_by_label[label][key][:minimumN]\n",
    "        y_train = y_train + y_train_by_label[label][:minimumN]\n",
    "    X_train_by_label = None\n",
    "    y_train_by_label = None\n",
    "    \n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    \n",
    "    indices = np.arange(len(y_train))\n",
    "    np.random.shuffle(indices)\n",
    "    X_train = X_train[indices]\n",
    "    y_train = y_train[indices]\n",
    "   \n",
    "    print(\"MAX_SIZE:\", max_size)\n",
    "    return X_train, y_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('/data/shared/Delphes/ttbar_lepFilter_13TeV/pandas_joined/ttbar_lepFilter_13TeV_1.h5', 0), ('/data/shared/Delphes/wjets_lepFilter_13TeV/pandas_joined/wjets_lepFilter_13TeV_10.h5', 1), ('/data/shared/Delphes/ttbar_lepFilter_13TeV/pandas_joined/ttbar_lepFilter_13TeV_10.h5', 0), ('/data/shared/Delphes/wjets_lepFilter_13TeV/pandas_joined/wjets_lepFilter_13TeV_100.h5', 1)]\n"
     ]
    }
   ],
   "source": [
    "nFiles = 2\n",
    "ttbar_files = glob.glob(\"/data/shared/Delphes/ttbar_lepFilter_13TeV/pandas_joined/*.h5\")\n",
    "WJet_files = glob.glob(\"/data/shared/Delphes/wjets_lepFilter_13TeV/pandas_joined/*.h5\")\n",
    "qcd_files = glob.glob(\"/data/shared/Delphes/qcd_lepFilter_13TeV/pandas_joined/*.h5\")\n",
    "#print(WJet_files)\n",
    "files = []\n",
    "for i in range(nFiles):\n",
    "    files.append((ttbar_files[i+1],0))\n",
    "    files.append((WJet_files[i+1],1))\n",
    "\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "5\n",
      "6\n",
      "6\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "11.0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-04a3ee24a8f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessFromPandas_file_label_pairs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcull\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-e03cacd07c3c>\u001b[0m in \u001b[0;36mpreprocessFromPandas_file_label_pairs\u001b[1;34m(files, cull, groupByPID)\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[1;31m#print(X_train_by_label[0])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[1;31m#Truncate the data so that we have the same amount in each catagory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m     \u001b[0mminimumN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_by_label\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m11.0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mX_train_by_label\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"mininumM:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminimumN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mX_train_by_label\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 11.0"
     ]
    }
   ],
   "source": [
    "X_train, y_train = preprocessFromPandas_file_label_pairs(files, cull=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "histories = {}\n",
    "cull=False\n",
    "add_title = \"_AllParticles\"\n",
    "for i in range (1):\n",
    "   \n",
    "    X_train, y_train = preprocessFromPandas_file_label_pairs(files, cull=cull)\n",
    "    X_train_flatten = np.array([np.ndarray.flatten(x) for x in X_train])\n",
    "    #DENSE\n",
    "    dense = Sequential()\n",
    "    dense.add(Dense(10, input_dim=set_size * vecsize,activation='relu'))\n",
    "    #model.add(Dropout(.5))\n",
    "    dense.add(Dense(1, activation='sigmoid'))\n",
    "    dense.compile(loss='binary_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    #CONVOLUTIONAL\n",
    "    #conv = Sequential()\n",
    "    #conv.add(Convolution2D(40,4,4, input_shape=(1,set_size,vecsize),activation='relu'))\n",
    "    #conv.add(Flatten())\n",
    "    #conv.add(Dense(1, activation='sigmoid'))\n",
    "    #conv.compile(loss='binary_crossentropy',\n",
    "    #              optimizer='rmsprop',\n",
    "    #              metrics=['accuracy'])\n",
    "    \n",
    "    earlyStopping = EarlyStopping(verbose=1, patience=10)\n",
    "    overfitStopping = OverfitStopping(verbose=1, patience=20)\n",
    "    #smartCheckpoint = SmartCheckpoint(\"dense\"+add_title)\n",
    "    #RUN Dense\n",
    "    dense_history = dense.fit(X_train_flatten, y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        nb_epoch=epochs,\n",
    "                        validation_split=.2,\n",
    "                        callbacks=[earlyStopping, overfitStopping])\n",
    "    histories[\"dense\"+add_title] = (dense,dense_history,X_train_flatten, y_train)\n",
    "#   plot_history([(\"dense\",dense_history)])\n",
    "\n",
    "    \n",
    "    #earlyStopping = EarlyStopping(verbose=1, patience=10)\n",
    "    #overfitStopping = OverfitStopping(verbose=1, patience=10)\n",
    "    #Run Conv\n",
    "    #conv_history = conv.fit(np.reshape(X_train, (len(X_train), 1, set_size, vecsize)), y_train,\n",
    "    #                    batch_size=batch_size,\n",
    "    #                    nb_epoch=epochs,\n",
    "    #                    validation_split=.2,\n",
    "    #                    callbacks=[earlyStopping, overfitStopping])\n",
    "    #plot_history([(\"conv\",conv_history)])\n",
    "    #histories[\"conv\"+add_title] = (conv,conv_history,X_train, y_train)\n",
    "    #add_title = \"_ObservableOnly\"\n",
    "    #cull=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keys = [key for key in histories]\n",
    "def p(key):\n",
    "    tup = histories[key]\n",
    "    model = tup[0]\n",
    "    history = tup[1]\n",
    "    #print_accuracy_m(model, tup[2], tup[3])\n",
    "    print(key + ': Best Validation accuracy: %r%%' % max(history.history[\"val_acc\"]))\n",
    "    plot_history([(key, history)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p(keys[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p(keys[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p(keys[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p(keys[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
