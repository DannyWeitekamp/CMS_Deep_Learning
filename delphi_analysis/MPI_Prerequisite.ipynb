{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is disabled, cuDNN 5005)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n",
      "using gpu0\n",
      "using theano\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#We can go into our root file and see what Trees are availiable\n",
    "%matplotlib inline\n",
    "import sys, os\n",
    "if __package__ is None:\n",
    "    import sys, os\n",
    "    sys.path.append(os.path.realpath(\"/data/shared/Software/\"))\n",
    "    sys.path.append(os.path.realpath(\"../../\"))\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ntpath\n",
    "import glob\n",
    "import deepconfig\n",
    "\n",
    "#from keras.utils.visualize_util import plot\n",
    "#from IPython.display import Image, display\n",
    "\n",
    "from CMS_Deep_Learning.preprocessing.preprocessing import *\n",
    "from CMS_Deep_Learning.callbacks import OverfitStopping, SmartCheckpoint\n",
    "from CMS_Deep_Learning.storage.batch import batchAssertArchived, batchExecuteAndTestTrials\n",
    "from CMS_Deep_Learning.storage.archiving import *\n",
    "from CMS_Deep_Learning.postprocessing.analysistools import findsubsets\n",
    "from CMS_Deep_Learning.layers.lorentz import Lorentz, _lorentz\n",
    "from CMS_Deep_Learning.layers.slice import Slice\n",
    "\n",
    "from keras.models import Sequential, Model, model_from_json\n",
    "from keras.layers import Dense, Flatten, Reshape, Activation, Dropout, Convolution2D, merge, Input, Flatten, Lambda, LSTM, Masking\n",
    "from keras.engine.topology import Layer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils.visualize_util import plot\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "\n",
    "\n",
    "dc = deepconfig.deepconfig(gpu='gpu0', backend='theano')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The observables taken from the table\n",
    "observ_types = ['E/c', 'Px', 'Py', 'Pz', 'PT_ET','Eta', 'Phi', 'Charge', 'X', 'Y', 'Z',\\\n",
    "                     'Dxy', 'Ehad', 'Eem', 'MuIso', 'EleIso', 'ChHadIso','NeuHadIso','GammaIso', \"ObjType\"]\n",
    "vecsize = len(observ_types)\n",
    "epochs = 60\n",
    "batch_size = 100\n",
    "\n",
    "label_dir_pairs = \\\n",
    "            [   (\"ttbar\", \"/data/shared/Delphes/ttbar_lepFilter_13TeV/pandas_h5/\"),\n",
    "                (\"wjet\", \"/data/shared/Delphes/wjets_lepFilter_13TeV/pandas_h5/\"),\n",
    "                (\"qcd\", \"/data/shared/Delphes/qcd_lepFilter_13TeV/pandas_h5/\")\n",
    "            ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Find all the subsets of label_dir_pairs and store them as sorted lists\n",
    "#ldpsubsets = [sorted(list(s)) for s in findsubsets(label_dir_pairs)]\n",
    "#Make sure that we do 3-way classification as well\n",
    "ldpsubsets = []\n",
    "ldpsubsets.append(label_dir_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def genModel(name,object_profiles,out_dim, depth, lstm_activation=\"relu\", lstm_dropout = 0.0, dropout=0.0):\n",
    "    inputs = []\n",
    "    mergelist = []\n",
    "    for i, profile in enumerate(object_profiles):\n",
    "        inp = a = Input(shape=(profile.max_size + 1*(profile.punctuation != None), vecsize), name=\"input_\"+str(i))\n",
    "        inputs.append(inp)\n",
    "        mergelist.append(a)\n",
    "    a = merge(mergelist,mode='concat',concat_axis=1, name=\"merge\")\n",
    "    for i in range(depth):\n",
    "        a = Masking(mask_value=0.0)(a)\n",
    "        a = LSTM(vecsize,\n",
    "                 input_shape=(None,vecsize),\n",
    "                 dropout_W=lstm_dropout,\n",
    "                 dropout_U=lstm_dropout,\n",
    "                 activation=lstm_activation,\n",
    "                 name = \"lstm_\" +str(i))(a)\n",
    "        if(dropout > 0.0):\n",
    "            a =  Dropout(dropout, name=\"dropout_\"+str(i))(a)\n",
    "    dense_out = Dense(out_dim, activation='softmax', name='main_output')(a)\n",
    "    model = Model(input=inputs, output=dense_out, name=name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def symlinkFolderFromDPS(dps, folder):\n",
    "    if(not os.path.exists(folder)):\n",
    "        os.makedirs(folder)\n",
    "    for i, dp in enumerate(dps):\n",
    "        path = folder + \"%03d\"% i + \".h5\"\n",
    "        try:\n",
    "            os.unlink(path)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        os.symlink(dp.get_path()+\"archive.h5\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "def readH5(filepath):\n",
    "    h5f = h5py.File(filepath, 'r')\n",
    "    X = []\n",
    "    X_group = h5f['X']\n",
    "    keys = list(X_group.keys())\n",
    "    keys.sort()\n",
    "    for key in keys:\n",
    "        X.append(X_group[key][:])\n",
    "\n",
    "\n",
    "    Y = []\n",
    "    Y_group = h5f['Y']\n",
    "    keys = list(Y_group.keys())\n",
    "    keys.sort()\n",
    "    for key in keys:\n",
    "        Y.append(Y_group[key][:])\n",
    "\n",
    "    h5f.close()\n",
    "    out = (X, Y)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batchAssertArchived...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-60e0c1655d2a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m#Don't worry about dependencies, this is for systems like CSCS where thing are run in parellel batches, but on titans it does nothing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m#batchAssertArchived is very important though, it makes sure that each 100MB chunk of data is actually archived.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mdependencies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatchAssertArchived\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m   \u001b[0mnum_val\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/data/shared/Software/CMS_Deep_Learning/storage/batch.pyc\u001b[0m in \u001b[0;36mbatchAssertArchived\u001b[1;34m(dps, time_str, repo, dp_out_dir, verbose)\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Starting batchAssertArchived...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mu\u001b[0m \u001b[1;32min\u001b[0m \u001b[0munarchived\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m             \u001b[0mu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marchive\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Done.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdependencies\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/data/shared/Software/CMS_Deep_Learning/storage/archiving.pyc\u001b[0m in \u001b[0;36mgetData\u001b[1;34m(self, archive, redo, verbose)\u001b[0m\n\u001b[0;32m    272\u001b[0m             \u001b[0mprep_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc_module\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 274\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprep_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/data/shared/Software/CMS_Deep_Learning/preprocessing/preprocessing.pyc\u001b[0m in \u001b[0;36mpreprocessFromPandas_label_dir_pairs\u001b[1;34m(label_dir_pairs, start, samples_per_label, object_profiles, observ_types, single_list, sort_columns, sort_ascending, verbose)\u001b[0m\n\u001b[0;32m    393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m             groupBys,store = _groupsByEntry(f, storeType, samples_per_label,samples_to_read, file_total_entries,\n\u001b[1;32m--> 395\u001b[1;33m                                             num_val_frame,file_start_read,object_profiles)\n\u001b[0m\u001b[0;32m    396\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m             \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Values/Sample from: %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_size\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mobject_profiles\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/data/shared/Software/CMS_Deep_Learning/preprocessing/preprocessing.pyc\u001b[0m in \u001b[0;36m_groupsByEntry\u001b[1;34m(f, storeType, samples_per_label, samples_to_read, file_total_entries, num_val_frame, file_start_read, object_profiles)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m         frame = _getFrame(store, storeType, key, select_start, select_stop,\n\u001b[1;32m--> 244\u001b[1;33m                           samples_to_read, file_total_entries,frames)\n\u001b[0m\u001b[0;32m    245\u001b[0m         \u001b[1;31m#Group by Entry\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m         \u001b[0mgroupBys\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Entry\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/data/shared/Software/CMS_Deep_Learning/preprocessing/preprocessing.pyc\u001b[0m in \u001b[0;36m_getFrame\u001b[1;34m(store, storeType, key, select_start, select_stop, samples_to_read, file_total_entries, frames)\u001b[0m\n\u001b[0;32m    218\u001b[0m             \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m             \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mselect_start\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mselect_stop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    221\u001b[0m     \u001b[1;32melif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstoreType\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"msgpack\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mframes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/io/pytables.pyc\u001b[0m in \u001b[0;36mselect\u001b[1;34m(self, key, where, start, stop, columns, iterator, chunksize, auto_close, **kwargs)\u001b[0m\n\u001b[0;32m    672\u001b[0m                            auto_close=auto_close)\n\u001b[0;32m    673\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 674\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    676\u001b[0m     def select_as_coordinates(\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/io/pytables.pyc\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self, coordinates)\u001b[0m\n\u001b[0;32m   1366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m         \u001b[1;31m# directly return the result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1368\u001b[1;33m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1369\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1370\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/io/pytables.pyc\u001b[0m in \u001b[0;36mfunc\u001b[1;34m(_start, _stop, _where)\u001b[0m\n\u001b[0;32m    665\u001b[0m             return s.read(start=_start, stop=_stop,\n\u001b[0;32m    666\u001b[0m                           \u001b[0mwhere\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_where\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 667\u001b[1;33m                           columns=columns, **kwargs)\n\u001b[0m\u001b[0;32m    668\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m         \u001b[1;31m# create the iterator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/io/pytables.pyc\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, where, columns, **kwargs)\u001b[0m\n\u001b[0;32m   4006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4008\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_axes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4009\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/io/pytables.pyc\u001b[0m in \u001b[0;36mread_axes\u001b[1;34m(self, where, **kwargs)\u001b[0m\n\u001b[0;32m   3213\u001b[0m         \u001b[1;31m# create the selection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3214\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSelection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3215\u001b[1;33m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3217\u001b[0m         \u001b[1;31m# convert the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/io/pytables.pyc\u001b[0m in \u001b[0;36mselect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   4535\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoordinates\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4536\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_coordinates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoordinates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4537\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4538\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4539\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mselect_coords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tables/table.pyc\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, start, stop, step, field, out)\u001b[0m\n\u001b[0;32m   1963\u001b[0m                                                   warn_negstep=False)\n\u001b[0;32m   1964\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1965\u001b[1;33m         \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1966\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minternal_to_flavor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflavor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1967\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tables/table.pyc\u001b[0m in \u001b[0;36m_read\u001b[1;34m(self, start, stop, step, field, out)\u001b[0m\n\u001b[0;32m   1876\u001b[0m             \u001b[1;31m# This optimization works three times faster than\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1877\u001b[0m             \u001b[1;31m# the row._fill_col method (up to 170 MB/s on a pentium IV @ 2GHz)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1878\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1879\u001b[0m         \u001b[1;31m# Warning!: _read_field_name should not be used until\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1880\u001b[0m         \u001b[1;31m# H5TBread_fields_name in tableextension will be finished\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "archive_dir = \"/data/shared/Delphes/keras_archive/\"\n",
    "dustin_dir = \"/data/shared/Delphes/dustin_MPI_files/\"\n",
    "patience = 8\n",
    "earlyStopping = EarlyStopping(verbose=1, patience=patience)\n",
    "#trial_tups = []\n",
    "#Loop over all subsets\n",
    "for ldp in ldpsubsets:\n",
    "    labels = [x[0] for x in ldp]\n",
    "    #for sort_on in [\"PT_ET\", \"Phi\", \"Eta\"]:\n",
    "    for sort_on in [\"Phi\"]:\n",
    "        #Use object maxes from Find_Maxes_From Query\n",
    "        object_profiles = [ObjectProfile(\"Electron\",-1, pre_sort_columns=[\"PT_ET\"], pre_sort_ascending=False, sort_columns=[sort_on], sort_ascending=False, addColumns={\"ObjType\":1}),\n",
    "                            ObjectProfile(\"MuonTight\", -1, pre_sort_columns=[\"PT_ET\"], pre_sort_ascending=False, sort_columns=[sort_on], sort_ascending=False, addColumns={\"ObjType\":2}),\n",
    "                            ObjectProfile(\"Photon\", -1, pre_sort_columns=[\"PT_ET\"], pre_sort_ascending=False, sort_columns=[sort_on], sort_ascending=False, addColumns={\"ObjType\":3}),\n",
    "                            ObjectProfile(\"MissingET\", 1, addColumns={\"ObjType\":4}),\n",
    "                            ObjectProfile(\"EFlowPhoton\",100, pre_sort_columns=[\"PT_ET\"], pre_sort_ascending=False, sort_columns=[sort_on], sort_ascending=False, addColumns={\"ObjType\":5}), \n",
    "                            ObjectProfile(\"EFlowNeutralHadron\",100, pre_sort_columns=[\"PT_ET\"], pre_sort_ascending=False, sort_columns=[sort_on], sort_ascending=False, addColumns={\"ObjType\":6}), \n",
    "                            ObjectProfile(\"EFlowTrack\",100, pre_sort_columns=[\"PT_ET\"], pre_sort_ascending=False, sort_columns=[sort_on], sort_ascending=False, addColumns={\"ObjType\":7})]  \n",
    "        #This will replace the -1's used in the previous lines with the acutal maximum number of particles for each object type\n",
    "        resolveProfileMaxes(object_profiles, ldp)\n",
    "        \n",
    "        #This outputs generators for training and validation with 75,000 events per process and 20,000 events per process respectively in files of roughly 100MB\n",
    "        dps, l = getGensDefaultFormat(archive_dir, (75000,20000), 115000, \\\n",
    "                             object_profiles,ldp,observ_types,megabytes=100, verbose=0)\n",
    "        \n",
    "        #Don't worry about dependencies, this is for systems like CSCS where thing are run in parellel batches, but on titans it does nothing\n",
    "        #batchAssertArchived is very important though, it makes sure that each 100MB chunk of data is actually archived.\n",
    "        dependencies = batchAssertArchived(dps)\n",
    "        train, num_train = l[0]\n",
    "        val,   num_val   = l[1]\n",
    "        max_q_size = l[2]\n",
    "        print(\"MAXQ: \",max_q_size)\n",
    "        \n",
    "        #Generate the model\n",
    "        name = 'LSTM'\n",
    "        model = genModel('LSTM',object_profiles, len(labels), 1, 'tanh', 0.0, 0.0)\n",
    "        model.summary()\n",
    "        \n",
    "        #Write it to a .json file, write_json_obj is in CMS_SURF_2016.utils.archiving\n",
    "        write_json_obj(model.to_json(), dustin_dir, \"model.json\")\n",
    "        \n",
    "        #Get the archived DataProcedures and make folders with symbolic links to their content\n",
    "        trainfolder = dustin_dir + \"train/\"\n",
    "        train_dps = train.args[0]\n",
    "        symlinkFolderFromDPS(train_dps, trainfolder)\n",
    "        valfolder = dustin_dir + \"val/\"\n",
    "        val_dps = val.args[0]\n",
    "        symlinkFolderFromDPS(val_dps, valfolder)\n",
    "        \n",
    "        #Read the data back\n",
    "        (t_X,t_Y)  = readH5(trainfolder + \"000.h5\")\n",
    "        (v_X,v_Y)  = readH5(valfolder + \"000.h5\")\n",
    "        \n",
    "        print(\"here are the shapes of our inputs (number of events, number of particles, number of observables)\")\n",
    "        print([x.shape for x in t_X])\n",
    "        print(\"here are the shapes of our targets (number of events, number of classes/processes)\")\n",
    "        print([y.shape for y in t_Y])\n",
    "                                \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
