{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using gpu0\n",
      "using theano\n"
     ]
    }
   ],
   "source": [
    "#We can go into our root file and see what Trees are availiable\n",
    "%matplotlib inline\n",
    "import sys, os\n",
    "if __package__ is None:\n",
    "    import sys, os\n",
    "    sys.path.append(os.path.realpath(\"/data/shared/Software/\"))\n",
    "    sys.path.append(os.path.realpath(\"../../\"))\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ntpath\n",
    "import glob\n",
    "import deepconfig\n",
    "\n",
    "#from keras.utils.visualize_util import plot\n",
    "#from IPython.display import Image, display\n",
    "\n",
    "from CMS_SURF_2016.utils.preprocessing import *\n",
    "from CMS_SURF_2016.utils.callbacks import OverfitStopping, SmartCheckpoint\n",
    "from CMS_SURF_2016.utils.batch import batchAssertArchived, batchExecuteAndTestTrials\n",
    "from CMS_SURF_2016.utils.archiving import *\n",
    "from CMS_SURF_2016.utils.analysistools import findsubsets\n",
    "from CMS_SURF_2016.layers.lorentz import Lorentz, _lorentz\n",
    "from CMS_SURF_2016.layers.slice import Slice\n",
    "\n",
    "from keras.models import Sequential, Model, model_from_json\n",
    "from keras.layers import Dense, Flatten, Reshape, Activation, Dropout, Convolution2D, merge, Input, Flatten, Lambda, LSTM, Masking\n",
    "from keras.engine.topology import Layer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils.visualize_util import plot\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "\n",
    "\n",
    "dc = deepconfig.deepconfig(gpu='gpu0', backend='theano')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The observables taken from the table\n",
    "observ_types = ['E/c', 'Px', 'Py', 'Pz', 'PT_ET','Eta', 'Phi', 'Charge', 'X', 'Y', 'Z',\\\n",
    "                     'Dxy', 'Ehad', 'Eem', 'MuIso', 'EleIso', 'ChHadIso','NeuHadIso','GammaIso', \"ObjType\"]\n",
    "vecsize = len(observ_types)\n",
    "epochs = 60\n",
    "batch_size = 100\n",
    "\n",
    "label_dir_pairs = \\\n",
    "            [   (\"ttbar\", \"/data/shared/Delphes/ttbar_lepFilter_13TeV/pandas_h5/\"),\n",
    "                (\"wjet\", \"/data/shared/Delphes/wjets_lepFilter_13TeV/pandas_h5/\"),\n",
    "                (\"qcd\", \"/data/shared/Delphes/qcd_lepFilter_13TeV/pandas_h5/\")\n",
    "            ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Find all the subsets of label_dir_pairs and store them as sorted lists\n",
    "#ldpsubsets = [sorted(list(s)) for s in findsubsets(label_dir_pairs)]\n",
    "#Make sure that we do 3-way classification as well\n",
    "ldpsubsets = []\n",
    "ldpsubsets.append(label_dir_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def genModel(name,object_profiles,out_dim, depth, lstm_activation=\"relu\", lstm_dropout = 0.0, dropout=0.0):\n",
    "    inputs = []\n",
    "    mergelist = []\n",
    "    for i, profile in enumerate(object_profiles):\n",
    "        inp = a = Input(shape=(profile.max_size + 1*(profile.punctuation != None), vecsize), name=\"input_\"+str(i))\n",
    "        inputs.append(inp)\n",
    "        mergelist.append(a)\n",
    "    a = merge(mergelist,mode='concat',concat_axis=1, name=\"merge\")\n",
    "    for i in range(depth):\n",
    "        a = Masking(mask_value=0.0)(a)\n",
    "        a = LSTM(vecsize,\n",
    "                 input_shape=(None,vecsize),\n",
    "                 dropout_W=lstm_dropout,\n",
    "                 dropout_U=lstm_dropout,\n",
    "                 activation=lstm_activation,\n",
    "                 name = \"lstm_\" +str(i))(a)\n",
    "        if(dropout > 0.0):\n",
    "            a =  Dropout(dropout, name=\"dropout_\"+str(i))(a)\n",
    "    dense_out = Dense(out_dim, activation='softmax', name='main_output')(a)\n",
    "    model = Model(input=inputs, output=dense_out, name=name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def symlinkFolderFromDPS(dps, folder):\n",
    "    if(not os.path.exists(folder)):\n",
    "        os.makedirs(folder)\n",
    "    for i, dp in enumerate(dps):\n",
    "        path = folder + \"%03d\"% i + \".h5\"\n",
    "        try:\n",
    "            os.unlink(path)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        os.symlink(dp.get_path()+\"archive.h5\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "def readH5(filepath):\n",
    "    h5f = h5py.File(filepath, 'r')\n",
    "    X = []\n",
    "    X_group = h5f['X']\n",
    "    keys = list(X_group.keys())\n",
    "    keys.sort()\n",
    "    for key in keys:\n",
    "        X.append(X_group[key][:])\n",
    "\n",
    "\n",
    "    Y = []\n",
    "    Y_group = h5f['Y']\n",
    "    keys = list(Y_group.keys())\n",
    "    keys.sort()\n",
    "    for key in keys:\n",
    "        Y.append(Y_group[key][:])\n",
    "\n",
    "    h5f.close()\n",
    "    out = (X, Y)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batchAssertArchived...\n",
      "('MAXQ: ', 31.0)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_0 (InputLayer)             (None, 6, 20)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_1 (InputLayer)             (None, 6, 20)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 25, 20)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_3 (InputLayer)             (None, 1, 20)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_4 (InputLayer)             (None, 100, 20)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_5 (InputLayer)             (None, 100, 20)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_6 (InputLayer)             (None, 100, 20)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "merge (Merge)                    (None, 338, 20)       0           input_0[0][0]                    \n",
      "                                                                   input_1[0][0]                    \n",
      "                                                                   input_2[0][0]                    \n",
      "                                                                   input_3[0][0]                    \n",
      "                                                                   input_4[0][0]                    \n",
      "                                                                   input_5[0][0]                    \n",
      "                                                                   input_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "masking_2 (Masking)              (None, 338, 20)       0           merge[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "lstm_0 (LSTM)                    (None, 20)            3280        masking_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "main_output (Dense)              (None, 3)             63          lstm_0[0][0]                     \n",
      "====================================================================================================\n",
      "Total params: 3343\n",
      "____________________________________________________________________________________________________\n",
      "here are the shapes of our inputs (number of events, number of particles, number of observables)\n",
      "[(9243, 6, 20), (9243, 6, 20), (9243, 25, 20), (9243, 1, 20), (9243, 100, 20), (9243, 100, 20), (9243, 100, 20)]\n",
      "here are the shapes of our targets (number of events, number of classes/processes)\n",
      "[(9243, 3)]\n"
     ]
    }
   ],
   "source": [
    "archive_dir = \"/data/shared/Delphes/keras_archive/\"\n",
    "dustin_dir = \"/data/shared/Delphes/dustin_MPI_files/\"\n",
    "patience = 8\n",
    "earlyStopping = EarlyStopping(verbose=1, patience=patience)\n",
    "#trial_tups = []\n",
    "#Loop over all subsets\n",
    "for ldp in ldpsubsets:\n",
    "    labels = [x[0] for x in ldp]\n",
    "    #for sort_on in [\"PT_ET\", \"Phi\", \"Eta\"]:\n",
    "    for sort_on in [\"Phi\"]:\n",
    "        #Use object maxes from Find_Maxes_From Query\n",
    "        object_profiles = [ObjectProfile(\"Electron\",-1, pre_sort_columns=[\"PT_ET\"], pre_sort_ascending=False, sort_columns=[sort_on], sort_ascending=False, addColumns={\"ObjType\":1}),\n",
    "                            ObjectProfile(\"MuonTight\", -1, pre_sort_columns=[\"PT_ET\"], pre_sort_ascending=False, sort_columns=[sort_on], sort_ascending=False, addColumns={\"ObjType\":2}),\n",
    "                            ObjectProfile(\"Photon\", -1, pre_sort_columns=[\"PT_ET\"], pre_sort_ascending=False, sort_columns=[sort_on], sort_ascending=False, addColumns={\"ObjType\":3}),\n",
    "                            ObjectProfile(\"MissingET\", 1, addColumns={\"ObjType\":4}),\n",
    "                            ObjectProfile(\"EFlowPhoton\",100, pre_sort_columns=[\"PT_ET\"], pre_sort_ascending=False, sort_columns=[sort_on], sort_ascending=False, addColumns={\"ObjType\":5}), \n",
    "                            ObjectProfile(\"EFlowNeutralHadron\",100, pre_sort_columns=[\"PT_ET\"], pre_sort_ascending=False, sort_columns=[sort_on], sort_ascending=False, addColumns={\"ObjType\":6}), \n",
    "                            ObjectProfile(\"EFlowTrack\",100, pre_sort_columns=[\"PT_ET\"], pre_sort_ascending=False, sort_columns=[sort_on], sort_ascending=False, addColumns={\"ObjType\":7})]  \n",
    "        #This will replace the -1's used in the previous lines with the acutal maximum number of particles for each object type\n",
    "        resolveProfileMaxes(object_profiles, ldp)\n",
    "        \n",
    "        #This outputs generators for training and validation with 75,000 events per process and 20,000 events per process respectively in files of roughly 100MB\n",
    "        dps, l = getGensDefaultFormat(archive_dir, (75000,20000), 115000, \\\n",
    "                             object_profiles,ldp,observ_types,megabytes=100, verbose=0)\n",
    "        \n",
    "        #Don't worry about dependencies, this is for systems like CSCS where thing are run in parellel batches, but on titans it does nothing\n",
    "        #batchAssertArchived is very important though, it makes sure that each 100MB chunk of data is actually archived.\n",
    "        dependencies = batchAssertArchived(dps)\n",
    "        train, num_train = l[0]\n",
    "        val,   num_val   = l[1]\n",
    "        max_q_size = l[2]\n",
    "        print(\"MAXQ: \",max_q_size)\n",
    "        \n",
    "        #Generate the model\n",
    "        name = 'LSTM'\n",
    "        model = genModel('LSTM',object_profiles, len(labels), 1, 'tanh', 0.0, 0.0)\n",
    "        model.summary()\n",
    "        \n",
    "        #Write it to a .json file, write_json_obj is in CMS_SURF_2016.utils.archiving\n",
    "        write_json_obj(model.to_json(), dustin_dir, \"model.json\")\n",
    "        \n",
    "        #Get the archived DataProcedures and make folders with symbolic links to their content\n",
    "        trainfolder = dustin_dir + \"train/\"\n",
    "        train_dps = train.args[0]\n",
    "        symlinkFolderFromDPS(train_dps, trainfolder)\n",
    "        valfolder = dustin_dir + \"val/\"\n",
    "        val_dps = val.args[0]\n",
    "        symlinkFolderFromDPS(val_dps, valfolder)\n",
    "        \n",
    "        #Read the data back\n",
    "        (t_X,t_Y)  = readH5(trainfolder + \"000.h5\")\n",
    "        (v_X,v_Y)  = readH5(valfolder + \"000.h5\")\n",
    "        \n",
    "        print(\"here are the shapes of our inputs (number of events, number of particles, number of observables)\")\n",
    "        print([x.shape for x in t_X])\n",
    "        print(\"here are the shapes of our targets (number of events, number of classes/processes)\")\n",
    "        print([y.shape for y in t_Y])\n",
    "                                \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
