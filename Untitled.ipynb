{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THIS NOTEBOOK is just a bunch of random junk and test code. Pay it no heed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Compare LSTM implementations on the IMDB sentiment classification task.\n",
    "\n",
    "consume_less='cpu' preprocesses input to the LSTM which typically results in\n",
    "faster computations at the expense of increased peak memory usage as the\n",
    "preprocessed input must be kept in memory.\n",
    "\n",
    "consume_less='mem' does away with the preprocessing, meaning that it might take\n",
    "a little longer, but should require less peak memory.\n",
    "\n",
    "consume_less='gpu' concatenates the input, output and forget gate's weights\n",
    "into one, large matrix, resulting in faster computation time as the GPU can\n",
    "utilize more cores, at the expense of reduced regularization because the same\n",
    "dropout is shared across the gates.\n",
    "\n",
    "Note that the relative performance of the different `consume_less` modes\n",
    "can vary depending on your device, your model and the size of your data.\n",
    "'''\n",
    "%matplotlib inline\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, LSTM\n",
    "from keras.datasets import imdb\n",
    "\n",
    "\n",
    "max_features = 20000\n",
    "max_length = 80\n",
    "embedding_dim = 256\n",
    "batch_size = 128\n",
    "epochs = 1\n",
    "modes = ['gpu']\n",
    "\n",
    "print('Loading data...')\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=max_features)\n",
    "X_train = sequence.pad_sequences(X_train, max_length)\n",
    "X_test = sequence.pad_sequences(X_test, max_length)\n",
    "\n",
    "# Compile and train different models while meauring performance.\n",
    "results = []\n",
    "for mode in modes:\n",
    "    print('Testing mode: consume_less=\"{}\"'.format(mode))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, embedding_dim, input_length=max_length, dropout=0.2))\n",
    "    model.add(LSTM(embedding_dim, dropout_W=0.2, dropout_U=0.2, consume_less=mode))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        nb_epoch=epochs,\n",
    "                        validation_data=(X_test, y_test))\n",
    "    average_time_per_epoch = (time.time() - start_time) / epochs\n",
    "\n",
    "    results.append((history, average_time_per_epoch))\n",
    "\n",
    "# Compare models' accuracy, loss and elapsed time per epoch.\n",
    "plt.style.use('ggplot')\n",
    "ax1 = plt.subplot2grid((2, 2), (0, 0))\n",
    "ax1.set_title('Accuracy')\n",
    "ax1.set_ylabel('Validation Accuracy')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax2 = plt.subplot2grid((2, 2), (1, 0))\n",
    "ax2.set_title('Loss')\n",
    "ax2.set_ylabel('Validation Loss')\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax3 = plt.subplot2grid((2, 2), (0, 1), rowspan=2)\n",
    "ax3.set_title('Time')\n",
    "ax3.set_ylabel('Seconds')\n",
    "for mode, result in zip(modes, results):\n",
    "    ax1.plot(result[0].epoch, result[0].history['val_acc'], label=mode)\n",
    "    ax2.plot(result[0].epoch, result[0].history['val_loss'], label=mode)\n",
    "ax1.legend()\n",
    "ax2.legend()\n",
    "ax3.bar(np.arange(len(results)), [x[1] for x in results],\n",
    "        tick_label=modes, align='center')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('md5', 'sha1', 'sha224', 'sha256', 'sha384', 'sha512')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hashlib\n",
    "hashlib.algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = hashlib.md5()\n",
    "m.update(\"Nobody inspects\")\n",
    "m.update(\" the spammish repetition\")\n",
    "print(m.hexdigest())\n",
    "\n",
    "\n",
    "m = hashlib.md5()\n",
    "m.update(\" the spammish repetition\")\n",
    "m.update(\"Nobody inspects\")\n",
    "print(m.hexdigest())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "dfs = [pd.DataFrame(np.random.rand(10, 3), columns=list('abc')) for i in range(3)]\n",
    "DF = pd.DataFrame(np.array(dfs), columns=list('ABC'))\n",
    "\n",
    "print(DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q = df[(df.a < df.b) & (df.b < df.c)]\n",
    "print(type(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.query('(a < b) & (b < c)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if(storeType == \"hdf5\"):\n",
    "    #If we are reading all the samples use get since it might be faster\n",
    "    #TODO: check if it is actually faster\n",
    "    if(samples_to_read == file_total_entries):\n",
    "        frame = store.get('/'+key)\n",
    "    else:\n",
    "        frame = store.select('/'+key, start=select_start, stop=select_stop)\n",
    "elif(storeType == \"msgpack\"):\n",
    "    frame = frames[key]\n",
    "    frame = frame[select_start:select_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from CMS_SURF_2016.utils.preprocessing import ObjectProfile, \\\n",
    "        preprocessFromPandas_label_dir_pairs, label_dir_pairs_args_decoder, \\\n",
    "        genFrom_label_dir_pairs,maxMutualLength, start_num_fromSplits, resolveProfileMaxes,procsFrom_label_dir_pairs,genFromPPs\n",
    "from CMS_SURF_2016.utils.archiving import *\n",
    "#The observables taken from the table\n",
    "#observ_types = ['E/c', 'Px', 'Py', 'Pz', 'Charge', \"PT_ET\", \"Eta\", \"Phi\", \"Dxy_Ehad_Eem\"]\n",
    "observ_types =  ['Entry','E/c', 'Px', 'Py', 'Pz', 'PT_ET','Eta', 'Phi', 'Charge', 'X', 'Y', 'Z',\\\n",
    "                     'Dxy', 'Ehad', 'Eem', 'MuIso', 'EleIso', 'ChHadIso','NeuHadIso','GammaIso']\n",
    "vecsize = len(observ_types)\n",
    "epochs = 100\n",
    "batch_size = 100\n",
    "sample_start = 0\n",
    "num_samples = 10000\n",
    "\n",
    "print(\"YAY!\")\n",
    "sys.stdout.flush()\n",
    "\n",
    "object_profiles = [ObjectProfile(\"Electron\",5),\n",
    "                    ObjectProfile(\"MuonTight\", 5),\n",
    "                    ObjectProfile(\"Photon\", 25),\n",
    "                    ObjectProfile(\"MissingET\", 1),\n",
    "                    ObjectProfile(\"EFlowPhoton\",1000, sort_columns=[\"PT_ET\"], sort_ascending=False),  #1300\n",
    "                    ObjectProfile(\"EFlowNeutralHadron\",1000, sort_columns=[\"PT_ET\"], sort_ascending=False),  #1000\n",
    "                    ObjectProfile(\"EFlowTrack\",1000, sort_columns=[\"PT_ET\"], sort_ascending=False)]  #1050\n",
    "\n",
    "\n",
    "\n",
    "label_dir_pairs = \\\n",
    "            [   (\"ttbar\", \"/data/shared/Delphes/ttbar_lepFilter_13TeV/pandas_msg/\"),\n",
    "                (\"wjet\", \"/data/shared/Delphes/wjets_lepFilter_13TeV/pandas_msg/\"),\n",
    "                (\"qcd\", \"/data/shared/Delphes/qcd_lepFilter_13TeV/pandas_msg/\")\n",
    "            ]\n",
    "print(\"RESOLVING MAXES!\")\n",
    "sys.stdout.flush()\n",
    "resolveProfileMaxes(object_profiles, label_dir_pairs)\n",
    "\n",
    "archive_dir = \"MyTestArchiveDir/\"\n",
    "trials = []\n",
    "ldps_pairs = []\n",
    "\n",
    "import socket\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(\"FINDING STRIDE!\")\n",
    "#sys.stdout.flush()\n",
    "#print(strideFromTargetSize(object_profiles, label_dir_pairs, observ_types, megabytes=500))\n",
    "#for i, j in [(2,0)]:\n",
    "    #ldps = [label_dir_pairs[i]] + [label_dir_pairs[j]]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "       \n",
    "    \n",
    "        \n",
    "dps, l = getGensDefaultFormat(archive_dir, (100000,25000,25000), 150000, \\\n",
    "                         object_profiles,label_dir_pairs,observ_types,megabytes=500)\n",
    "train, num_train = l[0]\n",
    "val,   num_val   = l[1]\n",
    "test,  num_test  = l[2]\n",
    "       \n",
    "batchAssertArchived(dps)\n",
    "print(dps)\n",
    "print(train)\n",
    "print(num_train)\n",
    "print(val)\n",
    "print(num_val)\n",
    "print(test)\n",
    "print(num_test)\n",
    "for X,Y in train.getData():\n",
    "    batch_size = X[0].shape[0]\n",
    "    num_read += batch_size\n",
    "    if(num_read >= num_train):\n",
    "        break\n",
    "raise ValueError()\n",
    "    #print(SNs)\n",
    "    #gen_lambda =  lambda s : (DataProcedure(archive_dir,\n",
    "    #                                       False,\n",
    "    #                                       genFrom_label_dir_pairs,\n",
    "    #                                       start=s[0],\n",
    "    #                                       samples_per_label=s[1],\n",
    "    #                                       stride=stride,\n",
    "    #                                        batch_size=batch_size,\n",
    "    #                                       archive_dir=archive_dir,\n",
    "    #                                       label_dir_pairs=label_dir_pairs,\n",
    "    #                                       object_profiles=object_profiles,\n",
    "    #                                       observ_types=observ_types),\n",
    "    #                         len(label_dir_pairs)*s[1])\n",
    "    #l = [gen_lambda(s) for s in SNs]\n",
    "    \n",
    "stride = strideFromTargetSize(object_profiles, label_dir_pairs, observ_types, megabytes=500)\n",
    "SNs = start_num_fromSplits((.7,.15,.15), 200000)\n",
    "print(SNs)\n",
    "gen_lambda =  lambda s : (DataProcedure(archive_dir,\n",
    "                                       False,\n",
    "                                       genFrom_label_dir_pairs,\n",
    "                                       start=s[0],\n",
    "                                       samples_per_label=s[1],\n",
    "                                       stride=stride,\n",
    "                                        batch_size=100,\n",
    "                                       archive_dir=archive_dir,\n",
    "                                       label_dir_pairs=label_dir_pairs,\n",
    "                                       object_profiles=object_profiles,\n",
    "                                       observ_types=observ_types),\n",
    "                         len(label_dir_pairs)*s[1])\n",
    "l = [gen_lambda(s) for s in SNs]\n",
    "train, num_train = l[0]\n",
    "val,   num_val   = l[1]\n",
    "test,  num_test  = l[2]\n",
    "\n",
    "num_read = 0\n",
    "for X,Y in train.getData():\n",
    "    batch_size = X[0].shape[0]\n",
    "    num_read += batch_size\n",
    "    if(num_read >= num_train):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "dep_clause = \"\"\n",
    "#string = \"Submitted batch job 365288 I AM A PRETTY RAINBOW 38341034  Submitted batch job 42388 Submitted batch job 665245458\"\n",
    "matches = re.findall(\"Submitted batch job [0-9]+\", string) \n",
    "\n",
    "dependencies = [re.findall(\"[0-9]+\", m)[0] for m in matches]\n",
    "if(len(dependencies) > 0):\n",
    "    dep_clause = \"--dependency=afterok:\" + \":\".join(dependencies)\n",
    "print(dep_clause)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YAY!\n",
      "0.03168\n",
      "[]\n",
      "Generating DataProcedure in range(0,100):\n",
      "   From 2 labels in range(0,100) for 2x100 = 200 Samples\n",
      "Generating DataProcedure in range(100,125):\n",
      "   From 2 labels in range(100,125) for 2x25 = 50 Samples\n",
      "Generating DataProcedure in range(125,150):\n",
      "   From 2 labels in range(125,150) for 2x25 = 50 Samples\n",
      "Failed to load history at /data/shared/Delphes/testArchive/blobs/8c1f7/28e3d7a17233d3ba7bd181d38887602459b/history.json\n",
      "Failed to load weights at /data/shared/Delphes/testArchive/blobs/8c1f7/28e3d7a17233d3ba7bd181d38887602459b/weights.h5\n",
      "Epoch 1/100\n",
      "DataProcedure results '0b3b7a30bcf059a1ae62dd38fe68b4c2b8565403' read from archive\n",
      "100/200 [==============>...............] - ETA: 0s - loss: 7.2453 - acc: 0.5200DataProcedure results 'fb2f74985aa1a48f85f0555422d8407f3f778ecb' read from archive\n",
      "Epoch 00001: acc improved from -inf to 0.52750, saving model to /data/shared/Delphes/testArchive/blobs/8c1f7/28e3d7a17233d3ba7bd181d38887602459b/weights.h5\n",
      "200/200 [==============================] - 0s - loss: 7.2158 - acc: 0.5275 - val_loss: 8.0151 - val_acc: 0.5000\n",
      "Epoch 2/100\n",
      "DataProcedure results '0b3b7a30bcf059a1ae62dd38fe68b4c2b8565403' read from archive\n",
      "100/200 [==============>...............] - ETA: 0s - loss: 7.6817 - acc: 0.5200DataProcedure results 'fb2f74985aa1a48f85f0555422d8407f3f778ecb' read from archive\n",
      "Epoch 00002: acc did not improve\n",
      "200/200 [==============================] - 0s - loss: 7.6084 - acc: 0.5250 - val_loss: 8.0151 - val_acc: 0.5000\n",
      "Epoch 3/100\n",
      "DataProcedure results '0b3b7a30bcf059a1ae62dd38fe68b4c2b8565403' read from archive\n",
      "100/200 [==============>...............] - ETA: 0s - loss: 7.6191 - acc: 0.5250DataProcedure results 'fb2f74985aa1a48f85f0555422d8407f3f778ecb' read from archive\n",
      "Epoch 00003: acc did not improve\n",
      "200/200 [==============================] - 0s - loss: 7.5214 - acc: 0.5250 - val_loss: 7.5392 - val_acc: 0.5300\n",
      "Epoch 4/100\n",
      "DataProcedure results '0b3b7a30bcf059a1ae62dd38fe68b4c2b8565403' read from archive\n",
      "100/200 [==============>...............] - ETA: 0s - loss: 7.0535 - acc: 0.5300DataProcedure results 'fb2f74985aa1a48f85f0555422d8407f3f778ecb' read from archive\n",
      "Epoch 00004: acc improved from 0.52750 to 0.53000, saving model to /data/shared/Delphes/testArchive/blobs/8c1f7/28e3d7a17233d3ba7bd181d38887602459b/weights.h5\n",
      "200/200 [==============================] - 0s - loss: 7.0541 - acc: 0.5300 - val_loss: 8.2976 - val_acc: 0.4800\n",
      "Epoch 5/100\n",
      "DataProcedure results '0b3b7a30bcf059a1ae62dd38fe68b4c2b8565403' read from archive\n",
      "100/200 [==============>...............] - ETA: 0s - loss: 7.1803 - acc: 0.5350DataProcedure results 'fb2f74985aa1a48f85f0555422d8407f3f778ecb' read from archive\n",
      "Epoch 00005: acc improved from 0.53000 to 0.54500, saving model to /data/shared/Delphes/testArchive/blobs/8c1f7/28e3d7a17233d3ba7bd181d38887602459b/weights.h5\n",
      "200/200 [==============================] - 0s - loss: 6.9601 - acc: 0.5450 - val_loss: 7.8310 - val_acc: 0.4900\n",
      "Epoch 6/100\n",
      "DataProcedure results '0b3b7a30bcf059a1ae62dd38fe68b4c2b8565403' read from archive\n",
      "100/200 [==============>...............] - ETA: 0s - loss: 6.1355 - acc: 0.5750DataProcedure results 'fb2f74985aa1a48f85f0555422d8407f3f778ecb' read from archive\n",
      "Epoch 00006: acc improved from 0.54500 to 0.58000, saving model to /data/shared/Delphes/testArchive/blobs/8c1f7/28e3d7a17233d3ba7bd181d38887602459b/weights.h5\n",
      "200/200 [==============================] - 0s - loss: 6.2133 - acc: 0.5800 - val_loss: 7.0574 - val_acc: 0.5100\n",
      "Epoch 7/100\n",
      "DataProcedure results '0b3b7a30bcf059a1ae62dd38fe68b4c2b8565403' read from archive\n",
      "100/200 [==============>...............] - ETA: 0s - loss: 5.8022 - acc: 0.6250DataProcedure results 'fb2f74985aa1a48f85f0555422d8407f3f778ecb' read from archive\n",
      "Epoch 00007: acc improved from 0.58000 to 0.61000, saving model to /data/shared/Delphes/testArchive/blobs/8c1f7/28e3d7a17233d3ba7bd181d38887602459b/weights.h5\n",
      "200/200 [==============================] - 0s - loss: 5.9244 - acc: 0.6100 - val_loss: 7.7386 - val_acc: 0.4800\n",
      "Epoch 8/100\n",
      "DataProcedure results '0b3b7a30bcf059a1ae62dd38fe68b4c2b8565403' read from archive\n",
      "100/200 [==============>...............] - ETA: 0s - loss: 5.6336 - acc: 0.6050DataProcedure results 'fb2f74985aa1a48f85f0555422d8407f3f778ecb' read from archive\n",
      "Epoch 00008: acc did not improve\n",
      "200/200 [==============================] - 0s - loss: 5.9918 - acc: 0.5975 - val_loss: 7.1167 - val_acc: 0.5400\n",
      "Epoch 9/100\n",
      "DataProcedure results '0b3b7a30bcf059a1ae62dd38fe68b4c2b8565403' read from archive\n",
      "100/200 [==============>...............] - ETA: 0s - loss: 5.2672 - acc: 0.6500DataProcedure results 'fb2f74985aa1a48f85f0555422d8407f3f778ecb' read from archive\n",
      "Epoch 00009: acc improved from 0.61000 to 0.64000, saving model to /data/shared/Delphes/testArchive/blobs/8c1f7/28e3d7a17233d3ba7bd181d38887602459b/weights.h5\n",
      "200/200 [==============================] - 0s - loss: 5.5011 - acc: 0.6400 - val_loss: 8.0373 - val_acc: 0.4800\n",
      "Epoch 10/100\n",
      "DataProcedure results '0b3b7a30bcf059a1ae62dd38fe68b4c2b8565403' read from archive\n",
      "100/200 [==============>...............] - ETA: 0s - loss: 5.5399 - acc: 0.6200DataProcedure results 'fb2f74985aa1a48f85f0555422d8407f3f778ecb' read from archive\n",
      "Epoch 00010: acc did not improve\n",
      "200/200 [==============================] - 0s - loss: 5.8331 - acc: 0.5975 - val_loss: 8.0365 - val_acc: 0.4900\n",
      "Epoch 11/100\n",
      "DataProcedure results '0b3b7a30bcf059a1ae62dd38fe68b4c2b8565403' read from archive\n",
      "100/200 [==============>...............] - ETA: 0s - loss: 5.5929 - acc: 0.6150DataProcedure results 'fb2f74985aa1a48f85f0555422d8407f3f778ecb' read from archive\n",
      "Epoch 00011: acc did not improve\n",
      "200/200 [==============================] - 0s - loss: 5.7756 - acc: 0.6150 - val_loss: 8.0364 - val_acc: 0.4600\n",
      "Epoch 12/100\n",
      "DataProcedure results '0b3b7a30bcf059a1ae62dd38fe68b4c2b8565403' read from archive\n",
      "100/200 [==============>...............] - ETA: 0s - loss: 5.2848 - acc: 0.6600DataProcedure results 'fb2f74985aa1a48f85f0555422d8407f3f778ecb' read from archive\n",
      "Epoch 00012: acc improved from 0.64000 to 0.65000, saving model to /data/shared/Delphes/testArchive/blobs/8c1f7/28e3d7a17233d3ba7bd181d38887602459b/weights.h5\n",
      "200/200 [==============================] - 0s - loss: 5.4446 - acc: 0.6500 - val_loss: 7.7702 - val_acc: 0.4800\n",
      "Epoch 13/100\n",
      "DataProcedure results '0b3b7a30bcf059a1ae62dd38fe68b4c2b8565403' read from archive\n",
      "100/200 [==============>...............] - ETA: 0s - loss: 4.8977 - acc: 0.6900DataProcedure results 'fb2f74985aa1a48f85f0555422d8407f3f778ecb' read from archive\n",
      "Epoch 00013: acc improved from 0.65000 to 0.66250, saving model to /data/shared/Delphes/testArchive/blobs/8c1f7/28e3d7a17233d3ba7bd181d38887602459b/weights.h5\n",
      "200/200 [==============================] - 0s - loss: 5.2478 - acc: 0.6625 - val_loss: 8.0422 - val_acc: 0.4900\n",
      "Epoch 14/100\n",
      "DataProcedure results '0b3b7a30bcf059a1ae62dd38fe68b4c2b8565403' read from archive\n",
      "100/200 [==============>...............] - ETA: 0s - loss: 5.1962 - acc: 0.6700DataProcedure results 'fb2f74985aa1a48f85f0555422d8407f3f778ecb' read from archive\n",
      "Epoch 00014: acc did not improve\n",
      "200/200 [==============================] - 0s - loss: 5.2759 - acc: 0.6625 - val_loss: 7.6886 - val_acc: 0.4900\n",
      "Epoch 15/100\n",
      "DataProcedure results '0b3b7a30bcf059a1ae62dd38fe68b4c2b8565403' read from archive\n",
      "100/200 [==============>...............] - ETA: 0s - loss: 5.2383 - acc: 0.6650DataProcedure results 'fb2f74985aa1a48f85f0555422d8407f3f778ecb' read from archive\n",
      "Epoch 00015: acc did not improve\n",
      "200/200 [==============================] - 0s - loss: 5.4384 - acc: 0.6500 - val_loss: 8.0408 - val_acc: 0.4800\n",
      "Epoch 16/100\n",
      "DataProcedure results '0b3b7a30bcf059a1ae62dd38fe68b4c2b8565403' read from archive\n",
      "100/200 [==============>...............] - ETA: 0s - loss: 5.1662 - acc: 0.6700DataProcedure results 'fb2f74985aa1a48f85f0555422d8407f3f778ecb' read from archive\n",
      "Epoch 00016: acc did not improve\n",
      "200/200 [==============================] - 0s - loss: 5.3220 - acc: 0.6625 - val_loss: 7.6023 - val_acc: 0.4700\n",
      "Epoch 17/100\n",
      "DataProcedure results '0b3b7a30bcf059a1ae62dd38fe68b4c2b8565403' read from archive\n",
      "100/200 [==============>...............] - ETA: 0s - loss: 4.8188 - acc: 0.6750DataProcedure results 'fb2f74985aa1a48f85f0555422d8407f3f778ecb' read from archive\n",
      "Epoch 00016: early stopping\n",
      "Epoch 00017: acc improved from 0.66250 to 0.66750, saving model to /data/shared/Delphes/testArchive/blobs/8c1f7/28e3d7a17233d3ba7bd181d38887602459b/weights.h5\n",
      "200/200 [==============================] - 0s - loss: 5.0931 - acc: 0.6675 - val_loss: 8.2241 - val_acc: 0.4600\n",
      "('T@:', <class 'CMS_SURF_2016.utils.archiving.DataProcedure'>)\n",
      "USING max_q_size: 1\n",
      "DataProcedure results '766588b56e461599f7333e7d45fecad15b860ccb' read from archive\n",
      "Test Complete [7.8014559745788574, 0.4699999988079071]\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import sys, os\n",
    "if(\"daint\" in socket.gethostname()):\n",
    "    DELPHES_DIR = \"/scratch/daint/dweiteka/Delphes/\"\n",
    "    SOFTWAR_DIR = \"/scratch/daint/dweiteka/\"\n",
    "else:\n",
    "    DELPHES_DIR = \"/data/shared/Delphes/\"\n",
    "    SOFTWAR_DIR = \"/data/shared/Software/\"\n",
    "\n",
    "if(not SOFTWAR_DIR in sys.path):\n",
    "    sys.path.append(SOFTWAR_DIR)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ntpath\n",
    "import glob\n",
    "#import deepconfig\n",
    "\n",
    "from CMS_SURF_2016.utils.preprocessing import *\n",
    "from CMS_SURF_2016.utils.callbacks import OverfitStopping, SmartCheckpoint\n",
    "from CMS_SURF_2016.utils.archiving import *\n",
    "from CMS_SURF_2016.layers.lorentz import Lorentz, _lorentz\n",
    "from CMS_SURF_2016.layers.slice import Slice\n",
    "\n",
    "from keras.models import Sequential, Model, model_from_json\n",
    "from keras.layers import Dense, Flatten, Reshape, Activation, Dropout, Convolution2D, merge, Input, Flatten, Lambda, LSTM, Masking\n",
    "from keras.engine.topology import Layer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils.visualize_util import plot\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "\n",
    "\n",
    "#dc = deepconfig.deepconfig(gpu='gpu0', backend='theano')\n",
    "\n",
    "\n",
    "#The observables taken from the table\n",
    "observ_types =  ['Entry','E/c', 'Px', 'Py', 'Pz', 'PT_ET','Eta', 'Phi', 'Charge', 'X', 'Y', 'Z',\\\n",
    "                     'Dxy', 'Ehad', 'Eem', 'MuIso', 'EleIso', 'ChHadIso','NeuHadIso','GammaIso']\n",
    "vecsize = len(observ_types)\n",
    "epochs = 100\n",
    "batch_size = 100\n",
    "sample_start = 0\n",
    "num_samples = 10000\n",
    "\n",
    "print(\"YAY!\")\n",
    "sys.stdout.flush()\n",
    "\n",
    "object_profiles = [ObjectProfile(\"Electron\",5),\n",
    "                    ObjectProfile(\"MuonTight\", 5),\n",
    "                    ObjectProfile(\"Photon\", 25),\n",
    "                    ObjectProfile(\"MissingET\", 1),\n",
    "                    ObjectProfile(\"EFlowPhoton\",10, sort_columns=[\"PT_ET\"], sort_ascending=False),  #1300\n",
    "                    ObjectProfile(\"EFlowNeutralHadron\",10, sort_columns=[\"PT_ET\"], sort_ascending=False),  #1000\n",
    "                    ObjectProfile(\"EFlowTrack\",10, sort_columns=[\"PT_ET\"], sort_ascending=False)]  #1050\n",
    "\n",
    "\n",
    "\n",
    "label_dir_pairs = \\\n",
    "            [   (\"ttbar\", DELPHES_DIR+\"ttbar_lepFilter_13TeV/pandas_msg/\"),\n",
    "                (\"wjet\", DELPHES_DIR+\"wjets_lepFilter_13TeV/pandas_msg/\"),\n",
    "                (\"qcd\", DELPHES_DIR+\"qcd_lepFilter_13TeV/pandas_msg/\")\n",
    "            ]\n",
    "\n",
    "def genModel(depth, width):\n",
    "    inputs = [None] * len(object_profiles)\n",
    "    fist_layer = [None] * len(object_profiles)\n",
    "    #Build Inputs\n",
    "    for i,profile in enumerate(object_profiles):\n",
    "        key = profile.name\n",
    "        max_size = profile.max_size\n",
    "        inp = Input(shape=(max_size, vecsize), name=key)\n",
    "        inputs[i] = inp\n",
    "        fist_layer[i] = Flatten()(inp)\n",
    "    #Merge inputs\n",
    "    merged = merge(fist_layer, mode='concat', name=\"merge\")\n",
    "    a = merged\n",
    "    v = [width] * depth\n",
    "    for j in range(len(v)):\n",
    "        a = Dense(v[j], activation='relu', name=\"dense_2_\"+str(j))(a)\n",
    "    second_dense = a \n",
    "    \n",
    "    #Create dense sigmoid layer for classification\n",
    "    dense_out = Dense(2, activation='sigmoid', name='main_output')(second_dense)\n",
    "    dense = Model(input=inputs, output=dense_out, name=\"dense\")\n",
    "    return dense\n",
    "\n",
    "archive_dir = DELPHES_DIR + \"testArchive/\"\n",
    "earlyStopping = EarlyStopping(verbose=1, patience=10)\n",
    "overfitStopping = OverfitStopping(verbose=1, patience=20)\n",
    "trial_tups = []\n",
    "ldps_pairs = []\n",
    "for i, j in [(2,0)]:\n",
    "    ldps = [label_dir_pairs[i]] + [label_dir_pairs[j]]\n",
    "    dps, l = getGensDefaultFormat(archive_dir, (100,25,25), 150, \\\n",
    "                         object_profiles,ldps,observ_types,megabytes=500)\n",
    "    \n",
    "    dependencies = batchAssertArchived(dps)\n",
    "    \n",
    "    train, num_train = l[0]\n",
    "    val,   num_val   = l[1]\n",
    "    test,  num_test  = l[2]\n",
    "\n",
    "    labels = [x[0] for x in ldps]\n",
    "    for name in ['dense']:\n",
    "        for depth in [1]:\n",
    "            for width in [100]:\n",
    "                for activation in ['tanh']:\n",
    "                    for lstm_dropout in [0.0]:\n",
    "                        for dropout in [0.0]:\n",
    "                            activation_name = activation if isinstance(activation, str) \\\n",
    "                                                else activation.__name__\n",
    "\n",
    "                            model = genModel(depth,width)\n",
    "\n",
    "                            trial = KerasTrial(archive_dir, name=name, model=model)\n",
    "\n",
    "                            trial.setTrain(train_procedure=train,\n",
    "                                           samples_per_epoch=num_train\n",
    "                                          )\n",
    "                            trial.setValidation(val_procedure=val,\n",
    "                                               nb_val_samples=num_val\n",
    "                                               )\n",
    "                            trial.setCompilation(loss='binary_crossentropy',\n",
    "                                      optimizer='rmsprop',\n",
    "                                      metrics=['accuracy']\n",
    "                                          )\n",
    "                            trial.setFit_Generator( \n",
    "                                            nb_epoch=epochs,\n",
    "                                            callbacks=[earlyStopping, overfitStopping],\n",
    "                                            max_q_size = min(int(num_train/batch_size), 1))\n",
    "\n",
    "\n",
    "                            trial.write()\n",
    "                            #print(trial.to_json())\n",
    "\n",
    "\n",
    "                            trial.to_record({\"lables\": labels,\n",
    "                                             \"depth\": depth,\n",
    "                                             #\"width\": width,\n",
    "                                             \"activation\": activation_name,\n",
    "                                             \"dropout\":dropout,\n",
    "                                             \"lstm_dropout\":lstm_dropout\n",
    "                                            })\n",
    "                                       \n",
    "                            trial_tups.append((trial, test, num_test, dependencies))\n",
    "                \n",
    "batchExecuteAndTestTrials(trial_tups)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"model\": \"{\"class_name\": \"Model\", \"keras_version\": \"1.0.6\", \"config\": {\"layers\": [{\"class_name\": \"InputLayer\", \"config\": {\"batch_input_shape\": [null, 10, 20], \"name\": \"EFlowNeutralHadron\", \"input_dtype\": \"float32\"}, \"inbound_nodes\": [], \"name\": \"EFlowNeutralHadron\"}, {\"class_name\": \"InputLayer\", \"config\": {\"batch_input_shape\": [null, 10, 20], \"name\": \"EFlowPhoton\", \"input_dtype\": \"float32\"}, \"inbound_nodes\": [], \"name\": \"EFlowPhoton\"}, {\"class_name\": \"InputLayer\", \"config\": {\"batch_input_shape\": [null, 10, 20], \"name\": \"EFlowTrack\", \"input_dtype\": \"float32\"}, \"inbound_nodes\": [], \"name\": \"EFlowTrack\"}, {\"class_name\": \"InputLayer\", \"config\": {\"batch_input_shape\": [null, 5, 20], \"name\": \"Electron\", \"input_dtype\": \"float32\"}, \"inbound_nodes\": [], \"name\": \"Electron\"}, {\"class_name\": \"InputLayer\", \"config\": {\"batch_input_shape\": [null, 1, 20], \"name\": \"MissingET\", \"input_dtype\": \"float32\"}, \"inbound_nodes\": [], \"name\": \"MissingET\"}, {\"class_name\": \"InputLayer\", \"config\": {\"batch_input_shape\": [null, 5, 20], \"name\": \"MuonTight\", \"input_dtype\": \"float32\"}, \"inbound_nodes\": [], \"name\": \"MuonTight\"}, {\"class_name\": \"InputLayer\", \"config\": {\"batch_input_shape\": [null, 25, 20], \"name\": \"Photon\", \"input_dtype\": \"float32\"}, \"inbound_nodes\": [], \"name\": \"Photon\"}, {\"class_name\": \"Flatten\", \"config\": {\"trainable\": true, \"name\": \"flatten_106\"}, \"inbound_nodes\": [[[\"Electron\", 0, 0]]], \"name\": \"flatten_106\"}, {\"class_name\": \"Flatten\", \"config\": {\"trainable\": true, \"name\": \"flatten_107\"}, \"inbound_nodes\": [[[\"MuonTight\", 0, 0]]], \"name\": \"flatten_107\"}, {\"class_name\": \"Flatten\", \"config\": {\"trainable\": true, \"name\": \"flatten_108\"}, \"inbound_nodes\": [[[\"Photon\", 0, 0]]], \"name\": \"flatten_108\"}, {\"class_name\": \"Flatten\", \"config\": {\"trainable\": true, \"name\": \"flatten_109\"}, \"inbound_nodes\": [[[\"MissingET\", 0, 0]]], \"name\": \"flatten_109\"}, {\"class_name\": \"Flatten\", \"config\": {\"trainable\": true, \"name\": \"flatten_110\"}, \"inbound_nodes\": [[[\"EFlowPhoton\", 0, 0]]], \"name\": \"flatten_110\"}, {\"class_name\": \"Flatten\", \"config\": {\"trainable\": true, \"name\": \"flatten_111\"}, \"inbound_nodes\": [[[\"EFlowNeutralHadron\", 0, 0]]], \"name\": \"flatten_111\"}, {\"class_name\": \"Flatten\", \"config\": {\"trainable\": true, \"name\": \"flatten_112\"}, \"inbound_nodes\": [[[\"EFlowTrack\", 0, 0]]], \"name\": \"flatten_112\"}, {\"class_name\": \"Merge\", \"config\": {\"name\": \"merge\", \"concat_axis\": -1, \"mode_type\": \"raw\", \"dot_axes\": [-1, -1], \"mode\": \"concat\", \"output_shape\": null, \"output_shape_type\": \"raw\"}, \"inbound_nodes\": [[[\"flatten_106\", 0, 0], [\"flatten_107\", 0, 0], [\"flatten_108\", 0, 0], [\"flatten_109\", 0, 0], [\"flatten_110\", 0, 0], [\"flatten_111\", 0, 0], [\"flatten_112\", 0, 0]]], \"name\": \"merge\"}, {\"class_name\": \"Dense\", \"config\": {\"W_constraint\": null, \"b_constraint\": null, \"name\": \"dense_2_0\", \"activity_regularizer\": null, \"trainable\": true, \"init\": \"glorot_uniform\", \"bias\": true, \"input_dim\": null, \"b_regularizer\": null, \"W_regularizer\": null, \"activation\": \"relu\", \"output_dim\": 100}, \"inbound_nodes\": [[[\"merge\", 0, 0]]], \"name\": \"dense_2_0\"}, {\"class_name\": \"Dense\", \"config\": {\"W_constraint\": null, \"b_constraint\": null, \"name\": \"main_output\", \"activity_regularizer\": null, \"trainable\": true, \"init\": \"glorot_uniform\", \"bias\": true, \"input_dim\": null, \"b_regularizer\": null, \"W_regularizer\": null, \"activation\": \"sigmoid\", \"output_dim\": 2}, \"inbound_nodes\": [[[\"dense_2_0\", 0, 0]]], \"name\": \"main_output\"}], \"input_layers\": [[\"Electron\", 0, 0], [\"MuonTight\", 0, 0], [\"Photon\", 0, 0], [\"MissingET\", 0, 0], [\"EFlowPhoton\", 0, 0], [\"EFlowNeutralHadron\", 0, 0], [\"EFlowTrack\", 0, 0]], \"output_layers\": [[\"main_output\", 0, 0]], \"name\": \"dense\"}}\"\n",
      "\n",
      "\n",
      "\n",
      "['\"EFlowNeutralHadron\"', '\"EFlowNeutralHadron\"', '\"EFlowPhoton\"', '\"EFlowPhoton\"', '\"EFlowTrack\"', '\"EFlowTrack\"', '\"Electron\"', '\"Electron\"', '\"MissingET\"', '\"MissingET\"', '\"MuonTight\"', '\"MuonTight\"', '\"Photon\"', '\"Photon\"', '\"flatten_106\"', '\"flatten_106\"', '\"flatten_107\"', '\"flatten_107\"', '\"flatten_108\"', '\"flatten_108\"', '\"flatten_109\"', '\"flatten_109\"', '\"flatten_110\"', '\"flatten_110\"', '\"flatten_111\"', '\"flatten_111\"', '\"flatten_112\"', '\"flatten_112\"', '\"merge\"', '\"merge\"', '\"dense_2_0\"', '\"dense_2_0\"', '\"main_output\"', '\"main_output\"', '\"dense\"']\n",
      "\n",
      "\n",
      "\n",
      "\"model\": \"{\"class_name\": \"Model\", , \"config\": {\"layers\": [{\"class_name\": \"InputLayer\", \"config\": {\"batch_input_shape\": [null, 10, 20], \"name\": @, \"input_dtype\": \"float32\"}, \"inbound_nodes\": [], \"name\": @}, {\"class_name\": \"InputLayer\", \"config\": {\"batch_input_shape\": [null, 10, 20], \"name\": @, \"input_dtype\": \"float32\"}, \"inbound_nodes\": [], \"name\": @}, {\"class_name\": \"InputLayer\", \"config\": {\"batch_input_shape\": [null, 10, 20], \"name\": @, \"input_dtype\": \"float32\"}, \"inbound_nodes\": [], \"name\": @}, {\"class_name\": \"InputLayer\", \"config\": {\"batch_input_shape\": [null, 5, 20], \"name\": @, \"input_dtype\": \"float32\"}, \"inbound_nodes\": [], \"name\": @}, {\"class_name\": \"InputLayer\", \"config\": {\"batch_input_shape\": [null, 1, 20], \"name\": @, \"input_dtype\": \"float32\"}, \"inbound_nodes\": [], \"name\": @}, {\"class_name\": \"InputLayer\", \"config\": {\"batch_input_shape\": [null, 5, 20], \"name\": @, \"input_dtype\": \"float32\"}, \"inbound_nodes\": [], \"name\": @}, {\"class_name\": \"InputLayer\", \"config\": {\"batch_input_shape\": [null, 25, 20], \"name\": @, \"input_dtype\": \"float32\"}, \"inbound_nodes\": [], \"name\": @}, {\"class_name\": \"Flatten\", \"config\": {\"trainable\": true, \"name\": @}, \"inbound_nodes\": [[[@, 0, 0]]], \"name\": @}, {\"class_name\": \"Flatten\", \"config\": {\"trainable\": true, \"name\": @}, \"inbound_nodes\": [[[@, 0, 0]]], \"name\": @}, {\"class_name\": \"Flatten\", \"config\": {\"trainable\": true, \"name\": @}, \"inbound_nodes\": [[[@, 0, 0]]], \"name\": @}, {\"class_name\": \"Flatten\", \"config\": {\"trainable\": true, \"name\": @}, \"inbound_nodes\": [[[@, 0, 0]]], \"name\": @}, {\"class_name\": \"Flatten\", \"config\": {\"trainable\": true, \"name\": @}, \"inbound_nodes\": [[[@, 0, 0]]], \"name\": @}, {\"class_name\": \"Flatten\", \"config\": {\"trainable\": true, \"name\": @}, \"inbound_nodes\": [[[@, 0, 0]]], \"name\": @}, {\"class_name\": \"Flatten\", \"config\": {\"trainable\": true, \"name\": @}, \"inbound_nodes\": [[[@, 0, 0]]], \"name\": @}, {\"class_name\": \"Merge\", \"config\": {\"name\": @, \"concat_axis\": -1, \"mode_type\": \"raw\", \"dot_axes\": [-1, -1], \"mode\": \"concat\", \"output_shape\": null, \"output_shape_type\": \"raw\"}, \"inbound_nodes\": [[[@, 0, 0], [@, 0, 0], [@, 0, 0], [@, 0, 0], [@, 0, 0], [@, 0, 0], [@, 0, 0]]], \"name\": @}, {\"class_name\": \"Dense\", \"config\": {\"W_constraint\": null, \"b_constraint\": null, \"name\": @, \"activity_regularizer\": null, \"trainable\": true, \"init\": \"glorot_uniform\", \"bias\": true, \"input_dim\": null, \"b_regularizer\": null, \"W_regularizer\": null, \"activation\": \"relu\", \"output_dim\": 100}, \"inbound_nodes\": [[[@, 0, 0]]], \"name\": @}, {\"class_name\": \"Dense\", \"config\": {\"W_constraint\": null, \"b_constraint\": null, \"name\": @, \"activity_regularizer\": null, \"trainable\": true, \"init\": \"glorot_uniform\", \"bias\": true, \"input_dim\": null, \"b_regularizer\": null, \"W_regularizer\": null, \"activation\": \"sigmoid\", \"output_dim\": 2}, \"inbound_nodes\": [[[@, 0, 0]]], \"name\": @}], \"input_layers\": [[@, 0, 0], [@, 0, 0], [@, 0, 0], [@, 0, 0], [@, 0, 0], [@, 0, 0], [@, 0, 0]], \"output_layers\": [[@, 0, 0]], \"name\": @}}\"\n"
     ]
    }
   ],
   "source": [
    "string = \\\n",
    "'\"model\": \"{\\\"class_name\\\": \\\"Model\\\", \\\"keras_version\\\": \\\"1.0.6\\\", \\\"config\\\": {\\\"layers\\\": [{\\\"class_name\\\": \\\"InputLayer\\\", \\\"config\\\": {\\\"batch_input_shape\\\": [null, 10, 20], \\\"name\\\": \\\"EFlowNeutralHadron\\\", \\\"input_dtype\\\": \\\"float32\\\"}, \\\"inbound_nodes\\\": [], \\\"name\\\": \\\"EFlowNeutralHadron\\\"}, {\\\"class_name\\\": \\\"InputLayer\\\", \\\"config\\\": {\\\"batch_input_shape\\\": [null, 10, 20], \\\"name\\\": \\\"EFlowPhoton\\\", \\\"input_dtype\\\": \\\"float32\\\"}, \\\"inbound_nodes\\\": [], \\\"name\\\": \\\"EFlowPhoton\\\"}, {\\\"class_name\\\": \\\"InputLayer\\\", \\\"config\\\": {\\\"batch_input_shape\\\": [null, 10, 20], \\\"name\\\": \\\"EFlowTrack\\\", \\\"input_dtype\\\": \\\"float32\\\"}, \\\"inbound_nodes\\\": [], \\\"name\\\": \\\"EFlowTrack\\\"}, {\\\"class_name\\\": \\\"InputLayer\\\", \\\"config\\\": {\\\"batch_input_shape\\\": [null, 5, 20], \\\"name\\\": \\\"Electron\\\", \\\"input_dtype\\\": \\\"float32\\\"}, \\\"inbound_nodes\\\": [], \\\"name\\\": \\\"Electron\\\"}, {\\\"class_name\\\": \\\"InputLayer\\\", \\\"config\\\": {\\\"batch_input_shape\\\": [null, 1, 20], \\\"name\\\": \\\"MissingET\\\", \\\"input_dtype\\\": \\\"float32\\\"}, \\\"inbound_nodes\\\": [], \\\"name\\\": \\\"MissingET\\\"}, {\\\"class_name\\\": \\\"InputLayer\\\", \\\"config\\\": {\\\"batch_input_shape\\\": [null, 5, 20], \\\"name\\\": \\\"MuonTight\\\", \\\"input_dtype\\\": \\\"float32\\\"}, \\\"inbound_nodes\\\": [], \\\"name\\\": \\\"MuonTight\\\"}, {\\\"class_name\\\": \\\"InputLayer\\\", \\\"config\\\": {\\\"batch_input_shape\\\": [null, 25, 20], \\\"name\\\": \\\"Photon\\\", \\\"input_dtype\\\": \\\"float32\\\"}, \\\"inbound_nodes\\\": [], \\\"name\\\": \\\"Photon\\\"}, {\\\"class_name\\\": \\\"Flatten\\\", \\\"config\\\": {\\\"trainable\\\": true, \\\"name\\\": \\\"flatten_106\\\"}, \\\"inbound_nodes\\\": [[[\\\"Electron\\\", 0, 0]]], \\\"name\\\": \\\"flatten_106\\\"}, {\\\"class_name\\\": \\\"Flatten\\\", \\\"config\\\": {\\\"trainable\\\": true, \\\"name\\\": \\\"flatten_107\\\"}, \\\"inbound_nodes\\\": [[[\\\"MuonTight\\\", 0, 0]]], \\\"name\\\": \\\"flatten_107\\\"}, {\\\"class_name\\\": \\\"Flatten\\\", \\\"config\\\": {\\\"trainable\\\": true, \\\"name\\\": \\\"flatten_108\\\"}, \\\"inbound_nodes\\\": [[[\\\"Photon\\\", 0, 0]]], \\\"name\\\": \\\"flatten_108\\\"}, {\\\"class_name\\\": \\\"Flatten\\\", \\\"config\\\": {\\\"trainable\\\": true, \\\"name\\\": \\\"flatten_109\\\"}, \\\"inbound_nodes\\\": [[[\\\"MissingET\\\", 0, 0]]], \\\"name\\\": \\\"flatten_109\\\"}, {\\\"class_name\\\": \\\"Flatten\\\", \\\"config\\\": {\\\"trainable\\\": true, \\\"name\\\": \\\"flatten_110\\\"}, \\\"inbound_nodes\\\": [[[\\\"EFlowPhoton\\\", 0, 0]]], \\\"name\\\": \\\"flatten_110\\\"}, {\\\"class_name\\\": \\\"Flatten\\\", \\\"config\\\": {\\\"trainable\\\": true, \\\"name\\\": \\\"flatten_111\\\"}, \\\"inbound_nodes\\\": [[[\\\"EFlowNeutralHadron\\\", 0, 0]]], \\\"name\\\": \\\"flatten_111\\\"}, {\\\"class_name\\\": \\\"Flatten\\\", \\\"config\\\": {\\\"trainable\\\": true, \\\"name\\\": \\\"flatten_112\\\"}, \\\"inbound_nodes\\\": [[[\\\"EFlowTrack\\\", 0, 0]]], \\\"name\\\": \\\"flatten_112\\\"}, {\\\"class_name\\\": \\\"Merge\\\", \\\"config\\\": {\\\"name\\\": \\\"merge\\\", \\\"concat_axis\\\": -1, \\\"mode_type\\\": \\\"raw\\\", \\\"dot_axes\\\": [-1, -1], \\\"mode\\\": \\\"concat\\\", \\\"output_shape\\\": null, \\\"output_shape_type\\\": \\\"raw\\\"}, \\\"inbound_nodes\\\": [[[\\\"flatten_106\\\", 0, 0], [\\\"flatten_107\\\", 0, 0], [\\\"flatten_108\\\", 0, 0], [\\\"flatten_109\\\", 0, 0], [\\\"flatten_110\\\", 0, 0], [\\\"flatten_111\\\", 0, 0], [\\\"flatten_112\\\", 0, 0]]], \\\"name\\\": \\\"merge\\\"}, {\\\"class_name\\\": \\\"Dense\\\", \\\"config\\\": {\\\"W_constraint\\\": null, \\\"b_constraint\\\": null, \\\"name\\\": \\\"dense_2_0\\\", \\\"activity_regularizer\\\": null, \\\"trainable\\\": true, \\\"init\\\": \\\"glorot_uniform\\\", \\\"bias\\\": true, \\\"input_dim\\\": null, \\\"b_regularizer\\\": null, \\\"W_regularizer\\\": null, \\\"activation\\\": \\\"relu\\\", \\\"output_dim\\\": 100}, \\\"inbound_nodes\\\": [[[\\\"merge\\\", 0, 0]]], \\\"name\\\": \\\"dense_2_0\\\"}, {\\\"class_name\\\": \\\"Dense\\\", \\\"config\\\": {\\\"W_constraint\\\": null, \\\"b_constraint\\\": null, \\\"name\\\": \\\"main_output\\\", \\\"activity_regularizer\\\": null, \\\"trainable\\\": true, \\\"init\\\": \\\"glorot_uniform\\\", \\\"bias\\\": true, \\\"input_dim\\\": null, \\\"b_regularizer\\\": null, \\\"W_regularizer\\\": null, \\\"activation\\\": \\\"sigmoid\\\", \\\"output_dim\\\": 2}, \\\"inbound_nodes\\\": [[[\\\"dense_2_0\\\", 0, 0]]], \\\"name\\\": \\\"main_output\\\"}], \\\"input_layers\\\": [[\\\"Electron\\\", 0, 0], [\\\"MuonTight\\\", 0, 0], [\\\"Photon\\\", 0, 0], [\\\"MissingET\\\", 0, 0], [\\\"EFlowPhoton\\\", 0, 0], [\\\"EFlowNeutralHadron\\\", 0, 0], [\\\"EFlowTrack\\\", 0, 0]], \\\"output_layers\\\": [[\\\"main_output\\\", 0, 0]], \\\"name\\\": \\\"dense\\\"}}\"'\n",
    "print(string)\n",
    "names = [s.replace('\"name\": ', \"\" ) for s in re.findall(r'\"name\": \"[^\"]*\"', string)]\n",
    "for name in names:\n",
    "    string = string.replace(name, \"@\")\n",
    "print('\\n\\n')\n",
    "print(names)\n",
    "string = re.sub(r'\"keras_version\": \"[^\"]*\"', \"\", string)\n",
    "\n",
    "print('\\n\\n')\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from CMS_SURF_2016.utils.archiving import get_all_trials\n",
    "l= get_all_trials(archive_dir)\n",
    "for x in l:\n",
    "    print(x.remove_from_archive())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
